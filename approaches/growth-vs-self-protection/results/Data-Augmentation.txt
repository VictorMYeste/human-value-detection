2025-04-30 12:00:43,184 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-30 12:00:43,185 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-30 12:00:49,052 - INFO - Setting random seed to 42
2025-04-30 12:00:49,053 - INFO - Running training for labels: ['Growth Anxiety-Free', 'Self-Protection Anxiety-Avoidance']
2025-04-30 12:00:49,053 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-04-30 12:00:49,239 - INFO - Loading lexicon embeddings for: No lexicon used
2025-04-30 12:00:49,240 - INFO - Preparing datasets for training and validation
2025-04-30 12:01:30,827 - INFO - Arguments validated successfully.
2025-04-30 12:01:30,834 - INFO - Previous sentences or augmented data detected. Adjusting batch size: 4 -> 2 and gradient accumulation steps: 4 -> 8
2025-04-30 12:01:30,834 - INFO - Clearing old checkpoints in models/checkpoints
2025-04-30 12:01:32,057 - INFO - Using CUDA for training.
2025-04-30 12:01:33,281 - INFO - TRAINING
2025-04-30 12:01:33,281 - INFO - ========
2025-04-30 12:01:33,281 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Filter labels: []
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
ResidualBlock: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
Adding topic detection features: No
Applying token pruning: No

2025-04-30 12:01:33,284 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6057, 'grad_norm': 2.2549476623535156, 'learning_rate': 1.9670184696569924e-05, 'epoch': 0.16}
{'loss': 0.5899, 'grad_norm': 3.5520176887512207, 'learning_rate': 1.9340369393139843e-05, 'epoch': 0.33}
{'loss': 0.5887, 'grad_norm': 2.1187081336975098, 'learning_rate': 1.9010554089709766e-05, 'epoch': 0.49}
{'loss': 0.5766, 'grad_norm': 1.8862015008926392, 'learning_rate': 1.8680738786279685e-05, 'epoch': 0.66}
{'loss': 0.5693, 'grad_norm': 2.1059160232543945, 'learning_rate': 1.8350923482849604e-05, 'epoch': 0.82}
{'loss': 0.5625, 'grad_norm': 2.4191315174102783, 'learning_rate': 1.8021108179419526e-05, 'epoch': 0.99}
{'eval_loss': 0.5364760160446167, 'eval_f1-score': {'Growth Anxiety-Free': 0.36, 'Self-Protection Anxiety-Avoidance': 0.34}, 'eval_macro-avg-f1-score': 0.35, 'eval_runtime': 182.8846, 'eval_samples_per_second': 81.494, 'eval_steps_per_second': 20.374, 'epoch': 1.0}
2025-04-30 13:26:35,775 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.5211, 'grad_norm': 3.76898455619812, 'learning_rate': 1.769129287598945e-05, 'epoch': 1.15}
{'loss': 0.5117, 'grad_norm': 3.526719093322754, 'learning_rate': 1.7361477572559368e-05, 'epoch': 1.32}
{'loss': 0.4984, 'grad_norm': 3.815176486968994, 'learning_rate': 1.703166226912929e-05, 'epoch': 1.48}
{'loss': 0.4936, 'grad_norm': 4.0455498695373535, 'learning_rate': 1.670184696569921e-05, 'epoch': 1.65}
{'loss': 0.4915, 'grad_norm': 3.5960707664489746, 'learning_rate': 1.637203166226913e-05, 'epoch': 1.81}
{'loss': 0.4838, 'grad_norm': 4.0280961990356445, 'learning_rate': 1.604221635883905e-05, 'epoch': 1.98}
{'eval_loss': 0.5905739665031433, 'eval_f1-score': {'Growth Anxiety-Free': 0.3, 'Self-Protection Anxiety-Avoidance': 0.47}, 'eval_macro-avg-f1-score': 0.38, 'eval_runtime': 182.911, 'eval_samples_per_second': 81.482, 'eval_steps_per_second': 20.371, 'epoch': 2.0}
2025-04-30 14:51:37,504 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.3902, 'grad_norm': 3.927520513534546, 'learning_rate': 1.5712401055408974e-05, 'epoch': 2.14}
{'loss': 0.374, 'grad_norm': 7.546274662017822, 'learning_rate': 1.5382585751978893e-05, 'epoch': 2.31}
{'loss': 0.3758, 'grad_norm': 6.729501247406006, 'learning_rate': 1.5052770448548815e-05, 'epoch': 2.47}
{'loss': 0.3684, 'grad_norm': 17.969345092773438, 'learning_rate': 1.4722955145118736e-05, 'epoch': 2.64}
{'loss': 0.3667, 'grad_norm': 6.855569362640381, 'learning_rate': 1.4393139841688655e-05, 'epoch': 2.8}
{'loss': 0.3599, 'grad_norm': 4.836562156677246, 'learning_rate': 1.4063324538258576e-05, 'epoch': 2.97}
{'eval_loss': 0.6994932889938354, 'eval_f1-score': {'Growth Anxiety-Free': 0.37, 'Self-Protection Anxiety-Avoidance': 0.48}, 'eval_macro-avg-f1-score': 0.42, 'eval_runtime': 183.4383, 'eval_samples_per_second': 81.248, 'eval_steps_per_second': 20.312, 'epoch': 3.0}
{'loss': 0.2753, 'grad_norm': 8.302858352661133, 'learning_rate': 1.3733509234828497e-05, 'epoch': 3.13}
{'loss': 0.2527, 'grad_norm': 10.333252906799316, 'learning_rate': 1.3403693931398417e-05, 'epoch': 3.3}
{'loss': 0.2599, 'grad_norm': 5.868342876434326, 'learning_rate': 1.307387862796834e-05, 'epoch': 3.46}
{'loss': 0.2529, 'grad_norm': 8.47712230682373, 'learning_rate': 1.274406332453826e-05, 'epoch': 3.63}
{'loss': 0.2447, 'grad_norm': 8.315211296081543, 'learning_rate': 1.241424802110818e-05, 'epoch': 3.79}
{'loss': 0.244, 'grad_norm': 6.939112663269043, 'learning_rate': 1.20844327176781e-05, 'epoch': 3.96}
{'eval_loss': 0.8842675089836121, 'eval_f1-score': {'Growth Anxiety-Free': 0.39, 'Self-Protection Anxiety-Avoidance': 0.48}, 'eval_macro-avg-f1-score': 0.44, 'eval_runtime': 183.1373, 'eval_samples_per_second': 81.382, 'eval_steps_per_second': 20.345, 'epoch': 4.0}
{'loss': 0.1778, 'grad_norm': 9.004186630249023, 'learning_rate': 1.1754617414248021e-05, 'epoch': 4.12}
{'loss': 0.1666, 'grad_norm': 6.167448997497559, 'learning_rate': 1.1424802110817944e-05, 'epoch': 4.29}
{'loss': 0.1632, 'grad_norm': 10.06960678100586, 'learning_rate': 1.1094986807387865e-05, 'epoch': 4.45}
{'loss': 0.1662, 'grad_norm': 8.670488357543945, 'learning_rate': 1.0765171503957785e-05, 'epoch': 4.62}
{'loss': 0.1613, 'grad_norm': 14.805336952209473, 'learning_rate': 1.0435356200527704e-05, 'epoch': 4.78}
{'loss': 0.1593, 'grad_norm': 8.1282958984375, 'learning_rate': 1.0105540897097625e-05, 'epoch': 4.95}
{'eval_loss': 1.1181100606918335, 'eval_f1-score': {'Growth Anxiety-Free': 0.37, 'Self-Protection Anxiety-Avoidance': 0.48}, 'eval_macro-avg-f1-score': 0.42, 'eval_runtime': 183.2083, 'eval_samples_per_second': 81.35, 'eval_steps_per_second': 20.338, 'epoch': 5.0}
{'loss': 0.1252, 'grad_norm': 6.8773193359375, 'learning_rate': 9.775725593667546e-06, 'epoch': 5.11}
{'loss': 0.1094, 'grad_norm': 10.684078216552734, 'learning_rate': 9.445910290237469e-06, 'epoch': 5.28}
{'loss': 0.1124, 'grad_norm': 14.062139511108398, 'learning_rate': 9.11609498680739e-06, 'epoch': 5.44}
{'loss': 0.1074, 'grad_norm': 10.232526779174805, 'learning_rate': 8.786279683377308e-06, 'epoch': 5.61}
{'loss': 0.1092, 'grad_norm': 4.37109375, 'learning_rate': 8.456464379947231e-06, 'epoch': 5.77}
{'loss': 0.1083, 'grad_norm': 12.818504333496094, 'learning_rate': 8.126649076517152e-06, 'epoch': 5.94}
{'eval_loss': 1.346358060836792, 'eval_f1-score': {'Growth Anxiety-Free': 0.39, 'Self-Protection Anxiety-Avoidance': 0.48}, 'eval_macro-avg-f1-score': 0.44, 'eval_runtime': 183.5224, 'eval_samples_per_second': 81.211, 'eval_steps_per_second': 20.303, 'epoch': 6.0}
{'loss': 0.0886, 'grad_norm': 7.539210319519043, 'learning_rate': 7.79683377308707e-06, 'epoch': 6.1}
{'loss': 0.0748, 'grad_norm': 6.212313175201416, 'learning_rate': 7.4670184696569924e-06, 'epoch': 6.27}
{'loss': 0.0749, 'grad_norm': 3.7195889949798584, 'learning_rate': 7.137203166226914e-06, 'epoch': 6.43}
{'loss': 0.0746, 'grad_norm': 16.52861213684082, 'learning_rate': 6.807387862796835e-06, 'epoch': 6.6}
{'loss': 0.0757, 'grad_norm': 10.546171188354492, 'learning_rate': 6.477572559366755e-06, 'epoch': 6.76}
{'loss': 0.0762, 'grad_norm': 8.843552589416504, 'learning_rate': 6.1477572559366764e-06, 'epoch': 6.93}
{'eval_loss': 1.6566708087921143, 'eval_f1-score': {'Growth Anxiety-Free': 0.37, 'Self-Protection Anxiety-Avoidance': 0.48}, 'eval_macro-avg-f1-score': 0.42, 'eval_runtime': 183.4078, 'eval_samples_per_second': 81.262, 'eval_steps_per_second': 20.315, 'epoch': 7.0}
{'loss': 0.0627, 'grad_norm': 4.682643890380859, 'learning_rate': 5.817941952506597e-06, 'epoch': 7.09}
{'loss': 0.0517, 'grad_norm': 23.845060348510742, 'learning_rate': 5.488126649076517e-06, 'epoch': 7.26}
{'loss': 0.0523, 'grad_norm': 5.235235214233398, 'learning_rate': 5.158311345646439e-06, 'epoch': 7.42}
{'loss': 0.0546, 'grad_norm': 13.16712760925293, 'learning_rate': 4.828496042216359e-06, 'epoch': 7.59}
{'loss': 0.0524, 'grad_norm': 6.776692867279053, 'learning_rate': 4.49868073878628e-06, 'epoch': 7.75}
{'loss': 0.0538, 'grad_norm': 7.757320880889893, 'learning_rate': 4.168865435356201e-06, 'epoch': 7.92}
{'eval_loss': 1.8634787797927856, 'eval_f1-score': {'Growth Anxiety-Free': 0.4, 'Self-Protection Anxiety-Avoidance': 0.47}, 'eval_macro-avg-f1-score': 0.44, 'eval_runtime': 182.5283, 'eval_samples_per_second': 81.653, 'eval_steps_per_second': 20.413, 'epoch': 8.0}
{'train_runtime': 40951.397, 'train_samples_per_second': 23.693, 'train_steps_per_second': 0.74, 'train_loss': 0.26259122868089824, 'epoch': 8.0}
2025-04-30 23:24:05,719 - INFO - 

VALIDATION
2025-04-30 23:24:05,720 - INFO - ==========
2025-04-30 23:27:07,506 - INFO - Growth Anxiety-Free: 0.39
2025-04-30 23:27:07,506 - INFO - Self-Protection Anxiety-Avoidance: 0.48
2025-04-30 23:27:07,506 - INFO - Macro average: 0.44
2025-04-30 23:27:07,974 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
