2025-04-16 14:59:56,156 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-16 14:59:56,156 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-16 15:00:01,715 - INFO - Setting random seed to 42
2025-04-16 15:00:01,716 - INFO - Running training for labels: ['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance']
2025-04-16 15:00:01,716 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-04-16 15:00:01,944 - INFO - Loading lexicon embeddings for: No lexicon used
2025-04-16 15:00:01,944 - INFO - Preparing datasets for training and validation
2025-04-16 15:00:32,176 - INFO - Arguments validated successfully.
2025-04-16 15:00:32,182 - INFO - Previous sentences or augmented data detected. Adjusting batch size: 4 -> 2 and gradient accumulation steps: 4 -> 8
2025-04-16 15:00:32,182 - INFO - Clearing old checkpoints in models/checkpoints
2025-04-16 15:00:32,667 - INFO - Using CUDA for training.
2025-04-16 15:00:33,923 - INFO - TRAINING
2025-04-16 15:00:33,923 - INFO - ========
2025-04-16 15:00:33,923 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Filter labels: ['Presence']
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
Adding topic detection features: No
Applying token pruning: No

2025-04-16 15:00:33,926 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.2207, 'grad_norm': 0.5028740763664246, 'learning_rate': 1.9361022364217256e-05, 'epoch': 0.32}
{'loss': 0.1951, 'grad_norm': 0.4653048813343048, 'learning_rate': 1.8722044728434506e-05, 'epoch': 0.64}
{'loss': 0.1911, 'grad_norm': 0.46402794122695923, 'learning_rate': 1.808306709265176e-05, 'epoch': 0.96}
{'eval_loss': 0.16884104907512665, 'eval_f1-score': {'Self-direction: thought': 0, 'Self-direction: action': 0.0, 'Stimulation': 0, 'Hedonism': 0, 'Achievement': 0.12, 'Power: dominance': 0.3, 'Power: resources': 0.06, 'Face': 0, 'Security: personal': 0, 'Security: societal': 0.36, 'Tradition': 0.24, 'Conformity: rules': 0.35, 'Conformity: interpersonal': 0, 'Humility': 0, 'Benevolence: caring': 0.01, 'Benevolence: dependability': 0.04, 'Universalism: concern': 0.08, 'Universalism: nature': 0.27, 'Universalism: tolerance': 0}, 'eval_macro-avg-f1-score': 0.1, 'eval_runtime': 93.0457, 'eval_samples_per_second': 81.68, 'eval_steps_per_second': 20.42, 'epoch': 1.0}
2025-04-16 15:44:32,271 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.1797, 'grad_norm': 0.674777626991272, 'learning_rate': 1.744408945686901e-05, 'epoch': 1.28}
{'loss': 0.1749, 'grad_norm': 0.6487080454826355, 'learning_rate': 1.6805111821086264e-05, 'epoch': 1.6}
{'loss': 0.1715, 'grad_norm': 0.6351837515830994, 'learning_rate': 1.6166134185303515e-05, 'epoch': 1.92}
{'eval_loss': 0.16590338945388794, 'eval_f1-score': {'Self-direction: thought': 0, 'Self-direction: action': 0.03, 'Stimulation': 0.09, 'Hedonism': 0.23, 'Achievement': 0.34, 'Power: dominance': 0.3, 'Power: resources': 0.32, 'Face': 0.08, 'Security: personal': 0.35, 'Security: societal': 0.41, 'Tradition': 0.43, 'Conformity: rules': 0.37, 'Conformity: interpersonal': 0, 'Humility': 0, 'Benevolence: caring': 0.1, 'Benevolence: dependability': 0.14, 'Universalism: concern': 0.14, 'Universalism: nature': 0.55, 'Universalism: tolerance': 0.05}, 'eval_macro-avg-f1-score': 0.21, 'eval_runtime': 93.0584, 'eval_samples_per_second': 81.669, 'eval_steps_per_second': 20.417, 'epoch': 2.0}
2025-04-16 16:28:37,508 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.154, 'grad_norm': 0.8407263159751892, 'learning_rate': 1.552715654952077e-05, 'epoch': 2.24}
{'loss': 0.1464, 'grad_norm': 0.8485423922538757, 'learning_rate': 1.488817891373802e-05, 'epoch': 2.56}
{'loss': 0.1442, 'grad_norm': 1.0429960489273071, 'learning_rate': 1.4249201277955273e-05, 'epoch': 2.87}
{'eval_loss': 0.179379403591156, 'eval_f1-score': {'Self-direction: thought': 0.04, 'Self-direction: action': 0.04, 'Stimulation': 0.24, 'Hedonism': 0.22, 'Achievement': 0.25, 'Power: dominance': 0.33, 'Power: resources': 0.34, 'Face': 0.15, 'Security: personal': 0.36, 'Security: societal': 0.44, 'Tradition': 0.34, 'Conformity: rules': 0.45, 'Conformity: interpersonal': 0, 'Humility': 0, 'Benevolence: caring': 0.15, 'Benevolence: dependability': 0.19, 'Universalism: concern': 0.29, 'Universalism: nature': 0.57, 'Universalism: tolerance': 0.12}, 'eval_macro-avg-f1-score': 0.24, 'eval_runtime': 93.1036, 'eval_samples_per_second': 81.63, 'eval_steps_per_second': 20.407, 'epoch': 3.0}
{'loss': 0.1274, 'grad_norm': 0.9824290871620178, 'learning_rate': 1.3610223642172523e-05, 'epoch': 3.19}
{'loss': 0.1168, 'grad_norm': 1.4504815340042114, 'learning_rate': 1.2971246006389777e-05, 'epoch': 3.51}
{'loss': 0.1134, 'grad_norm': 1.4887663125991821, 'learning_rate': 1.233226837060703e-05, 'epoch': 3.83}
{'eval_loss': 0.20098380744457245, 'eval_f1-score': {'Self-direction: thought': 0.11, 'Self-direction: action': 0.2, 'Stimulation': 0.2, 'Hedonism': 0.24, 'Achievement': 0.36, 'Power: dominance': 0.32, 'Power: resources': 0.33, 'Face': 0.19, 'Security: personal': 0.39, 'Security: societal': 0.43, 'Tradition': 0.34, 'Conformity: rules': 0.4, 'Conformity: interpersonal': 0, 'Humility': 0, 'Benevolence: caring': 0.21, 'Benevolence: dependability': 0.16, 'Universalism: concern': 0.26, 'Universalism: nature': 0.54, 'Universalism: tolerance': 0.18}, 'eval_macro-avg-f1-score': 0.26, 'eval_runtime': 93.2701, 'eval_samples_per_second': 81.484, 'eval_steps_per_second': 20.371, 'epoch': 4.0}
{'loss': 0.1008, 'grad_norm': 1.2740569114685059, 'learning_rate': 1.1693290734824283e-05, 'epoch': 4.15}
{'loss': 0.0888, 'grad_norm': 1.4166960716247559, 'learning_rate': 1.1054313099041534e-05, 'epoch': 4.47}
{'loss': 0.0876, 'grad_norm': 1.659909963607788, 'learning_rate': 1.0415335463258786e-05, 'epoch': 4.79}
{'eval_loss': 0.22744788229465485, 'eval_f1-score': {'Self-direction: thought': 0.07, 'Self-direction: action': 0.14, 'Stimulation': 0.21, 'Hedonism': 0.23, 'Achievement': 0.3, 'Power: dominance': 0.29, 'Power: resources': 0.3, 'Face': 0.2, 'Security: personal': 0.35, 'Security: societal': 0.43, 'Tradition': 0.37, 'Conformity: rules': 0.46, 'Conformity: interpersonal': 0.01, 'Humility': 0, 'Benevolence: caring': 0.19, 'Benevolence: dependability': 0.2, 'Universalism: concern': 0.29, 'Universalism: nature': 0.52, 'Universalism: tolerance': 0.16}, 'eval_macro-avg-f1-score': 0.25, 'eval_runtime': 93.2396, 'eval_samples_per_second': 81.51, 'eval_steps_per_second': 20.378, 'epoch': 5.0}
{'loss': 0.0796, 'grad_norm': 1.2148412466049194, 'learning_rate': 9.77635782747604e-06, 'epoch': 5.11}
{'loss': 0.0679, 'grad_norm': 1.3631057739257812, 'learning_rate': 9.137380191693292e-06, 'epoch': 5.43}
{'loss': 0.0665, 'grad_norm': 1.3365737199783325, 'learning_rate': 8.498402555910544e-06, 'epoch': 5.75}
{'eval_loss': 0.25328606367111206, 'eval_f1-score': {'Self-direction: thought': 0.08, 'Self-direction: action': 0.18, 'Stimulation': 0.18, 'Hedonism': 0.19, 'Achievement': 0.35, 'Power: dominance': 0.31, 'Power: resources': 0.33, 'Face': 0.18, 'Security: personal': 0.34, 'Security: societal': 0.38, 'Tradition': 0.4, 'Conformity: rules': 0.37, 'Conformity: interpersonal': 0.03, 'Humility': 0.04, 'Benevolence: caring': 0.15, 'Benevolence: dependability': 0.18, 'Universalism: concern': 0.26, 'Universalism: nature': 0.5, 'Universalism: tolerance': 0.18}, 'eval_macro-avg-f1-score': 0.24, 'eval_runtime': 92.986, 'eval_samples_per_second': 81.733, 'eval_steps_per_second': 20.433, 'epoch': 6.0}
{'loss': 0.0631, 'grad_norm': 1.2034056186676025, 'learning_rate': 7.859424920127796e-06, 'epoch': 6.07}
{'loss': 0.0524, 'grad_norm': 1.3040075302124023, 'learning_rate': 7.220447284345049e-06, 'epoch': 6.39}
{'loss': 0.0515, 'grad_norm': 1.2344765663146973, 'learning_rate': 6.581469648562301e-06, 'epoch': 6.71}
{'eval_loss': 0.2664918005466461, 'eval_f1-score': {'Self-direction: thought': 0.07, 'Self-direction: action': 0.17, 'Stimulation': 0.22, 'Hedonism': 0.2, 'Achievement': 0.32, 'Power: dominance': 0.31, 'Power: resources': 0.29, 'Face': 0.18, 'Security: personal': 0.36, 'Security: societal': 0.38, 'Tradition': 0.37, 'Conformity: rules': 0.4, 'Conformity: interpersonal': 0.04, 'Humility': 0, 'Benevolence: caring': 0.15, 'Benevolence: dependability': 0.21, 'Universalism: concern': 0.27, 'Universalism: nature': 0.51, 'Universalism: tolerance': 0.15}, 'eval_macro-avg-f1-score': 0.24, 'eval_runtime': 93.0159, 'eval_samples_per_second': 81.706, 'eval_steps_per_second': 20.427, 'epoch': 7.0}
{'loss': 0.0502, 'grad_norm': 1.2938015460968018, 'learning_rate': 5.942492012779553e-06, 'epoch': 7.03}
{'loss': 0.0414, 'grad_norm': 1.335004210472107, 'learning_rate': 5.303514376996806e-06, 'epoch': 7.35}
{'loss': 0.0401, 'grad_norm': 1.531586766242981, 'learning_rate': 4.664536741214058e-06, 'epoch': 7.67}
{'loss': 0.0395, 'grad_norm': 1.759054183959961, 'learning_rate': 4.02555910543131e-06, 'epoch': 7.99}
{'eval_loss': 0.28319501876831055, 'eval_f1-score': {'Self-direction: thought': 0.09, 'Self-direction: action': 0.12, 'Stimulation': 0.2, 'Hedonism': 0.21, 'Achievement': 0.32, 'Power: dominance': 0.31, 'Power: resources': 0.33, 'Face': 0.22, 'Security: personal': 0.32, 'Security: societal': 0.4, 'Tradition': 0.36, 'Conformity: rules': 0.42, 'Conformity: interpersonal': 0.04, 'Humility': 0.04, 'Benevolence: caring': 0.18, 'Benevolence: dependability': 0.2, 'Universalism: concern': 0.26, 'Universalism: nature': 0.52, 'Universalism: tolerance': 0.12}, 'eval_macro-avg-f1-score': 0.25, 'eval_runtime': 93.2415, 'eval_samples_per_second': 81.509, 'eval_steps_per_second': 20.377, 'epoch': 8.0}
{'train_runtime': 21148.8608, 'train_samples_per_second': 23.685, 'train_steps_per_second': 0.74, 'train_loss': 0.11045832179619426, 'epoch': 8.0}
2025-04-16 20:53:05,191 - INFO - 

VALIDATION
2025-04-16 20:53:05,191 - INFO - ==========
2025-04-16 20:54:38,082 - INFO - Self-direction: thought: 0.11
2025-04-16 20:54:38,082 - INFO - Self-direction: action: 0.20
2025-04-16 20:54:38,082 - INFO - Stimulation: 0.20
2025-04-16 20:54:38,082 - INFO - Hedonism: 0.24
2025-04-16 20:54:38,082 - INFO - Achievement: 0.36
2025-04-16 20:54:38,082 - INFO - Power: dominance: 0.32
2025-04-16 20:54:38,082 - INFO - Power: resources: 0.33
2025-04-16 20:54:38,082 - INFO - Face: 0.19
2025-04-16 20:54:38,082 - INFO - Security: personal: 0.39
2025-04-16 20:54:38,082 - INFO - Security: societal: 0.43
2025-04-16 20:54:38,082 - INFO - Tradition: 0.34
2025-04-16 20:54:38,082 - INFO - Conformity: rules: 0.40
2025-04-16 20:54:38,082 - INFO - Conformity: interpersonal: 0.00
2025-04-16 20:54:38,082 - INFO - Humility: 0.00
2025-04-16 20:54:38,082 - INFO - Benevolence: caring: 0.21
2025-04-16 20:54:38,083 - INFO - Benevolence: dependability: 0.16
2025-04-16 20:54:38,083 - INFO - Universalism: concern: 0.26
2025-04-16 20:54:38,083 - INFO - Universalism: nature: 0.54
2025-04-16 20:54:38,083 - INFO - Universalism: tolerance: 0.18
2025-04-16 20:54:38,083 - INFO - Macro average: 0.26
2025-04-16 20:54:38,295 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
