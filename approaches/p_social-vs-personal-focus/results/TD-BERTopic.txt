2025-04-24 02:24:24,667 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-24 02:24:24,671 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-24 02:24:30,990 - INFO - Setting random seed to 42
2025-04-24 02:24:30,991 - INFO - Running training for labels: ['Social Focus', 'Personal Focus']
2025-04-24 02:24:30,991 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-04-24 02:24:31,210 - INFO - Loading lexicon embeddings for: No lexicon used
2025-04-24 02:24:31,210 - INFO - Preparing datasets for training and validation
2025-04-24 02:24:31,401 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-24 02:24:31,405 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-04-24 02:25:16,117 - INFO - Applying bertopic for topic modeling.
2025-04-24 02:25:36,183 - INFO - Applying bertopic for topic modeling.
2025-04-24 02:25:37,244 - INFO - Arguments validated successfully.
2025-04-24 02:25:37,253 - INFO - Clearing old checkpoints in models/checkpoints
2025-04-24 02:25:38,380 - INFO - Using CUDA for training.
2025-04-24 02:25:38,509 - INFO - TRAINING
2025-04-24 02:25:38,509 - INFO - ========
2025-04-24 02:25:38,509 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: TD-BERTopic
Filter labels: ['Presence']
Batch size: 4
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: No
Adding topic detection features: Yes
Applying token pruning: No

2025-04-24 02:25:38,512 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6304, 'grad_norm': 2.9280426502227783, 'learning_rate': 1.8611111111111114e-05, 'epoch': 0.69}
{'eval_loss': 0.5610790252685547, 'eval_f1-score': {'Social Focus': 0.76, 'Personal Focus': 0.69}, 'eval_macro-avg-f1-score': 0.72, 'eval_runtime': 208.9452, 'eval_samples_per_second': 36.373, 'eval_steps_per_second': 4.547, 'epoch': 1.0}
2025-04-24 02:44:40,352 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.5266, 'grad_norm': 4.93672513961792, 'learning_rate': 1.7222222222222224e-05, 'epoch': 1.39}
{'eval_loss': 0.550825834274292, 'eval_f1-score': {'Social Focus': 0.76, 'Personal Focus': 0.71}, 'eval_macro-avg-f1-score': 0.74, 'eval_runtime': 209.08, 'eval_samples_per_second': 36.35, 'eval_steps_per_second': 4.544, 'epoch': 2.0}
2025-04-24 03:04:06,022 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.4763, 'grad_norm': 10.167500495910645, 'learning_rate': 1.5833333333333333e-05, 'epoch': 2.08}
{'loss': 0.3796, 'grad_norm': 8.738839149475098, 'learning_rate': 1.4444444444444446e-05, 'epoch': 2.77}
{'eval_loss': 0.6162122488021851, 'eval_f1-score': {'Social Focus': 0.77, 'Personal Focus': 0.67}, 'eval_macro-avg-f1-score': 0.72, 'eval_runtime': 209.0179, 'eval_samples_per_second': 36.361, 'eval_steps_per_second': 4.545, 'epoch': 3.0}
{'loss': 0.311, 'grad_norm': 7.526506423950195, 'learning_rate': 1.3055555555555557e-05, 'epoch': 3.47}
{'eval_loss': 0.8615348935127258, 'eval_f1-score': {'Social Focus': 0.72, 'Personal Focus': 0.71}, 'eval_macro-avg-f1-score': 0.72, 'eval_runtime': 209.187, 'eval_samples_per_second': 36.331, 'eval_steps_per_second': 4.541, 'epoch': 4.0}
{'loss': 0.2629, 'grad_norm': 9.20102310180664, 'learning_rate': 1.1666666666666668e-05, 'epoch': 4.16}
{'loss': 0.1981, 'grad_norm': 10.689533233642578, 'learning_rate': 1.0277777777777777e-05, 'epoch': 4.86}
{'eval_loss': 0.9201005101203918, 'eval_f1-score': {'Social Focus': 0.74, 'Personal Focus': 0.7}, 'eval_macro-avg-f1-score': 0.72, 'eval_runtime': 209.7752, 'eval_samples_per_second': 36.229, 'eval_steps_per_second': 4.529, 'epoch': 5.0}
{'loss': 0.1568, 'grad_norm': 16.14738655090332, 'learning_rate': 8.888888888888888e-06, 'epoch': 5.55}
{'eval_loss': 1.0913830995559692, 'eval_f1-score': {'Social Focus': 0.76, 'Personal Focus': 0.67}, 'eval_macro-avg-f1-score': 0.72, 'eval_runtime': 209.0893, 'eval_samples_per_second': 36.348, 'eval_steps_per_second': 4.544, 'epoch': 6.0}
{'loss': 0.1356, 'grad_norm': 14.854818344116211, 'learning_rate': 7.500000000000001e-06, 'epoch': 6.24}
{'loss': 0.1067, 'grad_norm': 1.4169615507125854, 'learning_rate': 6.111111111111112e-06, 'epoch': 6.94}
{'eval_loss': 1.1932237148284912, 'eval_f1-score': {'Social Focus': 0.75, 'Personal Focus': 0.7}, 'eval_macro-avg-f1-score': 0.72, 'eval_runtime': 209.3417, 'eval_samples_per_second': 36.304, 'eval_steps_per_second': 4.538, 'epoch': 7.0}
{'train_runtime': 8178.002, 'train_samples_per_second': 28.2, 'train_steps_per_second': 0.88, 'train_loss': 0.31652223447151295, 'epoch': 7.0}
2025-04-24 04:41:56,917 - INFO - 

VALIDATION
2025-04-24 04:41:56,917 - INFO - ==========
2025-04-24 04:45:23,854 - INFO - Social Focus: 0.77
2025-04-24 04:45:23,854 - INFO - Personal Focus: 0.67
2025-04-24 04:45:23,854 - INFO - Macro average: 0.72
2025-04-24 04:45:24,000 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-24 04:45:24,000 - INFO - Saving best model to models directory
2025-04-24 04:45:24,000 - INFO - UPLOAD to https://huggingface.co/TD-BERTopic (using HF_TOKEN environment variable)
2025-04-24 04:45:24,000 - INFO - SAVE to models
