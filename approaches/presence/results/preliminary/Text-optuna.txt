2025-01-23 10:18:12,053 - datasets - INFO - PyTorch version 2.4.1+cu118 available.
2025-01-23 10:18:15,083 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 10:18:15,271 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 10:18:15,271 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 10:18:16,093 - HVD - INFO - Arguments validated successfully.
2025-01-23 10:18:16,564 - HVD - INFO - Using CUDA for training.
2025-01-23 10:18:17,828 - HVD - INFO - TRAINING
2025-01-23 10:18:17,828 - HVD - INFO - ========
2025-01-23 10:18:17,828 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.678920284036235e-05
Weight decay: 0.13712088363727098
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 10:18:17,834 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6902384161949158, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0514, 'eval_samples_per_second': 71.167, 'eval_steps_per_second': 8.896, 'epoch': 0.99}
2025-01-23 10:19:05,281 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7994478940963745, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1161, 'eval_samples_per_second': 70.841, 'eval_steps_per_second': 8.855, 'epoch': 1.98}
2025-01-23 10:19:50,194 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.899417519569397, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0507, 'eval_samples_per_second': 71.171, 'eval_steps_per_second': 8.896, 'epoch': 2.98}
{'eval_loss': 0.7531665563583374, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.155, 'eval_samples_per_second': 70.646, 'eval_steps_per_second': 8.831, 'epoch': 4.0}
{'eval_loss': 0.7772397994995117, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1607, 'eval_samples_per_second': 70.618, 'eval_steps_per_second': 8.827, 'epoch': 4.99}
{'eval_loss': 0.9544882774353027, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1498, 'eval_samples_per_second': 70.672, 'eval_steps_per_second': 8.834, 'epoch': 5.98}
{'eval_loss': 0.8265568017959595, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1493, 'eval_samples_per_second': 70.675, 'eval_steps_per_second': 8.834, 'epoch': 6.98}
{'train_runtime': 326.3383, 'train_samples_per_second': 30.643, 'train_steps_per_second': 0.95, 'train_loss': 0.6207793349519782, 'epoch': 6.98}
2025-01-23 10:23:44,688 - HVD - INFO - 

VALIDATION
2025-01-23 10:23:44,688 - HVD - INFO - ==========
2025-01-23 10:23:58,830 - HVD - INFO - Presence: 0.72
2025-01-23 10:23:58,830 - HVD - INFO - Macro average: 0.72
2025-01-23 10:24:13,729 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 10:24:14,182 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 10:24:14,183 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 10:24:15,004 - HVD - INFO - Arguments validated successfully.
2025-01-23 10:24:15,577 - HVD - INFO - Using CUDA for training.
2025-01-23 10:24:15,694 - HVD - INFO - TRAINING
2025-01-23 10:24:15,694 - HVD - INFO - ========
2025-01-23 10:24:15,694 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.2352328651661463e-05
Weight decay: 0.1725967718170017
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 10:24:15,699 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.9795222878456116, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1365, 'eval_samples_per_second': 70.739, 'eval_steps_per_second': 8.842, 'epoch': 0.99}
2025-01-23 10:25:00,886 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6873326301574707, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0693, 'eval_samples_per_second': 71.077, 'eval_steps_per_second': 8.885, 'epoch': 1.98}
2025-01-23 10:25:45,813 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.9086704850196838, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1438, 'eval_samples_per_second': 70.702, 'eval_steps_per_second': 8.838, 'epoch': 2.98}
{'eval_loss': 1.0980429649353027, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.167, 'eval_samples_per_second': 70.587, 'eval_steps_per_second': 8.823, 'epoch': 4.0}
{'eval_loss': 0.8118448853492737, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 14.0776, 'eval_samples_per_second': 71.035, 'eval_steps_per_second': 8.879, 'epoch': 4.99}
{'eval_loss': 0.9420743584632874, 'eval_f1-score': {'Presence': 0.63}, 'eval_marco-avg-f1-score': 0.63, 'eval_runtime': 14.151, 'eval_samples_per_second': 70.667, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 1.1483007669448853, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1513, 'eval_samples_per_second': 70.665, 'eval_steps_per_second': 8.833, 'epoch': 6.98}
{'train_runtime': 324.6223, 'train_samples_per_second': 30.805, 'train_steps_per_second': 0.955, 'train_loss': 0.5841805694300101, 'epoch': 6.98}
2025-01-23 10:29:40,602 - HVD - INFO - 

VALIDATION
2025-01-23 10:29:40,602 - HVD - INFO - ==========
2025-01-23 10:29:54,669 - HVD - INFO - Presence: 0.72
2025-01-23 10:29:54,669 - HVD - INFO - Macro average: 0.72
2025-01-23 10:30:09,492 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 10:30:09,767 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 10:30:09,767 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 10:30:10,574 - HVD - INFO - Arguments validated successfully.
2025-01-23 10:30:11,084 - HVD - INFO - Using CUDA for training.
2025-01-23 10:30:11,200 - HVD - INFO - TRAINING
2025-01-23 10:30:11,200 - HVD - INFO - ========
2025-01-23 10:30:11,200 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 7.335247967034675e-06
Weight decay: 0.1838463707245533
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 10:30:11,203 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6874396800994873, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1488, 'eval_samples_per_second': 70.677, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 10:30:56,390 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6887173652648926, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.0726, 'eval_samples_per_second': 71.06, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 10:31:41,321 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7626610398292542, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1432, 'eval_samples_per_second': 70.705, 'eval_steps_per_second': 8.838, 'epoch': 2.98}
{'eval_loss': 0.704663097858429, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1793, 'eval_samples_per_second': 70.525, 'eval_steps_per_second': 8.816, 'epoch': 4.0}
{'eval_loss': 0.7210471034049988, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0822, 'eval_samples_per_second': 71.012, 'eval_steps_per_second': 8.876, 'epoch': 4.99}
{'eval_loss': 0.7692111730575562, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1696, 'eval_samples_per_second': 70.574, 'eval_steps_per_second': 8.822, 'epoch': 5.98}
{'eval_loss': 0.7388311624526978, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.0926, 'eval_samples_per_second': 70.959, 'eval_steps_per_second': 8.87, 'epoch': 6.98}
{'train_runtime': 324.6097, 'train_samples_per_second': 30.806, 'train_steps_per_second': 0.955, 'train_loss': 0.6417281824514407, 'epoch': 6.98}
2025-01-23 10:35:36,076 - HVD - INFO - 

VALIDATION
2025-01-23 10:35:36,076 - HVD - INFO - ==========
2025-01-23 10:35:50,232 - HVD - INFO - Presence: 0.72
2025-01-23 10:35:50,232 - HVD - INFO - Macro average: 0.72
2025-01-23 10:36:05,183 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 10:36:05,429 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 10:36:05,429 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 10:36:06,244 - HVD - INFO - Arguments validated successfully.
2025-01-23 10:36:06,857 - HVD - INFO - Using CUDA for training.
2025-01-23 10:36:06,976 - HVD - INFO - TRAINING
2025-01-23 10:36:06,976 - HVD - INFO - ========
2025-01-23 10:36:06,976 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.966307963065048e-05
Weight decay: 0.27585432773617946
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 10:36:06,978 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6921375393867493, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1511, 'eval_samples_per_second': 70.666, 'eval_steps_per_second': 8.833, 'epoch': 0.99}
2025-01-23 10:36:52,181 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.813194215297699, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0797, 'eval_samples_per_second': 71.024, 'eval_steps_per_second': 8.878, 'epoch': 1.98}
2025-01-23 10:37:37,136 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 1.2383390665054321, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1555, 'eval_samples_per_second': 70.644, 'eval_steps_per_second': 8.831, 'epoch': 2.98}
{'eval_loss': 1.067254662513733, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1817, 'eval_samples_per_second': 70.514, 'eval_steps_per_second': 8.814, 'epoch': 4.0}
{'eval_loss': 1.0862157344818115, 'eval_f1-score': {'Presence': 0.58}, 'eval_marco-avg-f1-score': 0.58, 'eval_runtime': 14.0856, 'eval_samples_per_second': 70.994, 'eval_steps_per_second': 8.874, 'epoch': 4.99}
{'eval_loss': 1.4929900169372559, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1639, 'eval_samples_per_second': 70.602, 'eval_steps_per_second': 8.825, 'epoch': 5.98}
{'eval_loss': 2.1392197608947754, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1528, 'eval_samples_per_second': 70.657, 'eval_steps_per_second': 8.832, 'epoch': 6.98}
{'train_runtime': 324.8096, 'train_samples_per_second': 30.787, 'train_steps_per_second': 0.954, 'train_loss': 0.4438903178643743, 'epoch': 6.98}
2025-01-23 10:41:32,055 - HVD - INFO - 

VALIDATION
2025-01-23 10:41:32,055 - HVD - INFO - ==========
2025-01-23 10:41:46,134 - HVD - INFO - Presence: 0.72
2025-01-23 10:41:46,134 - HVD - INFO - Macro average: 0.72
2025-01-23 10:42:01,008 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 10:42:01,248 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 10:42:01,248 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 10:42:02,060 - HVD - INFO - Arguments validated successfully.
2025-01-23 10:42:02,573 - HVD - INFO - Using CUDA for training.
2025-01-23 10:42:02,690 - HVD - INFO - TRAINING
2025-01-23 10:42:02,690 - HVD - INFO - ========
2025-01-23 10:42:02,690 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.5867116237951346e-05
Weight decay: 0.1521955054348562
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 10:42:02,692 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7135009765625, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1431, 'eval_samples_per_second': 70.706, 'eval_steps_per_second': 8.838, 'epoch': 0.99}
2025-01-23 10:42:47,850 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.764045774936676, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0712, 'eval_samples_per_second': 71.067, 'eval_steps_per_second': 8.883, 'epoch': 1.98}
2025-01-23 10:43:32,759 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 1.5616612434387207, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.142, 'eval_samples_per_second': 70.711, 'eval_steps_per_second': 8.839, 'epoch': 2.98}
{'eval_loss': 1.0632249116897583, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1062, 'eval_samples_per_second': 70.891, 'eval_steps_per_second': 8.861, 'epoch': 4.0}
{'eval_loss': 1.1978501081466675, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1481, 'eval_samples_per_second': 70.681, 'eval_steps_per_second': 8.835, 'epoch': 4.99}
{'eval_loss': 1.0217435359954834, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.1547, 'eval_samples_per_second': 70.648, 'eval_steps_per_second': 8.831, 'epoch': 5.98}
{'eval_loss': 1.249898076057434, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.1518, 'eval_samples_per_second': 70.662, 'eval_steps_per_second': 8.833, 'epoch': 6.98}
{'train_runtime': 324.1619, 'train_samples_per_second': 30.849, 'train_steps_per_second': 0.956, 'train_loss': 0.5376373431004515, 'epoch': 6.98}
2025-01-23 10:47:27,124 - HVD - INFO - 

VALIDATION
2025-01-23 10:47:27,124 - HVD - INFO - ==========
2025-01-23 10:47:41,201 - HVD - INFO - Presence: 0.72
2025-01-23 10:47:41,201 - HVD - INFO - Macro average: 0.72
2025-01-23 10:47:56,151 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 10:47:56,394 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 10:47:56,394 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 10:47:57,219 - HVD - INFO - Arguments validated successfully.
2025-01-23 10:47:57,733 - HVD - INFO - Using CUDA for training.
2025-01-23 10:47:57,852 - HVD - INFO - TRAINING
2025-01-23 10:47:57,852 - HVD - INFO - ========
2025-01-23 10:47:57,852 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 8.147596585657647e-06
Weight decay: 0.2673212663015243
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 10:47:57,855 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6957223415374756, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1498, 'eval_samples_per_second': 70.672, 'eval_steps_per_second': 8.834, 'epoch': 0.99}
2025-01-23 10:48:43,043 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.721783459186554, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0757, 'eval_samples_per_second': 71.045, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 10:49:27,979 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7585031390190125, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1493, 'eval_samples_per_second': 70.675, 'eval_steps_per_second': 8.834, 'epoch': 2.98}
{'eval_loss': 0.7378560304641724, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1721, 'eval_samples_per_second': 70.561, 'eval_steps_per_second': 8.82, 'epoch': 4.0}
{'eval_loss': 0.7389123439788818, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.079, 'eval_samples_per_second': 71.028, 'eval_steps_per_second': 8.878, 'epoch': 4.99}
{'eval_loss': 0.7458470463752747, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1466, 'eval_samples_per_second': 70.689, 'eval_steps_per_second': 8.836, 'epoch': 5.98}
{'eval_loss': 0.7409511804580688, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.0756, 'eval_samples_per_second': 71.045, 'eval_steps_per_second': 8.881, 'epoch': 6.98}
{'train_runtime': 324.4408, 'train_samples_per_second': 30.822, 'train_steps_per_second': 0.955, 'train_loss': 0.6295217811514479, 'epoch': 6.98}
2025-01-23 10:53:22,569 - HVD - INFO - 

VALIDATION
2025-01-23 10:53:22,569 - HVD - INFO - ==========
2025-01-23 10:53:36,721 - HVD - INFO - Presence: 0.72
2025-01-23 10:53:36,721 - HVD - INFO - Macro average: 0.72
2025-01-23 10:53:51,628 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 10:53:51,868 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 10:53:51,868 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 10:53:52,677 - HVD - INFO - Arguments validated successfully.
2025-01-23 10:53:53,399 - HVD - INFO - Using CUDA for training.
2025-01-23 10:53:53,516 - HVD - INFO - TRAINING
2025-01-23 10:53:53,516 - HVD - INFO - ========
2025-01-23 10:53:53,516 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 4.8459777546015575e-05
Weight decay: 0.1354894414841089
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 10:53:53,519 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7082836627960205, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1575, 'eval_samples_per_second': 70.634, 'eval_steps_per_second': 8.829, 'epoch': 0.99}
2025-01-23 10:54:38,755 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.746981680393219, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0787, 'eval_samples_per_second': 71.029, 'eval_steps_per_second': 8.879, 'epoch': 1.98}
2025-01-23 10:55:23,719 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.9291250109672546, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.1515, 'eval_samples_per_second': 70.664, 'eval_steps_per_second': 8.833, 'epoch': 2.98}
{'eval_loss': 1.687583327293396, 'eval_f1-score': {'Presence': 0.63}, 'eval_marco-avg-f1-score': 0.63, 'eval_runtime': 14.185, 'eval_samples_per_second': 70.497, 'eval_steps_per_second': 8.812, 'epoch': 4.0}
{'eval_loss': 1.3784515857696533, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.0858, 'eval_samples_per_second': 70.994, 'eval_steps_per_second': 8.874, 'epoch': 4.99}
{'eval_loss': 1.5115429162979126, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.161, 'eval_samples_per_second': 70.617, 'eval_steps_per_second': 8.827, 'epoch': 5.98}
{'eval_loss': 2.477722644805908, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.1613, 'eval_samples_per_second': 70.615, 'eval_steps_per_second': 8.827, 'epoch': 6.98}
{'eval_loss': 3.004992961883545, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.117, 'eval_samples_per_second': 70.837, 'eval_steps_per_second': 8.855, 'epoch': 8.0}
{'eval_loss': 3.281481981277466, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.166, 'eval_samples_per_second': 70.592, 'eval_steps_per_second': 8.824, 'epoch': 8.99}
{'eval_loss': 3.8308184146881104, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1667, 'eval_samples_per_second': 70.588, 'eval_steps_per_second': 8.823, 'epoch': 9.92}
{'train_runtime': 475.686, 'train_samples_per_second': 21.022, 'train_steps_per_second': 0.652, 'train_loss': 0.3275586774272303, 'epoch': 9.92}
2025-01-23 11:01:49,485 - HVD - INFO - 

VALIDATION
2025-01-23 11:01:49,485 - HVD - INFO - ==========
2025-01-23 11:02:03,557 - HVD - INFO - Presence: 0.69
2025-01-23 11:02:03,557 - HVD - INFO - Macro average: 0.69
2025-01-23 11:02:18,908 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:02:19,220 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:02:19,220 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:02:20,031 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:02:20,539 - HVD - INFO - Using CUDA for training.
2025-01-23 11:02:20,658 - HVD - INFO - TRAINING
2025-01-23 11:02:20,658 - HVD - INFO - ========
2025-01-23 11:02:20,658 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 4.867540277909884e-05
Weight decay: 0.1010946462647734
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:02:20,660 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7010657787322998, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1471, 'eval_samples_per_second': 70.686, 'eval_steps_per_second': 8.836, 'epoch': 0.99}
2025-01-23 11:03:05,833 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.703102171421051, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0814, 'eval_samples_per_second': 71.016, 'eval_steps_per_second': 8.877, 'epoch': 1.98}
2025-01-23 11:03:50,754 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.6963417530059814, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1588, 'eval_samples_per_second': 70.628, 'eval_steps_per_second': 8.828, 'epoch': 2.98}
{'eval_loss': 0.7695832848548889, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1831, 'eval_samples_per_second': 70.506, 'eval_steps_per_second': 8.813, 'epoch': 4.0}
{'eval_loss': 0.9758397936820984, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.0805, 'eval_samples_per_second': 71.02, 'eval_steps_per_second': 8.878, 'epoch': 4.99}
{'eval_loss': 1.3596200942993164, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.1462, 'eval_samples_per_second': 70.691, 'eval_steps_per_second': 8.836, 'epoch': 5.98}
{'eval_loss': 1.83664870262146, 'eval_f1-score': {'Presence': 0.59}, 'eval_marco-avg-f1-score': 0.59, 'eval_runtime': 14.1455, 'eval_samples_per_second': 70.694, 'eval_steps_per_second': 8.837, 'epoch': 6.98}
{'train_runtime': 324.4063, 'train_samples_per_second': 30.826, 'train_steps_per_second': 0.956, 'train_loss': 0.5239248363249892, 'epoch': 6.98}
2025-01-23 11:07:45,349 - HVD - INFO - 

VALIDATION
2025-01-23 11:07:45,349 - HVD - INFO - ==========
2025-01-23 11:07:59,427 - HVD - INFO - Presence: 0.72
2025-01-23 11:07:59,427 - HVD - INFO - Macro average: 0.72
2025-01-23 11:08:14,298 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:08:14,583 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:08:14,583 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:08:15,391 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:08:15,906 - HVD - INFO - Using CUDA for training.
2025-01-23 11:08:16,022 - HVD - INFO - TRAINING
2025-01-23 11:08:16,023 - HVD - INFO - ========
2025-01-23 11:08:16,023 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 4.2457474201310445e-05
Weight decay: 0.1228923452691665
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:08:16,025 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6849315762519836, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1421, 'eval_samples_per_second': 70.711, 'eval_steps_per_second': 8.839, 'epoch': 0.99}
2025-01-23 11:09:01,179 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6884100437164307, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0699, 'eval_samples_per_second': 71.074, 'eval_steps_per_second': 8.884, 'epoch': 1.98}
2025-01-23 11:09:46,082 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 1.2263898849487305, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1422, 'eval_samples_per_second': 70.71, 'eval_steps_per_second': 8.839, 'epoch': 2.98}
{'eval_loss': 1.3776289224624634, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.1665, 'eval_samples_per_second': 70.589, 'eval_steps_per_second': 8.824, 'epoch': 4.0}
{'eval_loss': 1.440534234046936, 'eval_f1-score': {'Presence': 0.59}, 'eval_marco-avg-f1-score': 0.59, 'eval_runtime': 14.0763, 'eval_samples_per_second': 71.041, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 1.8220670223236084, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.1442, 'eval_samples_per_second': 70.7, 'eval_steps_per_second': 8.838, 'epoch': 5.98}
{'eval_loss': 2.0691795349121094, 'eval_f1-score': {'Presence': 0.56}, 'eval_marco-avg-f1-score': 0.56, 'eval_runtime': 14.1533, 'eval_samples_per_second': 70.655, 'eval_steps_per_second': 8.832, 'epoch': 6.98}
{'train_runtime': 324.3254, 'train_samples_per_second': 30.833, 'train_steps_per_second': 0.956, 'train_loss': 0.4886825981490109, 'epoch': 6.98}
2025-01-23 11:13:40,616 - HVD - INFO - 

VALIDATION
2025-01-23 11:13:40,616 - HVD - INFO - ==========
2025-01-23 11:13:54,695 - HVD - INFO - Presence: 0.69
2025-01-23 11:13:54,696 - HVD - INFO - Macro average: 0.69
2025-01-23 11:14:09,570 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:14:09,833 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:14:09,833 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:14:10,647 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:14:11,193 - HVD - INFO - Using CUDA for training.
2025-01-23 11:14:11,311 - HVD - INFO - TRAINING
2025-01-23 11:14:11,312 - HVD - INFO - ========
2025-01-23 11:14:11,312 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 6.379996437535727e-06
Weight decay: 0.11594351851915308
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:14:11,314 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7279454469680786, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.148, 'eval_samples_per_second': 70.681, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 11:14:56,478 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6916300058364868, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0731, 'eval_samples_per_second': 71.057, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 11:15:41,391 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7228074073791504, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1458, 'eval_samples_per_second': 70.692, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.7090820670127869, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1815, 'eval_samples_per_second': 70.514, 'eval_steps_per_second': 8.814, 'epoch': 4.0}
{'eval_loss': 0.7105879187583923, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.0761, 'eval_samples_per_second': 71.042, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 0.7318498492240906, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.148, 'eval_samples_per_second': 70.681, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 0.7438024878501892, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.0735, 'eval_samples_per_second': 71.056, 'eval_steps_per_second': 8.882, 'epoch': 6.98}
{'train_runtime': 324.2978, 'train_samples_per_second': 30.836, 'train_steps_per_second': 0.956, 'train_loss': 0.6283548337603928, 'epoch': 6.98}
2025-01-23 11:19:35,880 - HVD - INFO - 

VALIDATION
2025-01-23 11:19:35,880 - HVD - INFO - ==========
2025-01-23 11:19:50,022 - HVD - INFO - Presence: 0.72
2025-01-23 11:19:50,022 - HVD - INFO - Macro average: 0.72
2025-01-23 11:20:04,932 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:20:05,248 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:20:05,248 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:20:06,055 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:20:06,629 - HVD - INFO - Using CUDA for training.
2025-01-23 11:20:06,746 - HVD - INFO - TRAINING
2025-01-23 11:20:06,746 - HVD - INFO - ========
2025-01-23 11:20:06,746 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.3397206183541742e-05
Weight decay: 0.21599282999163924
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:20:06,749 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.689487099647522, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1456, 'eval_samples_per_second': 70.693, 'eval_steps_per_second': 8.837, 'epoch': 0.99}
2025-01-23 11:20:51,936 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7185564041137695, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0748, 'eval_samples_per_second': 71.049, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 11:21:36,879 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.797439694404602, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1487, 'eval_samples_per_second': 70.678, 'eval_steps_per_second': 8.835, 'epoch': 2.98}
{'eval_loss': 0.7132471799850464, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1743, 'eval_samples_per_second': 70.55, 'eval_steps_per_second': 8.819, 'epoch': 4.0}
{'eval_loss': 0.7980855703353882, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0807, 'eval_samples_per_second': 71.019, 'eval_steps_per_second': 8.877, 'epoch': 4.99}
{'eval_loss': 0.7791039347648621, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1706, 'eval_samples_per_second': 70.569, 'eval_steps_per_second': 8.821, 'epoch': 5.98}
{'eval_loss': 0.7867959141731262, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.1699, 'eval_samples_per_second': 70.572, 'eval_steps_per_second': 8.822, 'epoch': 6.98}
{'train_runtime': 324.6805, 'train_samples_per_second': 30.8, 'train_steps_per_second': 0.955, 'train_loss': 0.6169192112913919, 'epoch': 6.98}
2025-01-23 11:25:31,694 - HVD - INFO - 

VALIDATION
2025-01-23 11:25:31,695 - HVD - INFO - ==========
2025-01-23 11:25:45,777 - HVD - INFO - Presence: 0.72
2025-01-23 11:25:45,777 - HVD - INFO - Macro average: 0.72
2025-01-23 11:26:00,660 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:26:01,093 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:26:01,093 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:26:01,904 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:26:02,615 - HVD - INFO - Using CUDA for training.
2025-01-23 11:26:02,733 - HVD - INFO - TRAINING
2025-01-23 11:26:02,733 - HVD - INFO - ========
2025-01-23 11:26:02,733 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.1282775687457802e-05
Weight decay: 0.16845879866532154
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:26:02,736 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.691939651966095, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1522, 'eval_samples_per_second': 70.661, 'eval_steps_per_second': 8.833, 'epoch': 0.99}
2025-01-23 11:26:47,936 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7408806085586548, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0724, 'eval_samples_per_second': 71.061, 'eval_steps_per_second': 8.883, 'epoch': 1.98}
2025-01-23 11:27:32,861 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7639523148536682, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1477, 'eval_samples_per_second': 70.683, 'eval_steps_per_second': 8.835, 'epoch': 2.98}
{'eval_loss': 1.002668857574463, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1769, 'eval_samples_per_second': 70.537, 'eval_steps_per_second': 8.817, 'epoch': 4.0}
{'eval_loss': 0.8430224061012268, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.0829, 'eval_samples_per_second': 71.008, 'eval_steps_per_second': 8.876, 'epoch': 4.99}
{'eval_loss': 0.931576132774353, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1567, 'eval_samples_per_second': 70.638, 'eval_steps_per_second': 8.83, 'epoch': 5.98}
{'eval_loss': 0.897082507610321, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.1535, 'eval_samples_per_second': 70.654, 'eval_steps_per_second': 8.832, 'epoch': 6.98}
{'train_runtime': 324.5077, 'train_samples_per_second': 30.816, 'train_steps_per_second': 0.955, 'train_loss': 0.5578346602413633, 'epoch': 6.98}
2025-01-23 11:31:27,524 - HVD - INFO - 

VALIDATION
2025-01-23 11:31:27,524 - HVD - INFO - ==========
2025-01-23 11:31:41,607 - HVD - INFO - Presence: 0.72
2025-01-23 11:31:41,607 - HVD - INFO - Macro average: 0.72
2025-01-23 11:31:56,459 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:31:56,681 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:31:56,681 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:31:57,492 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:31:57,999 - HVD - INFO - Using CUDA for training.
2025-01-23 11:31:58,116 - HVD - INFO - TRAINING
2025-01-23 11:31:58,116 - HVD - INFO - ========
2025-01-23 11:31:58,116 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.8848387991660962e-05
Weight decay: 0.19691476936378952
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:31:58,119 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6869048476219177, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1441, 'eval_samples_per_second': 70.701, 'eval_steps_per_second': 8.838, 'epoch': 0.99}
2025-01-23 11:32:43,300 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7272159457206726, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0684, 'eval_samples_per_second': 71.081, 'eval_steps_per_second': 8.885, 'epoch': 1.98}
2025-01-23 11:33:28,227 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7188127636909485, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1415, 'eval_samples_per_second': 70.714, 'eval_steps_per_second': 8.839, 'epoch': 2.98}
{'eval_loss': 0.7732604742050171, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1706, 'eval_samples_per_second': 70.569, 'eval_steps_per_second': 8.821, 'epoch': 4.0}
{'eval_loss': 0.7789788842201233, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.0796, 'eval_samples_per_second': 71.025, 'eval_steps_per_second': 8.878, 'epoch': 4.99}
{'eval_loss': 0.786806046962738, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1508, 'eval_samples_per_second': 70.668, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 0.9452352523803711, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1506, 'eval_samples_per_second': 70.668, 'eval_steps_per_second': 8.834, 'epoch': 6.98}
{'eval_loss': 1.0118457078933716, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.0995, 'eval_samples_per_second': 70.924, 'eval_steps_per_second': 8.866, 'epoch': 8.0}
{'train_runtime': 371.4875, 'train_samples_per_second': 26.919, 'train_steps_per_second': 0.834, 'train_loss': 0.555713623046875, 'epoch': 8.0}
2025-01-23 11:38:09,874 - HVD - INFO - 

VALIDATION
2025-01-23 11:38:09,874 - HVD - INFO - ==========
2025-01-23 11:38:24,023 - HVD - INFO - Presence: 0.72
2025-01-23 11:38:24,024 - HVD - INFO - Macro average: 0.72
2025-01-23 11:38:39,010 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:38:39,239 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:38:39,239 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:38:40,037 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:38:40,555 - HVD - INFO - Using CUDA for training.
2025-01-23 11:38:40,672 - HVD - INFO - TRAINING
2025-01-23 11:38:40,672 - HVD - INFO - ========
2025-01-23 11:38:40,672 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.155448700189831e-05
Weight decay: 0.14729387073586622
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:38:40,675 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7469267845153809, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1486, 'eval_samples_per_second': 70.679, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 11:39:25,847 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.706881582736969, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0762, 'eval_samples_per_second': 71.042, 'eval_steps_per_second': 8.88, 'epoch': 1.98}
2025-01-23 11:40:10,773 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8074066042900085, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1485, 'eval_samples_per_second': 70.679, 'eval_steps_per_second': 8.835, 'epoch': 2.98}
{'eval_loss': 0.7522805333137512, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.1711, 'eval_samples_per_second': 70.566, 'eval_steps_per_second': 8.821, 'epoch': 4.0}
{'eval_loss': 0.8074126839637756, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.0773, 'eval_samples_per_second': 71.036, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 0.8445704579353333, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1513, 'eval_samples_per_second': 70.665, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 0.890985369682312, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.1557, 'eval_samples_per_second': 70.643, 'eval_steps_per_second': 8.83, 'epoch': 6.98}
{'train_runtime': 324.5295, 'train_samples_per_second': 30.814, 'train_steps_per_second': 0.955, 'train_loss': 0.5562316474564578, 'epoch': 6.98}
2025-01-23 11:44:05,472 - HVD - INFO - 

VALIDATION
2025-01-23 11:44:05,472 - HVD - INFO - ==========
2025-01-23 11:44:19,551 - HVD - INFO - Presence: 0.72
2025-01-23 11:44:19,551 - HVD - INFO - Macro average: 0.72
2025-01-23 11:44:34,426 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:44:34,666 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:44:34,666 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:44:35,460 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:44:36,009 - HVD - INFO - Using CUDA for training.
2025-01-23 11:44:36,126 - HVD - INFO - TRAINING
2025-01-23 11:44:36,126 - HVD - INFO - ========
2025-01-23 11:44:36,126 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.6359879749544226e-05
Weight decay: 0.22082169643602764
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:44:36,129 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6999135613441467, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1435, 'eval_samples_per_second': 70.704, 'eval_steps_per_second': 8.838, 'epoch': 0.99}
2025-01-23 11:45:21,300 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.8262143135070801, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.073, 'eval_samples_per_second': 71.058, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 11:46:06,221 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.9128199219703674, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1469, 'eval_samples_per_second': 70.687, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.9338330030441284, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.1859, 'eval_samples_per_second': 70.492, 'eval_steps_per_second': 8.812, 'epoch': 4.0}
{'eval_loss': 0.8915324807167053, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.0863, 'eval_samples_per_second': 70.991, 'eval_steps_per_second': 8.874, 'epoch': 4.99}
{'eval_loss': 0.9981861114501953, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1521, 'eval_samples_per_second': 70.661, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 1.1009267568588257, 'eval_f1-score': {'Presence': 0.63}, 'eval_marco-avg-f1-score': 0.63, 'eval_runtime': 14.1502, 'eval_samples_per_second': 70.67, 'eval_steps_per_second': 8.834, 'epoch': 6.98}
{'train_runtime': 324.504, 'train_samples_per_second': 30.816, 'train_steps_per_second': 0.955, 'train_loss': 0.5493965848870234, 'epoch': 6.98}
2025-01-23 11:50:00,899 - HVD - INFO - 

VALIDATION
2025-01-23 11:50:00,899 - HVD - INFO - ==========
2025-01-23 11:50:14,973 - HVD - INFO - Presence: 0.72
2025-01-23 11:50:14,974 - HVD - INFO - Macro average: 0.72
2025-01-23 11:50:29,850 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:50:30,083 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:50:30,083 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:50:30,887 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:50:31,397 - HVD - INFO - Using CUDA for training.
2025-01-23 11:50:31,514 - HVD - INFO - TRAINING
2025-01-23 11:50:31,514 - HVD - INFO - ========
2025-01-23 11:50:31,514 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.760444022461518e-05
Weight decay: 0.1649540646568272
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:50:31,516 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7053362131118774, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1464, 'eval_samples_per_second': 70.69, 'eval_steps_per_second': 8.836, 'epoch': 0.99}
2025-01-23 11:51:16,710 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7280300259590149, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0813, 'eval_samples_per_second': 71.016, 'eval_steps_per_second': 8.877, 'epoch': 1.98}
2025-01-23 11:52:01,652 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 1.0411455631256104, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1469, 'eval_samples_per_second': 70.687, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.7104083895683289, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 14.1742, 'eval_samples_per_second': 70.551, 'eval_steps_per_second': 8.819, 'epoch': 4.0}
{'eval_loss': 0.9072413444519043, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.111, 'eval_samples_per_second': 70.867, 'eval_steps_per_second': 8.858, 'epoch': 4.99}
{'eval_loss': 1.0423277616500854, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.158, 'eval_samples_per_second': 70.632, 'eval_steps_per_second': 8.829, 'epoch': 5.98}
{'eval_loss': 0.9874411821365356, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1627, 'eval_samples_per_second': 70.608, 'eval_steps_per_second': 8.826, 'epoch': 6.98}
{'train_runtime': 324.6936, 'train_samples_per_second': 30.798, 'train_steps_per_second': 0.955, 'train_loss': 0.5795193593436425, 'epoch': 6.98}
2025-01-23 11:55:56,489 - HVD - INFO - 

VALIDATION
2025-01-23 11:55:56,489 - HVD - INFO - ==========
2025-01-23 11:56:10,568 - HVD - INFO - Presence: 0.72
2025-01-23 11:56:10,568 - HVD - INFO - Macro average: 0.72
2025-01-23 11:56:25,450 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 11:56:25,714 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 11:56:25,714 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 11:56:26,525 - HVD - INFO - Arguments validated successfully.
2025-01-23 11:56:27,037 - HVD - INFO - Using CUDA for training.
2025-01-23 11:56:27,155 - HVD - INFO - TRAINING
2025-01-23 11:56:27,155 - HVD - INFO - ========
2025-01-23 11:56:27,155 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.0244002491973669e-05
Weight decay: 0.1303991598415483
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 11:56:27,158 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7150228023529053, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1534, 'eval_samples_per_second': 70.654, 'eval_steps_per_second': 8.832, 'epoch': 0.99}
2025-01-23 11:57:12,379 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.701045036315918, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.079, 'eval_samples_per_second': 71.028, 'eval_steps_per_second': 8.878, 'epoch': 1.98}
2025-01-23 11:57:57,346 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7467399835586548, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1444, 'eval_samples_per_second': 70.699, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.7101526856422424, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1692, 'eval_samples_per_second': 70.576, 'eval_steps_per_second': 8.822, 'epoch': 4.0}
{'eval_loss': 0.7238725423812866, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.0795, 'eval_samples_per_second': 71.025, 'eval_steps_per_second': 8.878, 'epoch': 4.99}
{'eval_loss': 0.7681838274002075, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1517, 'eval_samples_per_second': 70.663, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 0.7952219247817993, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.15, 'eval_samples_per_second': 70.671, 'eval_steps_per_second': 8.834, 'epoch': 6.98}
{'train_runtime': 324.6943, 'train_samples_per_second': 30.798, 'train_steps_per_second': 0.955, 'train_loss': 0.6139116899682842, 'epoch': 6.98}
2025-01-23 12:01:52,128 - HVD - INFO - 

VALIDATION
2025-01-23 12:01:52,128 - HVD - INFO - ==========
2025-01-23 12:02:06,203 - HVD - INFO - Presence: 0.72
2025-01-23 12:02:06,203 - HVD - INFO - Macro average: 0.72
2025-01-23 12:02:21,043 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:02:21,241 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:02:21,242 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:02:22,045 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:02:22,629 - HVD - INFO - Using CUDA for training.
2025-01-23 12:02:22,745 - HVD - INFO - TRAINING
2025-01-23 12:02:22,745 - HVD - INFO - ========
2025-01-23 12:02:22,746 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.6794806922913357e-05
Weight decay: 0.10594060015550749
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:02:22,748 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6867913603782654, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1425, 'eval_samples_per_second': 70.709, 'eval_steps_per_second': 8.839, 'epoch': 0.99}
2025-01-23 12:03:07,917 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.694502055644989, 'eval_f1-score': {'Presence': 0.42}, 'eval_marco-avg-f1-score': 0.42, 'eval_runtime': 14.0733, 'eval_samples_per_second': 71.056, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 12:03:52,834 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.6870982050895691, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1457, 'eval_samples_per_second': 70.693, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.6930233240127563, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.169, 'eval_samples_per_second': 70.577, 'eval_steps_per_second': 8.822, 'epoch': 4.0}
{'eval_loss': 0.7388415336608887, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0746, 'eval_samples_per_second': 71.05, 'eval_steps_per_second': 8.881, 'epoch': 4.99}
{'eval_loss': 0.8594101667404175, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1512, 'eval_samples_per_second': 70.666, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 0.85688316822052, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1456, 'eval_samples_per_second': 70.694, 'eval_steps_per_second': 8.837, 'epoch': 6.98}
{'train_runtime': 324.4703, 'train_samples_per_second': 30.819, 'train_steps_per_second': 0.955, 'train_loss': 0.6444657351992545, 'epoch': 6.98}
2025-01-23 12:07:47,484 - HVD - INFO - 

VALIDATION
2025-01-23 12:07:47,484 - HVD - INFO - ==========
2025-01-23 12:08:01,558 - HVD - INFO - Presence: 0.72
2025-01-23 12:08:01,558 - HVD - INFO - Macro average: 0.72
2025-01-23 12:08:16,435 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:08:16,677 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:08:16,677 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:08:17,476 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:08:18,071 - HVD - INFO - Using CUDA for training.
2025-01-23 12:08:18,187 - HVD - INFO - TRAINING
2025-01-23 12:08:18,188 - HVD - INFO - ========
2025-01-23 12:08:18,188 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 5.15971097781326e-06
Weight decay: 0.14427491171560639
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:08:18,190 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7063573598861694, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1409, 'eval_samples_per_second': 70.717, 'eval_steps_per_second': 8.84, 'epoch': 0.99}
2025-01-23 12:09:03,369 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6875390410423279, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0687, 'eval_samples_per_second': 71.08, 'eval_steps_per_second': 8.885, 'epoch': 1.98}
2025-01-23 12:09:48,283 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7473113536834717, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1431, 'eval_samples_per_second': 70.706, 'eval_steps_per_second': 8.838, 'epoch': 2.98}
{'eval_loss': 0.7090804576873779, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1744, 'eval_samples_per_second': 70.55, 'eval_steps_per_second': 8.819, 'epoch': 4.0}
{'eval_loss': 0.7028787732124329, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0815, 'eval_samples_per_second': 71.015, 'eval_steps_per_second': 8.877, 'epoch': 4.99}
{'eval_loss': 0.722569465637207, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1554, 'eval_samples_per_second': 70.644, 'eval_steps_per_second': 8.831, 'epoch': 5.98}
{'eval_loss': 0.7217406034469604, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0869, 'eval_samples_per_second': 70.988, 'eval_steps_per_second': 8.873, 'epoch': 6.98}
{'train_runtime': 324.4623, 'train_samples_per_second': 30.82, 'train_steps_per_second': 0.955, 'train_loss': 0.6441891171516628, 'epoch': 6.98}
2025-01-23 12:13:42,929 - HVD - INFO - 

VALIDATION
2025-01-23 12:13:42,929 - HVD - INFO - ==========
2025-01-23 12:13:57,088 - HVD - INFO - Presence: 0.72
2025-01-23 12:13:57,089 - HVD - INFO - Macro average: 0.72
2025-01-23 12:14:12,052 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:14:12,321 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:14:12,321 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:14:13,131 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:14:13,653 - HVD - INFO - Using CUDA for training.
2025-01-23 12:14:13,771 - HVD - INFO - TRAINING
2025-01-23 12:14:13,771 - HVD - INFO - ========
2025-01-23 12:14:13,772 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.3243988457930982e-05
Weight decay: 0.23400201784716604
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:14:13,774 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7055912613868713, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1499, 'eval_samples_per_second': 70.672, 'eval_steps_per_second': 8.834, 'epoch': 0.99}
2025-01-23 12:14:58,952 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.689903736114502, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0772, 'eval_samples_per_second': 71.037, 'eval_steps_per_second': 8.88, 'epoch': 1.98}
2025-01-23 12:15:43,894 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.9894178509712219, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1476, 'eval_samples_per_second': 70.683, 'eval_steps_per_second': 8.835, 'epoch': 2.98}
{'eval_loss': 1.046060562133789, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1848, 'eval_samples_per_second': 70.498, 'eval_steps_per_second': 8.812, 'epoch': 4.0}
{'eval_loss': 0.9936037659645081, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.0739, 'eval_samples_per_second': 71.054, 'eval_steps_per_second': 8.882, 'epoch': 4.99}
{'eval_loss': 0.9856697916984558, 'eval_f1-score': {'Presence': 0.63}, 'eval_marco-avg-f1-score': 0.63, 'eval_runtime': 14.1482, 'eval_samples_per_second': 70.68, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 0.9531857371330261, 'eval_f1-score': {'Presence': 0.55}, 'eval_marco-avg-f1-score': 0.55, 'eval_runtime': 14.0751, 'eval_samples_per_second': 71.047, 'eval_steps_per_second': 8.881, 'epoch': 6.98}
{'train_runtime': 324.4564, 'train_samples_per_second': 30.821, 'train_steps_per_second': 0.955, 'train_loss': 0.5757736066065797, 'epoch': 6.98}
2025-01-23 12:19:38,498 - HVD - INFO - 

VALIDATION
2025-01-23 12:19:38,498 - HVD - INFO - ==========
2025-01-23 12:19:52,648 - HVD - INFO - Presence: 0.72
2025-01-23 12:19:52,648 - HVD - INFO - Macro average: 0.72
2025-01-23 12:20:07,598 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:20:07,924 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:20:07,925 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:20:08,728 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:20:09,317 - HVD - INFO - Using CUDA for training.
2025-01-23 12:20:09,433 - HVD - INFO - TRAINING
2025-01-23 12:20:09,434 - HVD - INFO - ========
2025-01-23 12:20:09,434 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.4390301107108236e-05
Weight decay: 0.19841243875487893
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:20:09,436 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.705231249332428, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1462, 'eval_samples_per_second': 70.691, 'eval_steps_per_second': 8.836, 'epoch': 0.99}
2025-01-23 12:20:54,596 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7003298401832581, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0718, 'eval_samples_per_second': 71.064, 'eval_steps_per_second': 8.883, 'epoch': 1.98}
2025-01-23 12:21:39,504 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7704227566719055, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1426, 'eval_samples_per_second': 70.708, 'eval_steps_per_second': 8.839, 'epoch': 2.98}
{'eval_loss': 0.8422142267227173, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1778, 'eval_samples_per_second': 70.533, 'eval_steps_per_second': 8.817, 'epoch': 4.0}
{'eval_loss': 0.8242026567459106, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.0782, 'eval_samples_per_second': 71.032, 'eval_steps_per_second': 8.879, 'epoch': 4.99}
{'eval_loss': 0.7910877466201782, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1462, 'eval_samples_per_second': 70.69, 'eval_steps_per_second': 8.836, 'epoch': 5.98}
{'eval_loss': 0.8745933175086975, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1486, 'eval_samples_per_second': 70.678, 'eval_steps_per_second': 8.835, 'epoch': 6.98}
{'train_runtime': 324.4324, 'train_samples_per_second': 30.823, 'train_steps_per_second': 0.956, 'train_loss': 0.5829456924298487, 'epoch': 6.98}
2025-01-23 12:25:34,134 - HVD - INFO - 

VALIDATION
2025-01-23 12:25:34,135 - HVD - INFO - ==========
2025-01-23 12:25:48,203 - HVD - INFO - Presence: 0.72
2025-01-23 12:25:48,203 - HVD - INFO - Macro average: 0.72
2025-01-23 12:26:03,076 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:26:03,305 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:26:03,305 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:26:04,112 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:26:04,910 - HVD - INFO - Using CUDA for training.
2025-01-23 12:26:05,028 - HVD - INFO - TRAINING
2025-01-23 12:26:05,028 - HVD - INFO - ========
2025-01-23 12:26:05,028 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 9.475913814019182e-06
Weight decay: 0.18526749179405946
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:26:05,031 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7066130638122559, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1482, 'eval_samples_per_second': 70.68, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 12:26:50,229 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7007302641868591, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.0772, 'eval_samples_per_second': 71.037, 'eval_steps_per_second': 8.88, 'epoch': 1.98}
2025-01-23 12:27:35,177 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7221200466156006, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1536, 'eval_samples_per_second': 70.654, 'eval_steps_per_second': 8.832, 'epoch': 2.98}
{'eval_loss': 0.6950057148933411, 'eval_f1-score': {'Presence': 0}, 'eval_marco-avg-f1-score': 0.0, 'eval_runtime': 14.1032, 'eval_samples_per_second': 70.906, 'eval_steps_per_second': 8.863, 'epoch': 4.0}
{'eval_loss': 0.6885689496994019, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1506, 'eval_samples_per_second': 70.668, 'eval_steps_per_second': 8.834, 'epoch': 4.99}
{'eval_loss': 0.6861835718154907, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1518, 'eval_samples_per_second': 70.663, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 0.6846963167190552, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1585, 'eval_samples_per_second': 70.629, 'eval_steps_per_second': 8.829, 'epoch': 6.98}
{'eval_loss': 0.6846970319747925, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1075, 'eval_samples_per_second': 70.884, 'eval_steps_per_second': 8.861, 'epoch': 8.0}
{'eval_loss': 0.6849521398544312, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1555, 'eval_samples_per_second': 70.644, 'eval_steps_per_second': 8.831, 'epoch': 8.99}
{'train_runtime': 418.0729, 'train_samples_per_second': 23.919, 'train_steps_per_second': 0.741, 'train_loss': 0.6774168184219306, 'epoch': 8.99}
2025-01-23 12:33:03,383 - HVD - INFO - 

VALIDATION
2025-01-23 12:33:03,383 - HVD - INFO - ==========
2025-01-23 12:33:17,458 - HVD - INFO - Presence: 0.72
2025-01-23 12:33:17,458 - HVD - INFO - Macro average: 0.72
2025-01-23 12:33:32,660 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:33:32,906 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:33:32,907 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:33:33,709 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:33:34,230 - HVD - INFO - Using CUDA for training.
2025-01-23 12:33:34,346 - HVD - INFO - TRAINING
2025-01-23 12:33:34,346 - HVD - INFO - ========
2025-01-23 12:33:34,346 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 7.720652417138362e-06
Weight decay: 0.17679070949200631
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:33:34,349 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7697005867958069, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1394, 'eval_samples_per_second': 70.724, 'eval_steps_per_second': 8.841, 'epoch': 0.99}
2025-01-23 12:34:19,537 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6925076842308044, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0672, 'eval_samples_per_second': 71.088, 'eval_steps_per_second': 8.886, 'epoch': 1.98}
2025-01-23 12:35:04,465 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7184769511222839, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1373, 'eval_samples_per_second': 70.735, 'eval_steps_per_second': 8.842, 'epoch': 2.98}
{'eval_loss': 0.7229727506637573, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1766, 'eval_samples_per_second': 70.539, 'eval_steps_per_second': 8.817, 'epoch': 4.0}
{'eval_loss': 0.6970163583755493, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.0729, 'eval_samples_per_second': 71.059, 'eval_steps_per_second': 8.882, 'epoch': 4.99}
{'eval_loss': 0.7228235602378845, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1455, 'eval_samples_per_second': 70.694, 'eval_steps_per_second': 8.837, 'epoch': 5.98}
{'eval_loss': 0.7199870347976685, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.0727, 'eval_samples_per_second': 71.059, 'eval_steps_per_second': 8.882, 'epoch': 6.98}
{'train_runtime': 324.4455, 'train_samples_per_second': 30.822, 'train_steps_per_second': 0.955, 'train_loss': 0.6288168670934274, 'epoch': 6.98}
2025-01-23 12:38:59,073 - HVD - INFO - 

VALIDATION
2025-01-23 12:38:59,073 - HVD - INFO - ==========
2025-01-23 12:39:13,220 - HVD - INFO - Presence: 0.72
2025-01-23 12:39:13,220 - HVD - INFO - Macro average: 0.72
2025-01-23 12:39:28,168 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:39:28,780 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:39:28,780 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:39:29,577 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:39:30,106 - HVD - INFO - Using CUDA for training.
2025-01-23 12:39:30,223 - HVD - INFO - TRAINING
2025-01-23 12:39:30,223 - HVD - INFO - ========
2025-01-23 12:39:30,223 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.0822858310017356e-05
Weight decay: 0.15671552995475743
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:39:30,226 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7031539082527161, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1489, 'eval_samples_per_second': 70.677, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 12:40:15,430 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7865626215934753, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0736, 'eval_samples_per_second': 71.055, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 12:41:00,377 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 1.0687446594238281, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1456, 'eval_samples_per_second': 70.693, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.7603067755699158, 'eval_f1-score': {'Presence': 0.63}, 'eval_marco-avg-f1-score': 0.63, 'eval_runtime': 14.1742, 'eval_samples_per_second': 70.551, 'eval_steps_per_second': 8.819, 'epoch': 4.0}
{'eval_loss': 1.416621208190918, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.0782, 'eval_samples_per_second': 71.032, 'eval_steps_per_second': 8.879, 'epoch': 4.99}
{'eval_loss': 1.0955666303634644, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1484, 'eval_samples_per_second': 70.679, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 1.3658804893493652, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.1517, 'eval_samples_per_second': 70.663, 'eval_steps_per_second': 8.833, 'epoch': 6.98}
{'train_runtime': 324.5967, 'train_samples_per_second': 30.807, 'train_steps_per_second': 0.955, 'train_loss': 0.5200087871026555, 'epoch': 6.98}
2025-01-23 12:44:55,101 - HVD - INFO - 

VALIDATION
2025-01-23 12:44:55,101 - HVD - INFO - ==========
2025-01-23 12:45:09,177 - HVD - INFO - Presence: 0.72
2025-01-23 12:45:09,177 - HVD - INFO - Macro average: 0.72
2025-01-23 12:45:24,019 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:45:24,269 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:45:24,269 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:45:25,060 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:45:25,784 - HVD - INFO - Using CUDA for training.
2025-01-23 12:45:25,901 - HVD - INFO - TRAINING
2025-01-23 12:45:25,901 - HVD - INFO - ========
2025-01-23 12:45:25,901 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.6902762464559573e-05
Weight decay: 0.18679431586927972
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:45:25,904 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6876280307769775, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1456, 'eval_samples_per_second': 70.693, 'eval_steps_per_second': 8.837, 'epoch': 0.99}
2025-01-23 12:46:11,088 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7221851944923401, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0709, 'eval_samples_per_second': 71.069, 'eval_steps_per_second': 8.884, 'epoch': 1.98}
2025-01-23 12:46:56,004 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7995353937149048, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1419, 'eval_samples_per_second': 70.712, 'eval_steps_per_second': 8.839, 'epoch': 2.98}
{'eval_loss': 0.8073095679283142, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1703, 'eval_samples_per_second': 70.57, 'eval_steps_per_second': 8.821, 'epoch': 4.0}
{'eval_loss': 0.8210635781288147, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.0747, 'eval_samples_per_second': 71.049, 'eval_steps_per_second': 8.881, 'epoch': 4.99}
{'eval_loss': 0.8876105546951294, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1503, 'eval_samples_per_second': 70.67, 'eval_steps_per_second': 8.834, 'epoch': 5.98}
{'eval_loss': 0.8427610397338867, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.0782, 'eval_samples_per_second': 71.032, 'eval_steps_per_second': 8.879, 'epoch': 6.98}
{'train_runtime': 324.3641, 'train_samples_per_second': 30.83, 'train_steps_per_second': 0.956, 'train_loss': 0.5813985002150229, 'epoch': 6.98}
2025-01-23 12:50:50,547 - HVD - INFO - 

VALIDATION
2025-01-23 12:50:50,547 - HVD - INFO - ==========
2025-01-23 12:51:04,700 - HVD - INFO - Presence: 0.72
2025-01-23 12:51:04,700 - HVD - INFO - Macro average: 0.72
2025-01-23 12:51:19,621 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:51:19,889 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:51:19,889 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:51:20,695 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:51:21,217 - HVD - INFO - Using CUDA for training.
2025-01-23 12:51:21,336 - HVD - INFO - TRAINING
2025-01-23 12:51:21,336 - HVD - INFO - ========
2025-01-23 12:51:21,336 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.1789080630496439e-05
Weight decay: 0.1390798460516515
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:51:21,339 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6872101426124573, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1534, 'eval_samples_per_second': 70.654, 'eval_steps_per_second': 8.832, 'epoch': 0.99}
2025-01-23 12:52:06,551 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6890289187431335, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0774, 'eval_samples_per_second': 71.036, 'eval_steps_per_second': 8.879, 'epoch': 1.98}
2025-01-23 12:52:51,501 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7252856492996216, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1523, 'eval_samples_per_second': 70.66, 'eval_steps_per_second': 8.833, 'epoch': 2.98}
{'eval_loss': 0.7067996859550476, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1773, 'eval_samples_per_second': 70.535, 'eval_steps_per_second': 8.817, 'epoch': 4.0}
{'eval_loss': 0.7812194228172302, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0802, 'eval_samples_per_second': 71.022, 'eval_steps_per_second': 8.878, 'epoch': 4.99}
{'eval_loss': 0.7968294024467468, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1536, 'eval_samples_per_second': 70.654, 'eval_steps_per_second': 8.832, 'epoch': 5.98}
{'eval_loss': 0.7812800407409668, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.0829, 'eval_samples_per_second': 71.008, 'eval_steps_per_second': 8.876, 'epoch': 6.98}
{'train_runtime': 324.6281, 'train_samples_per_second': 30.804, 'train_steps_per_second': 0.955, 'train_loss': 0.6104958910460866, 'epoch': 6.98}
2025-01-23 12:56:46,241 - HVD - INFO - 

VALIDATION
2025-01-23 12:56:46,241 - HVD - INFO - ==========
2025-01-23 12:57:00,394 - HVD - INFO - Presence: 0.72
2025-01-23 12:57:00,394 - HVD - INFO - Macro average: 0.72
2025-01-23 12:57:15,345 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 12:57:15,584 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 12:57:15,584 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 12:57:16,388 - HVD - INFO - Arguments validated successfully.
2025-01-23 12:57:17,337 - HVD - INFO - Using CUDA for training.
2025-01-23 12:57:17,455 - HVD - INFO - TRAINING
2025-01-23 12:57:17,456 - HVD - INFO - ========
2025-01-23 12:57:17,456 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.400612461836054e-05
Weight decay: 0.24215553907471307
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 12:57:17,458 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6855937242507935, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1484, 'eval_samples_per_second': 70.679, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 12:58:02,666 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6910407543182373, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0766, 'eval_samples_per_second': 71.04, 'eval_steps_per_second': 8.88, 'epoch': 1.98}
2025-01-23 12:58:47,600 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7178042531013489, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1524, 'eval_samples_per_second': 70.659, 'eval_steps_per_second': 8.832, 'epoch': 2.98}
{'eval_loss': 1.1377290487289429, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1743, 'eval_samples_per_second': 70.55, 'eval_steps_per_second': 8.819, 'epoch': 4.0}
{'eval_loss': 0.9294974207878113, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.0762, 'eval_samples_per_second': 71.042, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 1.083713412284851, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.1502, 'eval_samples_per_second': 70.67, 'eval_steps_per_second': 8.834, 'epoch': 5.98}
{'eval_loss': 1.2984651327133179, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.0804, 'eval_samples_per_second': 71.021, 'eval_steps_per_second': 8.878, 'epoch': 6.98}
{'train_runtime': 324.4199, 'train_samples_per_second': 30.824, 'train_steps_per_second': 0.956, 'train_loss': 0.5648550331045729, 'epoch': 6.98}
2025-01-23 13:02:42,172 - HVD - INFO - 

VALIDATION
2025-01-23 13:02:42,172 - HVD - INFO - ==========
2025-01-23 13:02:56,319 - HVD - INFO - Presence: 0.72
2025-01-23 13:02:56,319 - HVD - INFO - Macro average: 0.72
2025-01-23 13:03:11,268 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:03:11,508 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:03:11,508 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:03:12,309 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:03:13,030 - HVD - INFO - Using CUDA for training.
2025-01-23 13:03:13,147 - HVD - INFO - TRAINING
2025-01-23 13:03:13,147 - HVD - INFO - ========
2025-01-23 13:03:13,147 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.8403692508079313e-05
Weight decay: 0.16184066202499686
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:03:13,150 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6866146326065063, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1469, 'eval_samples_per_second': 70.687, 'eval_steps_per_second': 8.836, 'epoch': 0.99}
2025-01-23 13:03:58,334 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7067701816558838, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0739, 'eval_samples_per_second': 71.053, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 13:04:43,257 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7962028980255127, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1435, 'eval_samples_per_second': 70.704, 'eval_steps_per_second': 8.838, 'epoch': 2.98}
{'eval_loss': 0.7602083683013916, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1756, 'eval_samples_per_second': 70.544, 'eval_steps_per_second': 8.818, 'epoch': 4.0}
{'eval_loss': 0.8519238233566284, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.0763, 'eval_samples_per_second': 71.042, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 0.8727744221687317, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1507, 'eval_samples_per_second': 70.668, 'eval_steps_per_second': 8.834, 'epoch': 5.98}
{'eval_loss': 0.8395587801933289, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.0777, 'eval_samples_per_second': 71.035, 'eval_steps_per_second': 8.879, 'epoch': 6.98}
{'train_runtime': 324.4472, 'train_samples_per_second': 30.822, 'train_steps_per_second': 0.955, 'train_loss': 0.5751917427832928, 'epoch': 6.98}
2025-01-23 13:08:37,871 - HVD - INFO - 

VALIDATION
2025-01-23 13:08:37,871 - HVD - INFO - ==========
2025-01-23 13:08:52,018 - HVD - INFO - Presence: 0.72
2025-01-23 13:08:52,018 - HVD - INFO - Macro average: 0.72
2025-01-23 13:09:06,971 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:09:07,218 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:09:07,219 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:09:08,019 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:09:08,614 - HVD - INFO - Using CUDA for training.
2025-01-23 13:09:08,731 - HVD - INFO - TRAINING
2025-01-23 13:09:08,731 - HVD - INFO - ========
2025-01-23 13:09:08,731 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.467203066288588e-05
Weight decay: 0.11531905119886718
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:09:08,733 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6945125460624695, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.147, 'eval_samples_per_second': 70.686, 'eval_steps_per_second': 8.836, 'epoch': 0.99}
2025-01-23 13:09:53,900 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6902923583984375, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0691, 'eval_samples_per_second': 71.078, 'eval_steps_per_second': 8.885, 'epoch': 1.98}
2025-01-23 13:10:38,802 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7680335640907288, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.145, 'eval_samples_per_second': 70.696, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.8016170859336853, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1782, 'eval_samples_per_second': 70.531, 'eval_steps_per_second': 8.816, 'epoch': 4.0}
{'eval_loss': 0.7613707184791565, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.0816, 'eval_samples_per_second': 71.015, 'eval_steps_per_second': 8.877, 'epoch': 4.99}
{'eval_loss': 0.8070299029350281, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.1452, 'eval_samples_per_second': 70.695, 'eval_steps_per_second': 8.837, 'epoch': 5.98}
{'eval_loss': 0.8805605173110962, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.0739, 'eval_samples_per_second': 71.053, 'eval_steps_per_second': 8.882, 'epoch': 6.98}
{'train_runtime': 324.2942, 'train_samples_per_second': 30.836, 'train_steps_per_second': 0.956, 'train_loss': 0.5924429937240181, 'epoch': 6.98}
2025-01-23 13:14:33,301 - HVD - INFO - 

VALIDATION
2025-01-23 13:14:33,301 - HVD - INFO - ==========
2025-01-23 13:14:47,451 - HVD - INFO - Presence: 0.72
2025-01-23 13:14:47,451 - HVD - INFO - Macro average: 0.72
2025-01-23 13:15:02,404 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:15:02,845 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:15:02,845 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:15:03,650 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:15:04,166 - HVD - INFO - Using CUDA for training.
2025-01-23 13:15:04,284 - HVD - INFO - TRAINING
2025-01-23 13:15:04,284 - HVD - INFO - ========
2025-01-23 13:15:04,284 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 5.069472006608287e-06
Weight decay: 0.17836380895866946
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:15:04,287 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7338159084320068, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1415, 'eval_samples_per_second': 70.714, 'eval_steps_per_second': 8.839, 'epoch': 0.99}
2025-01-23 13:15:49,472 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6898335218429565, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0747, 'eval_samples_per_second': 71.049, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 13:16:34,408 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8196724653244019, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1463, 'eval_samples_per_second': 70.69, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.7111835479736328, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1732, 'eval_samples_per_second': 70.556, 'eval_steps_per_second': 8.819, 'epoch': 4.0}
{'eval_loss': 0.7101965546607971, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0765, 'eval_samples_per_second': 71.041, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 0.7193935513496399, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1526, 'eval_samples_per_second': 70.659, 'eval_steps_per_second': 8.832, 'epoch': 5.98}
{'eval_loss': 0.7177101373672485, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0867, 'eval_samples_per_second': 70.989, 'eval_steps_per_second': 8.874, 'epoch': 6.98}
{'train_runtime': 324.4454, 'train_samples_per_second': 30.822, 'train_steps_per_second': 0.955, 'train_loss': 0.6444380873933845, 'epoch': 6.98}
2025-01-23 13:20:29,011 - HVD - INFO - 

VALIDATION
2025-01-23 13:20:29,011 - HVD - INFO - ==========
2025-01-23 13:20:43,159 - HVD - INFO - Presence: 0.72
2025-01-23 13:20:43,159 - HVD - INFO - Macro average: 0.72
2025-01-23 13:20:58,107 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:20:58,361 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:20:58,361 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:20:59,163 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:20:59,873 - HVD - INFO - Using CUDA for training.
2025-01-23 13:20:59,990 - HVD - INFO - TRAINING
2025-01-23 13:20:59,990 - HVD - INFO - ========
2025-01-23 13:20:59,990 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.280736347155962e-05
Weight decay: 0.20111401788857727
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:20:59,992 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7048184871673584, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1508, 'eval_samples_per_second': 70.667, 'eval_steps_per_second': 8.833, 'epoch': 0.99}
2025-01-23 13:21:45,192 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 1.1025058031082153, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0774, 'eval_samples_per_second': 71.036, 'eval_steps_per_second': 8.879, 'epoch': 1.98}
2025-01-23 13:22:30,135 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.9422693848609924, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1494, 'eval_samples_per_second': 70.675, 'eval_steps_per_second': 8.834, 'epoch': 2.98}
{'eval_loss': 0.9765328168869019, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.1895, 'eval_samples_per_second': 70.474, 'eval_steps_per_second': 8.809, 'epoch': 4.0}
{'eval_loss': 0.9165753722190857, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.0937, 'eval_samples_per_second': 70.954, 'eval_steps_per_second': 8.869, 'epoch': 4.99}
{'eval_loss': 0.9276995658874512, 'eval_f1-score': {'Presence': 0.58}, 'eval_marco-avg-f1-score': 0.58, 'eval_runtime': 14.1619, 'eval_samples_per_second': 70.612, 'eval_steps_per_second': 8.826, 'epoch': 5.98}
{'eval_loss': 1.212092638015747, 'eval_f1-score': {'Presence': 0.56}, 'eval_marco-avg-f1-score': 0.56, 'eval_runtime': 14.1624, 'eval_samples_per_second': 70.609, 'eval_steps_per_second': 8.826, 'epoch': 6.98}
{'train_runtime': 324.6962, 'train_samples_per_second': 30.798, 'train_steps_per_second': 0.955, 'train_loss': 0.539367395803469, 'epoch': 6.98}
2025-01-23 13:26:24,959 - HVD - INFO - 

VALIDATION
2025-01-23 13:26:24,959 - HVD - INFO - ==========
2025-01-23 13:26:39,040 - HVD - INFO - Presence: 0.72
2025-01-23 13:26:39,040 - HVD - INFO - Macro average: 0.72
2025-01-23 13:26:53,920 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:26:54,161 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:26:54,161 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:26:54,954 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:26:55,465 - HVD - INFO - Using CUDA for training.
2025-01-23 13:26:55,584 - HVD - INFO - TRAINING
2025-01-23 13:26:55,584 - HVD - INFO - ========
2025-01-23 13:26:55,584 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.0932277679290523e-05
Weight decay: 0.2650914227442357
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:26:55,587 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6907731890678406, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1508, 'eval_samples_per_second': 70.667, 'eval_steps_per_second': 8.833, 'epoch': 0.99}
2025-01-23 13:27:40,783 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7283264994621277, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0764, 'eval_samples_per_second': 71.041, 'eval_steps_per_second': 8.88, 'epoch': 1.98}
2025-01-23 13:28:25,724 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7328429818153381, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1487, 'eval_samples_per_second': 70.678, 'eval_steps_per_second': 8.835, 'epoch': 2.98}
{'eval_loss': 1.1127597093582153, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1135, 'eval_samples_per_second': 70.854, 'eval_steps_per_second': 8.857, 'epoch': 4.0}
{'eval_loss': 0.8493324518203735, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1597, 'eval_samples_per_second': 70.623, 'eval_steps_per_second': 8.828, 'epoch': 4.99}
{'eval_loss': 0.9718537330627441, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1609, 'eval_samples_per_second': 70.617, 'eval_steps_per_second': 8.827, 'epoch': 5.98}
{'eval_loss': 1.1489601135253906, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1612, 'eval_samples_per_second': 70.616, 'eval_steps_per_second': 8.827, 'epoch': 6.98}
{'train_runtime': 324.3093, 'train_samples_per_second': 30.835, 'train_steps_per_second': 0.956, 'train_loss': 0.5524402933383207, 'epoch': 6.98}
2025-01-23 13:32:20,168 - HVD - INFO - 

VALIDATION
2025-01-23 13:32:20,168 - HVD - INFO - ==========
2025-01-23 13:32:34,249 - HVD - INFO - Presence: 0.72
2025-01-23 13:32:34,249 - HVD - INFO - Macro average: 0.72
2025-01-23 13:32:49,202 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:32:49,465 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:32:49,465 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:32:50,266 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:32:50,781 - HVD - INFO - Using CUDA for training.
2025-01-23 13:32:50,898 - HVD - INFO - TRAINING
2025-01-23 13:32:50,898 - HVD - INFO - ========
2025-01-23 13:32:50,898 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.8754508113095654e-05
Weight decay: 0.29553965688413636
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:32:50,901 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6921888589859009, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1427, 'eval_samples_per_second': 70.708, 'eval_steps_per_second': 8.838, 'epoch': 0.99}
2025-01-23 13:33:36,084 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.8932008147239685, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0748, 'eval_samples_per_second': 71.049, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 13:34:21,009 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 1.0084325075149536, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1456, 'eval_samples_per_second': 70.693, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.8745858073234558, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.1736, 'eval_samples_per_second': 70.554, 'eval_steps_per_second': 8.819, 'epoch': 4.0}
{'eval_loss': 0.9880787134170532, 'eval_f1-score': {'Presence': 0.51}, 'eval_marco-avg-f1-score': 0.51, 'eval_runtime': 14.0762, 'eval_samples_per_second': 71.042, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 1.34586763381958, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1479, 'eval_samples_per_second': 70.682, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 1.6178700923919678, 'eval_f1-score': {'Presence': 0.57}, 'eval_marco-avg-f1-score': 0.57, 'eval_runtime': 14.0813, 'eval_samples_per_second': 71.016, 'eval_steps_per_second': 8.877, 'epoch': 6.98}
{'train_runtime': 324.3821, 'train_samples_per_second': 30.828, 'train_steps_per_second': 0.956, 'train_loss': 0.46870572851338516, 'epoch': 6.98}
2025-01-23 13:38:15,564 - HVD - INFO - 

VALIDATION
2025-01-23 13:38:15,564 - HVD - INFO - ==========
2025-01-23 13:38:29,713 - HVD - INFO - Presence: 0.72
2025-01-23 13:38:29,713 - HVD - INFO - Macro average: 0.72
2025-01-23 13:38:44,626 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:38:44,870 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:38:44,870 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:38:45,673 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:38:46,222 - HVD - INFO - Using CUDA for training.
2025-01-23 13:38:46,339 - HVD - INFO - TRAINING
2025-01-23 13:38:46,339 - HVD - INFO - ========
2025-01-23 13:38:46,339 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.6031855494386575e-05
Weight decay: 0.2838297043323286
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:38:46,342 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7093995213508606, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1414, 'eval_samples_per_second': 70.714, 'eval_steps_per_second': 8.839, 'epoch': 0.99}
2025-01-23 13:39:31,511 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.8311959505081177, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0756, 'eval_samples_per_second': 71.045, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 13:40:16,438 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7908889055252075, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.1466, 'eval_samples_per_second': 70.689, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.8969965577125549, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1874, 'eval_samples_per_second': 70.485, 'eval_steps_per_second': 8.811, 'epoch': 4.0}
{'eval_loss': 1.0289251804351807, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.0898, 'eval_samples_per_second': 70.974, 'eval_steps_per_second': 8.872, 'epoch': 4.99}
{'eval_loss': 1.32777738571167, 'eval_f1-score': {'Presence': 0.54}, 'eval_marco-avg-f1-score': 0.54, 'eval_runtime': 14.1479, 'eval_samples_per_second': 70.682, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 2.029561996459961, 'eval_f1-score': {'Presence': 0.56}, 'eval_marco-avg-f1-score': 0.56, 'eval_runtime': 14.1448, 'eval_samples_per_second': 70.698, 'eval_steps_per_second': 8.837, 'epoch': 6.98}
{'eval_loss': 2.3714637756347656, 'eval_f1-score': {'Presence': 0.59}, 'eval_marco-avg-f1-score': 0.59, 'eval_runtime': 14.1025, 'eval_samples_per_second': 70.91, 'eval_steps_per_second': 8.864, 'epoch': 8.0}
{'train_runtime': 371.4433, 'train_samples_per_second': 26.922, 'train_steps_per_second': 0.835, 'train_loss': 0.404775634765625, 'epoch': 8.0}
2025-01-23 13:44:58,051 - HVD - INFO - 

VALIDATION
2025-01-23 13:44:58,051 - HVD - INFO - ==========
2025-01-23 13:45:12,203 - HVD - INFO - Presence: 0.66
2025-01-23 13:45:12,203 - HVD - INFO - Macro average: 0.66
2025-01-23 13:45:27,263 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:45:27,501 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:45:27,501 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:45:28,298 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:45:29,010 - HVD - INFO - Using CUDA for training.
2025-01-23 13:45:29,128 - HVD - INFO - TRAINING
2025-01-23 13:45:29,128 - HVD - INFO - ========
2025-01-23 13:45:29,128 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.263474868249861e-05
Weight decay: 0.2514730411386134
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:45:29,130 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6907826066017151, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1468, 'eval_samples_per_second': 70.688, 'eval_steps_per_second': 8.836, 'epoch': 0.99}
2025-01-23 13:46:14,347 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7272262573242188, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0786, 'eval_samples_per_second': 71.03, 'eval_steps_per_second': 8.879, 'epoch': 1.98}
2025-01-23 13:46:59,285 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8021494150161743, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1525, 'eval_samples_per_second': 70.659, 'eval_steps_per_second': 8.832, 'epoch': 2.98}
{'eval_loss': 0.7530207633972168, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.193, 'eval_samples_per_second': 70.457, 'eval_steps_per_second': 8.807, 'epoch': 4.0}
{'eval_loss': 0.8243841528892517, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.0933, 'eval_samples_per_second': 70.956, 'eval_steps_per_second': 8.869, 'epoch': 4.99}
{'eval_loss': 0.8638508319854736, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1703, 'eval_samples_per_second': 70.57, 'eval_steps_per_second': 8.821, 'epoch': 5.98}
{'eval_loss': 0.9426707029342651, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.0896, 'eval_samples_per_second': 70.974, 'eval_steps_per_second': 8.872, 'epoch': 6.98}
{'train_runtime': 324.6751, 'train_samples_per_second': 30.8, 'train_steps_per_second': 0.955, 'train_loss': 0.5511936922685816, 'epoch': 6.98}
2025-01-23 13:50:54,086 - HVD - INFO - 

VALIDATION
2025-01-23 13:50:54,086 - HVD - INFO - ==========
2025-01-23 13:51:08,237 - HVD - INFO - Presence: 0.72
2025-01-23 13:51:08,237 - HVD - INFO - Macro average: 0.72
2025-01-23 13:51:23,197 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:51:23,434 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:51:23,435 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:51:24,235 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:51:24,752 - HVD - INFO - Using CUDA for training.
2025-01-23 13:51:24,869 - HVD - INFO - TRAINING
2025-01-23 13:51:24,869 - HVD - INFO - ========
2025-01-23 13:51:24,869 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 7.2189711159659456e-06
Weight decay: 0.1286454287956615
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:51:24,872 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7338274121284485, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1419, 'eval_samples_per_second': 70.712, 'eval_steps_per_second': 8.839, 'epoch': 0.99}
2025-01-23 13:52:10,037 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6928071975708008, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0737, 'eval_samples_per_second': 71.054, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 13:52:54,961 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7277554869651794, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1505, 'eval_samples_per_second': 70.669, 'eval_steps_per_second': 8.834, 'epoch': 2.98}
{'eval_loss': 0.7147669792175293, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1773, 'eval_samples_per_second': 70.535, 'eval_steps_per_second': 8.817, 'epoch': 4.0}
{'eval_loss': 0.710546612739563, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.0799, 'eval_samples_per_second': 71.023, 'eval_steps_per_second': 8.878, 'epoch': 4.99}
{'eval_loss': 0.7386515140533447, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1546, 'eval_samples_per_second': 70.648, 'eval_steps_per_second': 8.831, 'epoch': 5.98}
{'eval_loss': 0.7571364641189575, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.151, 'eval_samples_per_second': 70.666, 'eval_steps_per_second': 8.833, 'epoch': 6.98}
{'train_runtime': 324.5355, 'train_samples_per_second': 30.813, 'train_steps_per_second': 0.955, 'train_loss': 0.6314245801453197, 'epoch': 6.98}
2025-01-23 13:56:49,673 - HVD - INFO - 

VALIDATION
2025-01-23 13:56:49,673 - HVD - INFO - ==========
2025-01-23 13:57:03,754 - HVD - INFO - Presence: 0.72
2025-01-23 13:57:03,754 - HVD - INFO - Macro average: 0.72
2025-01-23 13:57:18,598 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 13:57:18,834 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 13:57:18,834 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 13:57:19,634 - HVD - INFO - Arguments validated successfully.
2025-01-23 13:57:20,139 - HVD - INFO - Using CUDA for training.
2025-01-23 13:57:20,256 - HVD - INFO - TRAINING
2025-01-23 13:57:20,256 - HVD - INFO - ========
2025-01-23 13:57:20,256 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 4.623286256118017e-05
Weight decay: 0.1526273482401731
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 13:57:20,259 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6998510360717773, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1508, 'eval_samples_per_second': 70.667, 'eval_steps_per_second': 8.833, 'epoch': 0.99}
2025-01-23 13:58:05,451 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.8217293620109558, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0778, 'eval_samples_per_second': 71.034, 'eval_steps_per_second': 8.879, 'epoch': 1.98}
2025-01-23 13:58:50,391 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 1.319516897201538, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1468, 'eval_samples_per_second': 70.688, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 1.2542622089385986, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1818, 'eval_samples_per_second': 70.513, 'eval_steps_per_second': 8.814, 'epoch': 4.0}
{'eval_loss': 1.5437227487564087, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.0821, 'eval_samples_per_second': 71.012, 'eval_steps_per_second': 8.877, 'epoch': 4.99}
{'eval_loss': 1.5853791236877441, 'eval_f1-score': {'Presence': 0.59}, 'eval_marco-avg-f1-score': 0.59, 'eval_runtime': 14.1464, 'eval_samples_per_second': 70.689, 'eval_steps_per_second': 8.836, 'epoch': 5.98}
{'eval_loss': 2.322319507598877, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.0858, 'eval_samples_per_second': 70.993, 'eval_steps_per_second': 8.874, 'epoch': 6.98}
{'train_runtime': 324.4664, 'train_samples_per_second': 30.82, 'train_steps_per_second': 0.955, 'train_loss': 0.4657632670271287, 'epoch': 6.98}
2025-01-23 14:02:45,006 - HVD - INFO - 

VALIDATION
2025-01-23 14:02:45,006 - HVD - INFO - ==========
2025-01-23 14:02:59,159 - HVD - INFO - Presence: 0.72
2025-01-23 14:02:59,159 - HVD - INFO - Macro average: 0.72
2025-01-23 14:03:14,114 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:03:14,343 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:03:14,343 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:03:15,151 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:03:15,684 - HVD - INFO - Using CUDA for training.
2025-01-23 14:03:15,801 - HVD - INFO - TRAINING
2025-01-23 14:03:15,801 - HVD - INFO - ========
2025-01-23 14:03:15,802 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.021666694968169e-05
Weight decay: 0.21699867157470615
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:03:15,804 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7019901275634766, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1433, 'eval_samples_per_second': 70.705, 'eval_steps_per_second': 8.838, 'epoch': 0.99}
2025-01-23 14:04:00,979 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7039216756820679, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.0729, 'eval_samples_per_second': 71.059, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 14:04:45,905 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.747342586517334, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1466, 'eval_samples_per_second': 70.688, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.7075409293174744, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.1062, 'eval_samples_per_second': 70.891, 'eval_steps_per_second': 8.861, 'epoch': 4.0}
{'eval_loss': 0.7410385608673096, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.163, 'eval_samples_per_second': 70.607, 'eval_steps_per_second': 8.826, 'epoch': 4.99}
{'eval_loss': 0.773689329624176, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1509, 'eval_samples_per_second': 70.667, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 0.7658094763755798, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1517, 'eval_samples_per_second': 70.663, 'eval_steps_per_second': 8.833, 'epoch': 6.98}
{'train_runtime': 324.2225, 'train_samples_per_second': 30.843, 'train_steps_per_second': 0.956, 'train_loss': 0.6194439809256738, 'epoch': 6.98}
2025-01-23 14:08:40,295 - HVD - INFO - 

VALIDATION
2025-01-23 14:08:40,295 - HVD - INFO - ==========
2025-01-23 14:08:54,374 - HVD - INFO - Presence: 0.72
2025-01-23 14:08:54,374 - HVD - INFO - Macro average: 0.72
2025-01-23 14:09:09,296 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:09:09,540 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:09:09,540 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:09:10,340 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:09:10,849 - HVD - INFO - Using CUDA for training.
2025-01-23 14:09:10,966 - HVD - INFO - TRAINING
2025-01-23 14:09:10,967 - HVD - INFO - ========
2025-01-23 14:09:10,967 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.84687816139961e-05
Weight decay: 0.12330102297072153
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:09:10,969 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7072451114654541, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.148, 'eval_samples_per_second': 70.682, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 14:09:56,149 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7883266806602478, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0728, 'eval_samples_per_second': 71.059, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 14:10:41,079 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7982310652732849, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1467, 'eval_samples_per_second': 70.688, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.8280259966850281, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1792, 'eval_samples_per_second': 70.526, 'eval_steps_per_second': 8.816, 'epoch': 4.0}
{'eval_loss': 0.8872743844985962, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.0766, 'eval_samples_per_second': 71.04, 'eval_steps_per_second': 8.88, 'epoch': 4.99}
{'eval_loss': 0.881773054599762, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.1548, 'eval_samples_per_second': 70.648, 'eval_steps_per_second': 8.831, 'epoch': 5.98}
{'eval_loss': 1.060880422592163, 'eval_f1-score': {'Presence': 0.63}, 'eval_marco-avg-f1-score': 0.63, 'eval_runtime': 14.1558, 'eval_samples_per_second': 70.642, 'eval_steps_per_second': 8.83, 'epoch': 6.98}
{'train_runtime': 324.4862, 'train_samples_per_second': 30.818, 'train_steps_per_second': 0.955, 'train_loss': 0.5513999658987063, 'epoch': 6.98}
2025-01-23 14:14:35,721 - HVD - INFO - 

VALIDATION
2025-01-23 14:14:35,721 - HVD - INFO - ==========
2025-01-23 14:14:49,797 - HVD - INFO - Presence: 0.72
2025-01-23 14:14:49,797 - HVD - INFO - Macro average: 0.72
2025-01-23 14:15:04,678 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:15:04,947 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:15:04,947 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:15:05,756 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:15:06,397 - HVD - INFO - Using CUDA for training.
2025-01-23 14:15:06,513 - HVD - INFO - TRAINING
2025-01-23 14:15:06,513 - HVD - INFO - ========
2025-01-23 14:15:06,514 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.9752320484503534e-05
Weight decay: 0.11321770863151705
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:15:06,516 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6923182010650635, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1493, 'eval_samples_per_second': 70.675, 'eval_steps_per_second': 8.834, 'epoch': 0.99}
2025-01-23 14:15:51,697 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7886901497840881, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0733, 'eval_samples_per_second': 71.056, 'eval_steps_per_second': 8.882, 'epoch': 1.98}
2025-01-23 14:16:36,621 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.9476494789123535, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1467, 'eval_samples_per_second': 70.688, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 1.0504382848739624, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.1025, 'eval_samples_per_second': 70.909, 'eval_steps_per_second': 8.864, 'epoch': 4.0}
{'eval_loss': 1.1417862176895142, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1549, 'eval_samples_per_second': 70.647, 'eval_steps_per_second': 8.831, 'epoch': 4.99}
{'eval_loss': 1.245315432548523, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1483, 'eval_samples_per_second': 70.68, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 1.6425600051879883, 'eval_f1-score': {'Presence': 0.63}, 'eval_marco-avg-f1-score': 0.63, 'eval_runtime': 14.1644, 'eval_samples_per_second': 70.599, 'eval_steps_per_second': 8.825, 'epoch': 6.98}
{'train_runtime': 324.2226, 'train_samples_per_second': 30.843, 'train_steps_per_second': 0.956, 'train_loss': 0.4849123473561138, 'epoch': 6.98}
2025-01-23 14:20:31,004 - HVD - INFO - 

VALIDATION
2025-01-23 14:20:31,005 - HVD - INFO - ==========
2025-01-23 14:20:45,083 - HVD - INFO - Presence: 0.71
2025-01-23 14:20:45,084 - HVD - INFO - Macro average: 0.71
2025-01-23 14:21:00,002 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:21:00,349 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:21:00,349 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:21:01,154 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:21:01,671 - HVD - INFO - Using CUDA for training.
2025-01-23 14:21:01,788 - HVD - INFO - TRAINING
2025-01-23 14:21:01,788 - HVD - INFO - ========
2025-01-23 14:21:01,788 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 6.120396330995253e-06
Weight decay: 0.20695238351618933
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:21:01,791 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7588451504707336, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1449, 'eval_samples_per_second': 70.697, 'eval_steps_per_second': 8.837, 'epoch': 0.99}
2025-01-23 14:21:46,973 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.694387674331665, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0726, 'eval_samples_per_second': 71.06, 'eval_steps_per_second': 8.883, 'epoch': 1.98}
2025-01-23 14:22:31,907 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7214306592941284, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.147, 'eval_samples_per_second': 70.686, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.7147194147109985, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1758, 'eval_samples_per_second': 70.543, 'eval_steps_per_second': 8.818, 'epoch': 4.0}
{'eval_loss': 0.699190080165863, 'eval_f1-score': {'Presence': 0.55}, 'eval_marco-avg-f1-score': 0.55, 'eval_runtime': 14.0756, 'eval_samples_per_second': 71.045, 'eval_steps_per_second': 8.881, 'epoch': 4.99}
{'eval_loss': 0.6980701684951782, 'eval_f1-score': {'Presence': 0.3}, 'eval_marco-avg-f1-score': 0.3, 'eval_runtime': 14.1469, 'eval_samples_per_second': 70.687, 'eval_steps_per_second': 8.836, 'epoch': 5.98}
{'eval_loss': 0.7205913066864014, 'eval_f1-score': {'Presence': 0.53}, 'eval_marco-avg-f1-score': 0.53, 'eval_runtime': 14.1647, 'eval_samples_per_second': 70.598, 'eval_steps_per_second': 8.825, 'epoch': 6.98}
{'train_runtime': 324.501, 'train_samples_per_second': 30.817, 'train_steps_per_second': 0.955, 'train_loss': 0.6766240531151447, 'epoch': 6.98}
2025-01-23 14:26:26,560 - HVD - INFO - 

VALIDATION
2025-01-23 14:26:26,560 - HVD - INFO - ==========
2025-01-23 14:26:40,636 - HVD - INFO - Presence: 0.71
2025-01-23 14:26:40,637 - HVD - INFO - Macro average: 0.71
2025-01-23 14:26:55,484 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:26:55,723 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:26:55,723 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:26:56,526 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:26:57,035 - HVD - INFO - Using CUDA for training.
2025-01-23 14:26:57,152 - HVD - INFO - TRAINING
2025-01-23 14:26:57,152 - HVD - INFO - ========
2025-01-23 14:26:57,152 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 4.993025767696698e-05
Weight decay: 0.13998337109237907
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:26:57,155 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6949983239173889, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.155, 'eval_samples_per_second': 70.646, 'eval_steps_per_second': 8.831, 'epoch': 0.99}
2025-01-23 14:27:42,366 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7667363882064819, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0796, 'eval_samples_per_second': 71.025, 'eval_steps_per_second': 8.878, 'epoch': 1.98}
2025-01-23 14:28:27,317 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8984229564666748, 'eval_f1-score': {'Presence': 0.71}, 'eval_marco-avg-f1-score': 0.71, 'eval_runtime': 14.1493, 'eval_samples_per_second': 70.675, 'eval_steps_per_second': 8.834, 'epoch': 2.98}
{'eval_loss': 1.3389077186584473, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1766, 'eval_samples_per_second': 70.539, 'eval_steps_per_second': 8.817, 'epoch': 4.0}
{'eval_loss': 1.100142240524292, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.0846, 'eval_samples_per_second': 71.0, 'eval_steps_per_second': 8.875, 'epoch': 4.99}
{'eval_loss': 1.5038352012634277, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1516, 'eval_samples_per_second': 70.664, 'eval_steps_per_second': 8.833, 'epoch': 5.98}
{'eval_loss': 3.642643690109253, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.0806, 'eval_samples_per_second': 71.02, 'eval_steps_per_second': 8.877, 'epoch': 6.98}
{'train_runtime': 324.5857, 'train_samples_per_second': 30.809, 'train_steps_per_second': 0.955, 'train_loss': 0.4514842252118872, 'epoch': 6.98}
2025-01-23 14:32:22,008 - HVD - INFO - 

VALIDATION
2025-01-23 14:32:22,008 - HVD - INFO - ==========
2025-01-23 14:32:36,157 - HVD - INFO - Presence: 0.71
2025-01-23 14:32:36,157 - HVD - INFO - Macro average: 0.71
2025-01-23 14:32:51,111 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:32:51,352 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:32:51,352 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:32:52,164 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:32:52,695 - HVD - INFO - Using CUDA for training.
2025-01-23 14:32:52,812 - HVD - INFO - TRAINING
2025-01-23 14:32:52,812 - HVD - INFO - ========
2025-01-23 14:32:52,812 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.513714438619972e-05
Weight decay: 0.1510540599081611
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:32:52,815 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7092820405960083, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1478, 'eval_samples_per_second': 70.683, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 14:33:38,020 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7718172669410706, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0742, 'eval_samples_per_second': 71.052, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 14:34:22,965 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8682818412780762, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1472, 'eval_samples_per_second': 70.686, 'eval_steps_per_second': 8.836, 'epoch': 2.98}
{'eval_loss': 0.9741681814193726, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 14.1828, 'eval_samples_per_second': 70.508, 'eval_steps_per_second': 8.813, 'epoch': 4.0}
{'eval_loss': 1.0869516134262085, 'eval_f1-score': {'Presence': 0.58}, 'eval_marco-avg-f1-score': 0.58, 'eval_runtime': 14.0791, 'eval_samples_per_second': 71.027, 'eval_steps_per_second': 8.878, 'epoch': 4.99}
{'eval_loss': 1.138271450996399, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1488, 'eval_samples_per_second': 70.677, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 1.4091054201126099, 'eval_f1-score': {'Presence': 0.58}, 'eval_marco-avg-f1-score': 0.58, 'eval_runtime': 14.0821, 'eval_samples_per_second': 71.012, 'eval_steps_per_second': 8.877, 'epoch': 6.98}
{'train_runtime': 324.5359, 'train_samples_per_second': 30.813, 'train_steps_per_second': 0.955, 'train_loss': 0.4940216344430906, 'epoch': 6.98}
2025-01-23 14:38:17,620 - HVD - INFO - 

VALIDATION
2025-01-23 14:38:17,620 - HVD - INFO - ==========
2025-01-23 14:38:31,772 - HVD - INFO - Presence: 0.64
2025-01-23 14:38:31,772 - HVD - INFO - Macro average: 0.64
2025-01-23 14:38:46,728 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:38:46,969 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:38:46,970 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:38:47,775 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:38:48,289 - HVD - INFO - Using CUDA for training.
2025-01-23 14:38:48,406 - HVD - INFO - TRAINING
2025-01-23 14:38:48,406 - HVD - INFO - ========
2025-01-23 14:38:48,406 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 4.3782388679485083e-05
Weight decay: 0.1693202267941346
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:38:48,408 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7042896747589111, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1488, 'eval_samples_per_second': 70.677, 'eval_steps_per_second': 8.835, 'epoch': 0.99}
2025-01-23 14:39:33,600 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.888134777545929, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0761, 'eval_samples_per_second': 71.042, 'eval_steps_per_second': 8.88, 'epoch': 1.98}
2025-01-23 14:40:18,534 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8465354442596436, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1525, 'eval_samples_per_second': 70.659, 'eval_steps_per_second': 8.832, 'epoch': 2.98}
{'eval_loss': 1.078985333442688, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1863, 'eval_samples_per_second': 70.49, 'eval_steps_per_second': 8.811, 'epoch': 4.0}
{'eval_loss': 1.109815239906311, 'eval_f1-score': {'Presence': 0.52}, 'eval_marco-avg-f1-score': 0.52, 'eval_runtime': 14.0863, 'eval_samples_per_second': 70.991, 'eval_steps_per_second': 8.874, 'epoch': 4.99}
{'eval_loss': 1.818720817565918, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 14.1578, 'eval_samples_per_second': 70.632, 'eval_steps_per_second': 8.829, 'epoch': 5.98}
{'eval_loss': 2.4185400009155273, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1484, 'eval_samples_per_second': 70.68, 'eval_steps_per_second': 8.835, 'epoch': 6.98}
{'eval_loss': 2.91416335105896, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.0988, 'eval_samples_per_second': 70.928, 'eval_steps_per_second': 8.866, 'epoch': 8.0}
{'train_runtime': 371.4407, 'train_samples_per_second': 26.922, 'train_steps_per_second': 0.835, 'train_loss': 0.38104083251953125, 'epoch': 8.0}
2025-01-23 14:45:00,128 - HVD - INFO - 

VALIDATION
2025-01-23 14:45:00,128 - HVD - INFO - ==========
2025-01-23 14:45:14,276 - HVD - INFO - Presence: 0.67
2025-01-23 14:45:14,276 - HVD - INFO - Macro average: 0.67
2025-01-23 14:45:29,305 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:45:29,548 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:45:29,548 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:45:30,347 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:45:30,940 - HVD - INFO - Using CUDA for training.
2025-01-23 14:45:31,056 - HVD - INFO - TRAINING
2025-01-23 14:45:31,056 - HVD - INFO - ========
2025-01-23 14:45:31,056 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.083735168724116e-05
Weight decay: 0.13456981901674114
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:45:31,059 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6869073510169983, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1457, 'eval_samples_per_second': 70.693, 'eval_steps_per_second': 8.837, 'epoch': 0.99}
2025-01-23 14:46:16,234 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.8010572195053101, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0756, 'eval_samples_per_second': 71.045, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 14:47:01,164 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8147070407867432, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.143, 'eval_samples_per_second': 70.707, 'eval_steps_per_second': 8.838, 'epoch': 2.98}
{'eval_loss': 0.8420407772064209, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1704, 'eval_samples_per_second': 70.569, 'eval_steps_per_second': 8.821, 'epoch': 4.0}
{'eval_loss': 0.9026939272880554, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0859, 'eval_samples_per_second': 70.993, 'eval_steps_per_second': 8.874, 'epoch': 4.99}
{'eval_loss': 0.9070318937301636, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1489, 'eval_samples_per_second': 70.677, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 1.0734143257141113, 'eval_f1-score': {'Presence': 0.67}, 'eval_marco-avg-f1-score': 0.67, 'eval_runtime': 14.1499, 'eval_samples_per_second': 70.672, 'eval_steps_per_second': 8.834, 'epoch': 6.98}
{'train_runtime': 324.4814, 'train_samples_per_second': 30.818, 'train_steps_per_second': 0.955, 'train_loss': 0.5554041731248208, 'epoch': 6.98}
2025-01-23 14:50:55,806 - HVD - INFO - 

VALIDATION
2025-01-23 14:50:55,806 - HVD - INFO - ==========
2025-01-23 14:51:09,886 - HVD - INFO - Presence: 0.72
2025-01-23 14:51:09,886 - HVD - INFO - Macro average: 0.72
2025-01-23 14:51:24,768 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:51:25,012 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:51:25,012 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:51:25,820 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:51:26,937 - HVD - INFO - Using CUDA for training.
2025-01-23 14:51:27,054 - HVD - INFO - TRAINING
2025-01-23 14:51:27,054 - HVD - INFO - ========
2025-01-23 14:51:27,054 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.4988861758364553e-05
Weight decay: 0.1581214819905428
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:51:27,056 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6840801239013672, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1505, 'eval_samples_per_second': 70.669, 'eval_steps_per_second': 8.834, 'epoch': 0.99}
2025-01-23 14:52:12,249 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6942631602287292, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0756, 'eval_samples_per_second': 71.045, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 14:52:57,188 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7122393846511841, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1452, 'eval_samples_per_second': 70.695, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 1.2280479669570923, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1018, 'eval_samples_per_second': 70.913, 'eval_steps_per_second': 8.864, 'epoch': 4.0}
{'eval_loss': 1.0712618827819824, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1481, 'eval_samples_per_second': 70.681, 'eval_steps_per_second': 8.835, 'epoch': 4.99}
{'eval_loss': 1.1947541236877441, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.161, 'eval_samples_per_second': 70.617, 'eval_steps_per_second': 8.827, 'epoch': 5.98}
{'eval_loss': 1.1434603929519653, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1505, 'eval_samples_per_second': 70.669, 'eval_steps_per_second': 8.834, 'epoch': 6.98}
{'train_runtime': 324.2158, 'train_samples_per_second': 30.844, 'train_steps_per_second': 0.956, 'train_loss': 0.550467219921427, 'epoch': 6.98}
2025-01-23 14:56:51,551 - HVD - INFO - 

VALIDATION
2025-01-23 14:56:51,551 - HVD - INFO - ==========
2025-01-23 14:57:05,628 - HVD - INFO - Presence: 0.72
2025-01-23 14:57:05,628 - HVD - INFO - Macro average: 0.72
2025-01-23 14:57:20,540 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 14:57:20,787 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 14:57:20,787 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 14:57:21,589 - HVD - INFO - Arguments validated successfully.
2025-01-23 14:57:22,158 - HVD - INFO - Using CUDA for training.
2025-01-23 14:57:22,275 - HVD - INFO - TRAINING
2025-01-23 14:57:22,275 - HVD - INFO - ========
2025-01-23 14:57:22,275 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 3.11748486059552e-05
Weight decay: 0.14482456994335666
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 14:57:22,277 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7246273159980774, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1454, 'eval_samples_per_second': 70.695, 'eval_steps_per_second': 8.837, 'epoch': 0.99}
2025-01-23 14:58:07,453 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6853656768798828, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0717, 'eval_samples_per_second': 71.064, 'eval_steps_per_second': 8.883, 'epoch': 1.98}
2025-01-23 14:58:52,376 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8442432880401611, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1414, 'eval_samples_per_second': 70.714, 'eval_steps_per_second': 8.839, 'epoch': 2.98}
{'eval_loss': 1.1368287801742554, 'eval_f1-score': {'Presence': 0.69}, 'eval_marco-avg-f1-score': 0.69, 'eval_runtime': 14.1704, 'eval_samples_per_second': 70.57, 'eval_steps_per_second': 8.821, 'epoch': 4.0}
{'eval_loss': 1.6990221738815308, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.0751, 'eval_samples_per_second': 71.048, 'eval_steps_per_second': 8.881, 'epoch': 4.99}
{'eval_loss': 1.2686101198196411, 'eval_f1-score': {'Presence': 0.58}, 'eval_marco-avg-f1-score': 0.58, 'eval_runtime': 14.1504, 'eval_samples_per_second': 70.669, 'eval_steps_per_second': 8.834, 'epoch': 5.98}
{'eval_loss': 1.9660345315933228, 'eval_f1-score': {'Presence': 0.58}, 'eval_marco-avg-f1-score': 0.58, 'eval_runtime': 14.1522, 'eval_samples_per_second': 70.66, 'eval_steps_per_second': 8.833, 'epoch': 6.98}
{'train_runtime': 324.461, 'train_samples_per_second': 30.82, 'train_steps_per_second': 0.955, 'train_loss': 0.5054589017815546, 'epoch': 6.98}
2025-01-23 15:02:47,004 - HVD - INFO - 

VALIDATION
2025-01-23 15:02:47,004 - HVD - INFO - ==========
2025-01-23 15:03:01,085 - HVD - INFO - Presence: 0.72
2025-01-23 15:03:01,085 - HVD - INFO - Macro average: 0.72
2025-01-23 15:03:15,939 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 15:03:16,178 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 15:03:16,178 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 15:03:16,989 - HVD - INFO - Arguments validated successfully.
2025-01-23 15:03:17,499 - HVD - INFO - Using CUDA for training.
2025-01-23 15:03:17,617 - HVD - INFO - TRAINING
2025-01-23 15:03:17,617 - HVD - INFO - ========
2025-01-23 15:03:17,617 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.5474113095979414e-05
Weight decay: 0.18831824656952378
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 15:03:17,620 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6956737041473389, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.1447, 'eval_samples_per_second': 70.698, 'eval_steps_per_second': 8.837, 'epoch': 0.99}
2025-01-23 15:04:02,801 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.6985284686088562, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0725, 'eval_samples_per_second': 71.06, 'eval_steps_per_second': 8.883, 'epoch': 1.98}
2025-01-23 15:04:47,730 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7628079652786255, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1449, 'eval_samples_per_second': 70.697, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.7177867293357849, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 14.1003, 'eval_samples_per_second': 70.92, 'eval_steps_per_second': 8.865, 'epoch': 4.0}
{'eval_loss': 0.7608118653297424, 'eval_f1-score': {'Presence': 0.65}, 'eval_marco-avg-f1-score': 0.65, 'eval_runtime': 14.1519, 'eval_samples_per_second': 70.662, 'eval_steps_per_second': 8.833, 'epoch': 4.99}
{'eval_loss': 0.8106605410575867, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 14.1529, 'eval_samples_per_second': 70.657, 'eval_steps_per_second': 8.832, 'epoch': 5.98}
{'eval_loss': 0.8673178553581238, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 14.1511, 'eval_samples_per_second': 70.666, 'eval_steps_per_second': 8.833, 'epoch': 6.98}
{'train_runtime': 324.2274, 'train_samples_per_second': 30.843, 'train_steps_per_second': 0.956, 'train_loss': 0.5737534969224842, 'epoch': 6.98}
2025-01-23 15:08:42,115 - HVD - INFO - 

VALIDATION
2025-01-23 15:08:42,115 - HVD - INFO - ==========
2025-01-23 15:08:56,196 - HVD - INFO - Presence: 0.72
2025-01-23 15:08:56,196 - HVD - INFO - Macro average: 0.72
2025-01-23 15:09:11,160 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 15:09:11,404 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 15:09:11,404 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 15:09:12,199 - HVD - INFO - Arguments validated successfully.
2025-01-23 15:09:12,707 - HVD - INFO - Using CUDA for training.
2025-01-23 15:09:12,826 - HVD - INFO - TRAINING
2025-01-23 15:09:12,826 - HVD - INFO - ========
2025-01-23 15:09:12,826 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 1.8812523546740234e-05
Weight decay: 0.12183682203760264
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 15:09:12,828 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.6863396167755127, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1495, 'eval_samples_per_second': 70.674, 'eval_steps_per_second': 8.834, 'epoch': 0.99}
2025-01-23 15:09:58,040 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7385506629943848, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0778, 'eval_samples_per_second': 71.034, 'eval_steps_per_second': 8.879, 'epoch': 1.98}
2025-01-23 15:10:42,990 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.7354369163513184, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1524, 'eval_samples_per_second': 70.659, 'eval_steps_per_second': 8.832, 'epoch': 2.98}
{'eval_loss': 0.8641698360443115, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1771, 'eval_samples_per_second': 70.536, 'eval_steps_per_second': 8.817, 'epoch': 4.0}
{'eval_loss': 0.6941452622413635, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0844, 'eval_samples_per_second': 71.0, 'eval_steps_per_second': 8.875, 'epoch': 4.99}
{'eval_loss': 0.6863412857055664, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.154, 'eval_samples_per_second': 70.652, 'eval_steps_per_second': 8.831, 'epoch': 5.98}
{'eval_loss': 0.6988707780838013, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0809, 'eval_samples_per_second': 71.018, 'eval_steps_per_second': 8.877, 'epoch': 6.98}
{'train_runtime': 324.6271, 'train_samples_per_second': 30.805, 'train_steps_per_second': 0.955, 'train_loss': 0.657497441003082, 'epoch': 6.98}
2025-01-23 15:14:37,734 - HVD - INFO - 

VALIDATION
2025-01-23 15:14:37,735 - HVD - INFO - ==========
2025-01-23 15:14:51,895 - HVD - INFO - Presence: 0.72
2025-01-23 15:14:51,895 - HVD - INFO - Macro average: 0.72
2025-01-23 15:15:06,858 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-23 15:15:07,100 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-23 15:15:07,100 - HVD - INFO - Preparing datasets for training and validation
2025-01-23 15:15:07,907 - HVD - INFO - Arguments validated successfully.
2025-01-23 15:15:08,429 - HVD - INFO - Using CUDA for training.
2025-01-23 15:15:08,547 - HVD - INFO - TRAINING
2025-01-23 15:15:08,547 - HVD - INFO - ========
2025-01-23 15:15:08,547 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 10
Learning rate: 2.8021658113613452e-05
Weight decay: 0.10043472068795961
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0

2025-01-23 15:15:08,550 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'eval_loss': 0.7233155369758606, 'eval_f1-score': {'Presence': 0.7}, 'eval_marco-avg-f1-score': 0.7, 'eval_runtime': 14.1504, 'eval_samples_per_second': 70.669, 'eval_steps_per_second': 8.834, 'epoch': 0.99}
2025-01-23 15:15:53,742 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'eval_loss': 0.7497053146362305, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.0753, 'eval_samples_per_second': 71.046, 'eval_steps_per_second': 8.881, 'epoch': 1.98}
2025-01-23 15:16:38,669 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.8611264824867249, 'eval_f1-score': {'Presence': 0.72}, 'eval_marco-avg-f1-score': 0.72, 'eval_runtime': 14.1457, 'eval_samples_per_second': 70.693, 'eval_steps_per_second': 8.837, 'epoch': 2.98}
{'eval_loss': 0.8154476284980774, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.1852, 'eval_samples_per_second': 70.496, 'eval_steps_per_second': 8.812, 'epoch': 4.0}
{'eval_loss': 0.8645440936088562, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 14.0866, 'eval_samples_per_second': 70.989, 'eval_steps_per_second': 8.874, 'epoch': 4.99}
{'eval_loss': 0.9493924975395203, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1483, 'eval_samples_per_second': 70.68, 'eval_steps_per_second': 8.835, 'epoch': 5.98}
{'eval_loss': 1.0397014617919922, 'eval_f1-score': {'Presence': 0.66}, 'eval_marco-avg-f1-score': 0.66, 'eval_runtime': 14.1504, 'eval_samples_per_second': 70.669, 'eval_steps_per_second': 8.834, 'epoch': 6.98}
{'train_runtime': 324.5074, 'train_samples_per_second': 30.816, 'train_steps_per_second': 0.955, 'train_loss': 0.5548834494494517, 'epoch': 6.98}
2025-01-23 15:20:33,337 - HVD - INFO - 

VALIDATION
2025-01-23 15:20:33,337 - HVD - INFO - ==========
2025-01-23 15:20:47,414 - HVD - INFO - Presence: 0.72
2025-01-23 15:20:47,414 - HVD - INFO - Macro average: 0.72
2025-01-23 15:21:02,294 - HVD - INFO - Best value: 0.72
2025-01-23 15:21:02,294 - HVD - INFO - Best params: {'learning_rate': 1.678920284036235e-05, 'weight_decay': 0.13712088363727098}
