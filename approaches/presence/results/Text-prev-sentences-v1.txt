2024-12-13 01:22:30,706 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2024-12-13 01:22:30,973 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2024-12-13 01:22:30,974 - HVD - INFO - Preparing datasets for training and validation
2024-12-13 01:22:49,304 - HVD - INFO - Arguments validated successfully.
2024-12-13 01:22:49,944 - HVD - INFO - Using CUDA for training.
2024-12-13 01:22:51,206 - HVD - INFO - TRAINING
2024-12-13 01:22:51,206 - HVD - INFO - ========
2024-12-13 01:22:51,206 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 2
Number of epochs: 7
Learning rate: 4.7791613843996165e-05
Weight decay: 2.3667239330748746e-08
Gradient accumulation steps: 4
Early stopping patience: 3
Previous sentences used: Yes
Using lexicon: No
Adding linguistic features: No
Number of categories (lexicon): 0

2024-12-13 01:22:51,211 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6964, 'grad_norm': 36638.80859375, 'learning_rate': 4.6571132362715296e-05, 'epoch': 0.18}
{'loss': 0.6927, 'grad_norm': 34516.38671875, 'learning_rate': 4.5350650881434434e-05, 'epoch': 0.36}
{'loss': 0.6932, 'grad_norm': 40603.38671875, 'learning_rate': 4.4130169400153565e-05, 'epoch': 0.54}
{'loss': 0.6936, 'grad_norm': 70132.234375, 'learning_rate': 4.29096879188727e-05, 'epoch': 0.71}
{'loss': 0.6936, 'grad_norm': 23034.935546875, 'learning_rate': 4.1689206437591834e-05, 'epoch': 0.89}
4
{'eval_loss': 0.6948633193969727, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 503.3275, 'eval_samples_per_second': 29.611, 'eval_steps_per_second': 7.403, 'epoch': 1.0}
{'loss': 0.6931, 'grad_norm': 45373.28515625, 'learning_rate': 4.0468724956310965e-05, 'epoch': 1.07}
{'loss': 0.6938, 'grad_norm': 45362.25, 'learning_rate': 3.92482434750301e-05, 'epoch': 1.25}
{'loss': 0.694, 'grad_norm': 26960.703125, 'learning_rate': 3.8027761993749234e-05, 'epoch': 1.43}
{'loss': 0.693, 'grad_norm': 76459.4921875, 'learning_rate': 3.680728051246837e-05, 'epoch': 1.61}
{'loss': 0.693, 'grad_norm': 79997.515625, 'learning_rate': 3.55867990311875e-05, 'epoch': 1.79}
{'loss': 0.6928, 'grad_norm': 47466.48046875, 'learning_rate': 3.4366317549906634e-05, 'epoch': 1.97}
4
{'eval_loss': 0.6932200789451599, 'eval_f1-score': {'Presence': 0}, 'eval_marco-avg-f1-score': 0.0, 'eval_runtime': 503.9408, 'eval_samples_per_second': 29.575, 'eval_steps_per_second': 7.394, 'epoch': 2.0}
{'loss': 0.6932, 'grad_norm': 68665.2421875, 'learning_rate': 3.314583606862577e-05, 'epoch': 2.14}
{'loss': 0.6933, 'grad_norm': 38181.234375, 'learning_rate': 3.19253545873449e-05, 'epoch': 2.32}
{'loss': 0.693, 'grad_norm': 69824.28125, 'learning_rate': 3.070487310606404e-05, 'epoch': 2.5}
{'loss': 0.693, 'grad_norm': 45830.9453125, 'learning_rate': 2.948439162478317e-05, 'epoch': 2.68}
{'loss': 0.6932, 'grad_norm': 57055.50390625, 'learning_rate': 2.8263910143502306e-05, 'epoch': 2.86}
4
{'eval_loss': 0.6929513216018677, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 503.91, 'eval_samples_per_second': 29.577, 'eval_steps_per_second': 7.394, 'epoch': 3.0}
{'loss': 0.6928, 'grad_norm': 44116.875, 'learning_rate': 2.704342866222144e-05, 'epoch': 3.04}
{'loss': 0.6928, 'grad_norm': 19295.103515625, 'learning_rate': 2.582294718094057e-05, 'epoch': 3.22}
{'loss': 0.6932, 'grad_norm': 60613.48046875, 'learning_rate': 2.4602465699659705e-05, 'epoch': 3.4}
{'loss': 0.6933, 'grad_norm': 26927.80859375, 'learning_rate': 2.3381984218378836e-05, 'epoch': 3.57}
{'loss': 0.693, 'grad_norm': 28713.599609375, 'learning_rate': 2.216150273709797e-05, 'epoch': 3.75}
{'loss': 0.6941, 'grad_norm': 80025.9765625, 'learning_rate': 2.0941021255817105e-05, 'epoch': 3.93}
4
{'eval_loss': 0.6929596066474915, 'eval_f1-score': {'Presence': 0.68}, 'eval_marco-avg-f1-score': 0.68, 'eval_runtime': 503.379, 'eval_samples_per_second': 29.608, 'eval_steps_per_second': 7.402, 'epoch': 4.0}
{'train_runtime': 16046.9763, 'train_samples_per_second': 19.524, 'train_steps_per_second': 1.22, 'train_loss': 0.6934528940591139, 'epoch': 4.0}
2024-12-13 05:50:18,714 - HVD - INFO - 

VALIDATION
2024-12-13 05:50:18,714 - HVD - INFO - ==========
4
2024-12-13 05:58:43,430 - HVD - INFO - Presence: 0.68
2024-12-13 05:58:43,430 - HVD - INFO - Macro average: 0.68
2024-12-13 05:58:44,032 - HVD - INFO - SAVE to models/Text-prev-sentences
