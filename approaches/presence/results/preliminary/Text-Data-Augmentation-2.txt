INFO:datasets:PyTorch version 2.5.1+cu118 available.
INFO:datasets:PyTorch version 2.5.1+cu118 available.
INFO:HVD:Initializing tokenizer for model: microsoft/deberta-base
INFO:HVD:Loading lexicon embeddings for: No lexicon used
INFO:HVD:Preparing datasets for training and validation
INFO:HVD:Initializing tokenizer for model: microsoft/deberta-base
INFO:HVD:Loading lexicon embeddings for: No lexicon used
INFO:HVD:Preparing datasets for training and validation
INFO:HVD:Arguments validated successfully.
INFO:HVD:Using CUDA for training.
INFO:HVD:Arguments validated successfully.
INFO:HVD:Using CUDA for training.
INFO:HVD:TRAINING
INFO:HVD:========
INFO:HVD:Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: Text-Data-Augmentation-2
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommende
d to upgrade the kernel to the minimum version or higher.
INFO:HVD:TRAINING
INFO:HVD:========
INFO:HVD:Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: Text-Data-Augmentation-2
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
{'loss': 5.5973, 'grad_norm': 51.034908294677734, 'learning_rate': 1.9670184696569924e-05, 'epoch': 0.16}
{'loss': 5.5066, 'grad_norm': 39.21952438354492, 'learning_rate': 1.9340369393139843e-05, 'epoch': 0.33}
{'loss': 5.3454, 'grad_norm': 30.493274688720703, 'learning_rate': 1.9010554089709766e-05, 'epoch': 0.49}
{'loss': 5.2968, 'grad_norm': 34.89100646972656, 'learning_rate': 1.8680738786279685e-05, 'epoch': 0.66}
{'loss': 5.2809, 'grad_norm': 14.916741371154785, 'learning_rate': 1.8350923482849604e-05, 'epoch': 0.82}
{'loss': 5.2006, 'grad_norm': 21.27871322631836, 'learning_rate': 1.8021108179419526e-05, 'epoch': 0.99}
INFO:HVD:Skipping evaluation for warm-up phase (epoch 2).
{'eval_loss': 0.6566560864448547, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 183.466, 'eval_samples_per_second': 81.236,
 'eval_steps_per_second': 20.309, 'epoch': 1.0}
 10%|███████████▋                                                                                                         | 3033/30320 [1:21:04<8:54:26,  1.18s/it]
INFO:HVD:Skipping evaluation for warm-up phase (epoch 2).
{'loss': 5.0061, 'grad_norm': 16.954383850097656, 'learning_rate': 1.769129287598945e-05, 'epoch': 1.15}
{'loss': 4.9569, 'grad_norm': 17.667978286743164, 'learning_rate': 1.7361477572559368e-05, 'epoch': 1.32}
{'loss': 4.9305, 'grad_norm': 33.41469955444336, 'learning_rate': 1.703166226912929e-05, 'epoch': 1.48}
{'loss': 4.8783, 'grad_norm': 18.47834587097168, 'learning_rate': 1.670184696569921e-05, 'epoch': 1.65}
{'loss': 4.8209, 'grad_norm': 57.69099044799805, 'learning_rate': 1.637203166226913e-05, 'epoch': 1.81}
{'loss': 4.7726, 'grad_norm': 24.74013328552246, 'learning_rate': 1.604221635883905e-05, 'epoch': 1.98}
{'eval_loss': 0.6911731958389282, 'eval_f1-score': {'Presence': 0.52}, 'eval_marco-avg-f1-score': 0.52, 'eval_runtime': 183.3404, 'eval_samples_per_second': 81.291
, 'eval_steps_per_second': 20.323, 'epoch': 2.0}
{'loss': 4.2539, 'grad_norm': 24.65362548828125, 'learning_rate': 1.5712401055408974e-05, 'epoch': 2.14}
{'loss': 4.1542, 'grad_norm': 79.84239196777344, 'learning_rate': 1.5382585751978893e-05, 'epoch': 2.31}
{'loss': 4.1358, 'grad_norm': 35.53221130371094, 'learning_rate': 1.5052770448548815e-05, 'epoch': 2.47}
{'loss': 4.0265, 'grad_norm': 60.36339569091797, 'learning_rate': 1.4722955145118736e-05, 'epoch': 2.64}
{'loss': 4.0065, 'grad_norm': 52.92121124267578, 'learning_rate': 1.4393139841688655e-05, 'epoch': 2.8}
{'loss': 3.9895, 'grad_norm': 42.34184265136719, 'learning_rate': 1.4063324538258576e-05, 'epoch': 2.97}
{'eval_loss': 0.785094141960144, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 183.6314, 'eval_samples_per_second': 81.163,
 'eval_steps_per_second': 20.291, 'epoch': 3.0}
{'loss': 3.4747, 'grad_norm': 38.28456115722656, 'learning_rate': 1.3733509234828497e-05, 'epoch': 3.13}
{'loss': 3.3525, 'grad_norm': 50.923744201660156, 'learning_rate': 1.3403693931398417e-05, 'epoch': 3.3}
{'loss': 3.215, 'grad_norm': 50.72881317138672, 'learning_rate': 1.307387862796834e-05, 'epoch': 3.46}
{'loss': 3.3114, 'grad_norm': 87.64287567138672, 'learning_rate': 1.274406332453826e-05, 'epoch': 3.63}
{'loss': 3.1764, 'grad_norm': 69.53890228271484, 'learning_rate': 1.241424802110818e-05, 'epoch': 3.79}
{'loss': 3.1535, 'grad_norm': 65.80313873291016, 'learning_rate': 1.20844327176781e-05, 'epoch': 3.96}
{'eval_loss': 0.952443540096283, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 183.6835, 'eval_samples_per_second': 81.14, 'eval_steps_per_second': 20.285, 'epoch': 4.0}
{'loss': 2.7131, 'grad_norm': 102.94445037841797, 'learning_rate': 1.1754617414248021e-05, 'epoch': 4.12}
{'loss': 2.5439, 'grad_norm': 44.22775650024414, 'learning_rate': 1.1424802110817944e-05, 'epoch': 4.29}
{'loss': 2.5605, 'grad_norm': 116.693359375, 'learning_rate': 1.1094986807387865e-05, 'epoch': 4.45}
{'loss': 2.4661, 'grad_norm': 36.97846603393555, 'learning_rate': 1.0765171503957785e-05, 'epoch': 4.62}
{'loss': 2.4621, 'grad_norm': 54.27267074584961, 'learning_rate': 1.0435356200527704e-05, 'epoch': 4.78}
{'loss': 2.4999, 'grad_norm': 67.42118835449219, 'learning_rate': 1.0105540897097625e-05, 'epoch': 4.95}
{'eval_loss': 1.0599702596664429, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 183.6972, 'eval_samples_per_second': 81.134, 'eval_steps_per_second': 20.283, 'epoch': 5.0}
 50%|██████████████████████████████████████████████████████████                                                          | 15165/30320 [6:46:05<4:56:51,  1.18s/it]Could not locate the best model at /tmp/tmpbwtiehhb/checkpoint-3033/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
Could not locate the best model at /tmp/tmpkrh0uy6i/checkpoint-3033/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
{'train_runtime': 24367.7135, 'train_samples_per_second': 39.818, 'train_steps_per_second': 1.244, 'train_loss': 4.0200308548511945, 'epoch': 5.0}
 50%|██████████████████████████████████████████████████████████                                                          | 15165/30320 [6:46:07<4:56:51,  1.18s/it]INFO:HVD:   

VALIDATION  
INFO:HVD:==========
 50%|██████████████████████████████████████████████████████████                                                          | 15165/30320 [6:46:07<6:45:51,  1.61s/it]
INFO:HVD:   

VALIDATION  
INFO:HVD:==========
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3726/3726 [03:03<00:00, 20.32it/s]INFO:HVD:Presence: 0.61
INFO:HVD:Macro average: 0.61
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3726/3726 [03:03<00:00, 20.29it/s]
INFO:HVD:Presence: 0.61
INFO:HVD:Macro average: 0.61
INFO:HVD:UPLOAD to https://huggingface.co/Text-Data-Augmentation-2 (using HF_TOKEN environment variable)
INFO:HVD:SAVE to models
INFO:HVD:UPLOAD to https://huggingface.co/Text-Data-Augmentation-2 (using HF_TOKEN environment variable)
INFO:HVD:SAVE to models
[rank0]:[W210 14:56:27.412916446 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())