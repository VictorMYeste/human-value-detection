2024-12-17 08:17:59,521 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2024-12-17 08:17:59,805 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2024-12-17 08:17:59,805 - HVD - INFO - Preparing datasets for training and validation
2024-12-17 08:18:21,403 - HVD - INFO - Arguments validated successfully.
2024-12-17 08:18:22,039 - HVD - INFO - Using CUDA for training.
2024-12-17 08:18:23,367 - HVD - INFO - TRAINING
2024-12-17 08:18:23,367 - HVD - INFO - ========
2024-12-17 08:18:23,367 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: Text-v12
Batch size: 4
Number of epochs: 10
Learning rate: 5e-06
Weight decay: 0.01
Gradient accumulation steps: 4
Early stopping patience: 7
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Number of categories (lexicon): 0

2024-12-17 08:18:23,373 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6727, 'grad_norm': 767957.0625, 'learning_rate': 4.821173104434908e-06, 'epoch': 0.36}
{'loss': 0.6489, 'grad_norm': 685788.0, 'learning_rate': 4.642346208869814e-06, 'epoch': 0.71}
{'eval_loss': 0.6461647748947144, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 329.3275, 'eval_samples_per_second': 45.256, 'eval_steps_per_second': 5.657, 'epoch': 1.0}
{'loss': 0.6399, 'grad_norm': 1048630.5, 'learning_rate': 4.4635193133047216e-06, 'epoch': 1.07}
{'loss': 0.6248, 'grad_norm': 811677.4375, 'learning_rate': 4.284692417739628e-06, 'epoch': 1.43}
{'loss': 0.6222, 'grad_norm': 788373.6875, 'learning_rate': 4.105865522174535e-06, 'epoch': 1.79}
{'eval_loss': 0.6553314924240112, 'eval_f1-score': {'Presence': 0.56}, 'eval_marco-avg-f1-score': 0.56, 'eval_runtime': 329.5988, 'eval_samples_per_second': 45.219, 'eval_steps_per_second': 5.652, 'epoch': 2.0}
{'loss': 0.6085, 'grad_norm': 1308576.5, 'learning_rate': 3.927038626609443e-06, 'epoch': 2.15}
{'loss': 0.594, 'grad_norm': 1604174.0, 'learning_rate': 3.7482117310443496e-06, 'epoch': 2.5}
{'loss': 0.5908, 'grad_norm': 1311512.75, 'learning_rate': 3.5693848354792566e-06, 'epoch': 2.86}
{'eval_loss': 0.6568030118942261, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 329.3437, 'eval_samples_per_second': 45.254, 'eval_steps_per_second': 5.657, 'epoch': 3.0}
{'loss': 0.5678, 'grad_norm': 2037538.0, 'learning_rate': 3.3905579399141635e-06, 'epoch': 3.22}
{'loss': 0.5616, 'grad_norm': 1760377.0, 'learning_rate': 3.21173104434907e-06, 'epoch': 3.58}
{'loss': 0.5573, 'grad_norm': 3069980.25, 'learning_rate': 3.0329041487839773e-06, 'epoch': 3.93}
{'eval_loss': 0.6780661344528198, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 329.3234, 'eval_samples_per_second': 45.256, 'eval_steps_per_second': 5.657, 'epoch': 4.0}
{'loss': 0.5278, 'grad_norm': 1444869.125, 'learning_rate': 2.854077253218884e-06, 'epoch': 4.29}
{'loss': 0.5221, 'grad_norm': 2252961.0, 'learning_rate': 2.6752503576537915e-06, 'epoch': 4.65}
{'eval_loss': 0.7106035947799683, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 329.3859, 'eval_samples_per_second': 45.248, 'eval_steps_per_second': 5.656, 'epoch': 5.0}
{'loss': 0.5167, 'grad_norm': 1653128.375, 'learning_rate': 2.4964234620886985e-06, 'epoch': 5.01}
{'loss': 0.4842, 'grad_norm': 3254999.75, 'learning_rate': 2.3175965665236054e-06, 'epoch': 5.36}
{'loss': 0.4893, 'grad_norm': 2200795.0, 'learning_rate': 2.1387696709585123e-06, 'epoch': 5.72}
{'eval_loss': 0.735341489315033, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 329.4168, 'eval_samples_per_second': 45.244, 'eval_steps_per_second': 5.655, 'epoch': 6.0}
{'loss': 0.4847, 'grad_norm': 1973365.625, 'learning_rate': 1.9599427753934196e-06, 'epoch': 6.08}
{'loss': 0.4492, 'grad_norm': 2215766.25, 'learning_rate': 1.7811158798283263e-06, 'epoch': 6.44}
{'loss': 0.4499, 'grad_norm': 3567794.75, 'learning_rate': 1.6022889842632332e-06, 'epoch': 6.79}
{'eval_loss': 0.7661882042884827, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 329.5102, 'eval_samples_per_second': 45.231, 'eval_steps_per_second': 5.654, 'epoch': 7.0}
{'loss': 0.4374, 'grad_norm': 2730753.25, 'learning_rate': 1.4234620886981404e-06, 'epoch': 7.15}
{'loss': 0.4226, 'grad_norm': 3132054.5, 'learning_rate': 1.2446351931330473e-06, 'epoch': 7.51}
{'loss': 0.4242, 'grad_norm': 2454457.25, 'learning_rate': 1.0658082975679544e-06, 'epoch': 7.87}
{'eval_loss': 0.8062093257904053, 'eval_f1-score': {'Presence': 0.59}, 'eval_marco-avg-f1-score': 0.59, 'eval_runtime': 329.6028, 'eval_samples_per_second': 45.218, 'eval_steps_per_second': 5.652, 'epoch': 8.0}
{'train_runtime': 20947.9612, 'train_samples_per_second': 21.366, 'train_steps_per_second': 0.667, 'train_loss': 0.5391251832801044, 'epoch': 8.0}
2024-12-17 14:07:31,487 - HVD - INFO - 

VALIDATION
2024-12-17 14:07:31,487 - HVD - INFO - ==========
2024-12-17 14:13:00,999 - HVD - INFO - Presence: 0.64
2024-12-17 14:13:00,999 - HVD - INFO - Macro average: 0.64
2024-12-17 14:13:02,175 - HVD - INFO - UPLOAD to https://huggingface.co/Text-v12 (using HF_TOKEN environment variable)
2024-12-17 14:13:02,175 - HVD - INFO - SAVE to models
