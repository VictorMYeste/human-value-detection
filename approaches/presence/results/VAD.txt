2024-12-13 05:58:50,437 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2024-12-13 05:58:50,906 - HVD - INFO - Loading lexicon embeddings for: VAD
2024-12-13 05:58:51,105 - HVD - INFO - Preparing datasets for training and validation
2024-12-13 05:59:09,438 - HVD - INFO - Arguments validated successfully.
2024-12-13 05:59:10,248 - HVD - INFO - Using CUDA for training.
2024-12-13 05:59:11,515 - HVD - INFO - TRAINING
2024-12-13 05:59:11,515 - HVD - INFO - ========
2024-12-13 05:59:11,515 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 4
Number of epochs: 7
Learning rate: 4.7791613843996165e-05
Weight decay: 2.3667239330748746e-08
Gradient accumulation steps: 2
Early stopping patience: 3
Previous sentences used: No
Using lexicon: VAD
Adding linguistic features: No
Number of categories (lexicon): 3

2024-12-13 05:59:11,519 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6957, 'grad_norm': 131370.8125, 'learning_rate': 4.6571132362715296e-05, 'epoch': 0.18}
{'loss': 0.6938, 'grad_norm': 51941.0, 'learning_rate': 4.5350650881434434e-05, 'epoch': 0.36}
{'loss': 0.6945, 'grad_norm': 93756.328125, 'learning_rate': 4.4130169400153565e-05, 'epoch': 0.54}
{'loss': 0.6967, 'grad_norm': 133546.71875, 'learning_rate': 4.29096879188727e-05, 'epoch': 0.71}
{'loss': 0.6933, 'grad_norm': 46856.8125, 'learning_rate': 4.1689206437591834e-05, 'epoch': 0.89}
4
{'eval_loss': 0.6933363080024719, 'eval_f1-score': {'Presence': 0}, 'eval_marco-avg-f1-score': 0.0, 'eval_runtime': 325.1529, 'eval_samples_per_second': 45.837, 'eval_steps_per_second': 5.73, 'epoch': 1.0}
{'loss': 0.6894, 'grad_norm': 99744.40625, 'learning_rate': 4.0468724956310965e-05, 'epoch': 1.07}
{'loss': 0.6952, 'grad_norm': 218670.265625, 'learning_rate': 3.92482434750301e-05, 'epoch': 1.25}
{'loss': 0.7007, 'grad_norm': 115437.7265625, 'learning_rate': 3.8027761993749234e-05, 'epoch': 1.43}
{'loss': 0.6954, 'grad_norm': 315418.96875, 'learning_rate': 3.680728051246837e-05, 'epoch': 1.61}
{'loss': 0.6945, 'grad_norm': 381316.5, 'learning_rate': 3.55867990311875e-05, 'epoch': 1.79}
{'loss': 0.6945, 'grad_norm': 121104.7578125, 'learning_rate': 3.4366317549906634e-05, 'epoch': 1.97}
4
{'eval_loss': 0.7093141078948975, 'eval_f1-score': {'Presence': 0}, 'eval_marco-avg-f1-score': 0.0, 'eval_runtime': 325.1749, 'eval_samples_per_second': 45.834, 'eval_steps_per_second': 5.729, 'epoch': 2.0}
{'loss': 0.69, 'grad_norm': 291400.03125, 'learning_rate': 3.314583606862577e-05, 'epoch': 2.14}
{'loss': 0.6924, 'grad_norm': 109045.171875, 'learning_rate': 3.19253545873449e-05, 'epoch': 2.32}
{'loss': 0.6923, 'grad_norm': 119368.9375, 'learning_rate': 3.070487310606404e-05, 'epoch': 2.5}
{'loss': 0.692, 'grad_norm': 208329.203125, 'learning_rate': 2.948439162478317e-05, 'epoch': 2.68}
{'loss': 0.6916, 'grad_norm': 203523.203125, 'learning_rate': 2.8263910143502306e-05, 'epoch': 2.86}
