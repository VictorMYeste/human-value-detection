2025-05-07 08:33:38,310 - INFO - PyTorch version 2.6.0+cu118 available.
2025-05-07 08:33:38,310 - INFO - PyTorch version 2.6.0+cu118 available.
2025-05-07 08:33:46,007 - INFO - Setting random seed to 42
2025-05-07 08:33:46,008 - INFO - Running training for labels: ['Openness to Change', 'Conservation']
2025-05-07 08:33:46,008 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-05-07 08:33:46,192 - INFO - Loading lexicon embeddings for: No lexicon used
2025-05-07 08:33:46,192 - INFO - Preparing datasets for training and validation
2025-05-07 08:34:25,447 - INFO - Arguments validated successfully.
2025-05-07 08:34:25,453 - INFO - Previous sentences or augmented data detected. Adjusting batch size: 4 -> 2 and gradient accumulation steps: 4 -> 8
2025-05-07 08:34:25,453 - INFO - Clearing old checkpoints in models/checkpoints
2025-05-07 08:34:27,506 - INFO - Using CUDA for training.
2025-05-07 08:34:28,738 - INFO - TRAINING
2025-05-07 08:34:28,738 - INFO - ========
2025-05-07 08:34:28,739 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Filter labels: []
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
ResidualBlock: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
Adding topic detection features: No
Applying token pruning: No

2025-05-07 08:34:28,741 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.3964, 'grad_norm': 2.8404040336608887, 'learning_rate': 1.9670184696569924e-05, 'epoch': 0.16}
{'loss': 0.3873, 'grad_norm': 1.9407377243041992, 'learning_rate': 1.9340369393139843e-05, 'epoch': 0.33}
{'loss': 0.3682, 'grad_norm': 0.9629511833190918, 'learning_rate': 1.9010554089709766e-05, 'epoch': 0.49}
{'loss': 0.3666, 'grad_norm': 1.2722448110580444, 'learning_rate': 1.8680738786279685e-05, 'epoch': 0.66}
{'loss': 0.3584, 'grad_norm': 2.6062183380126953, 'learning_rate': 1.8350923482849604e-05, 'epoch': 0.82}
{'loss': 0.3505, 'grad_norm': 3.9043214321136475, 'learning_rate': 1.8021108179419526e-05, 'epoch': 0.99}
{'eval_loss': 0.35164013504981995, 'eval_f1-score': {'Openness to Change': 0.06, 'Conservation': 0.27}, 'eval_macro-avg-f1-score': 0.16, 'eval_runtime': 186.9928, 'eval_samples_per_second': 79.704, 'eval_steps_per_second': 19.926, 'epoch': 1.0}
2025-05-07 10:00:55,010 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.2976, 'grad_norm': 4.035421371459961, 'learning_rate': 1.769129287598945e-05, 'epoch': 1.15}
{'loss': 0.2858, 'grad_norm': 4.457322120666504, 'learning_rate': 1.7361477572559368e-05, 'epoch': 1.32}
{'loss': 0.2709, 'grad_norm': 7.182798385620117, 'learning_rate': 1.703166226912929e-05, 'epoch': 1.48}
{'loss': 0.2671, 'grad_norm': 4.432902812957764, 'learning_rate': 1.670184696569921e-05, 'epoch': 1.65}
{'loss': 0.2474, 'grad_norm': 5.136764049530029, 'learning_rate': 1.637203166226913e-05, 'epoch': 1.81}
{'loss': 0.2428, 'grad_norm': 4.614516258239746, 'learning_rate': 1.604221635883905e-05, 'epoch': 1.98}
{'eval_loss': 0.4706231951713562, 'eval_f1-score': {'Openness to Change': 0.14, 'Conservation': 0.39}, 'eval_macro-avg-f1-score': 0.26, 'eval_runtime': 185.4917, 'eval_samples_per_second': 80.349, 'eval_steps_per_second': 20.087, 'epoch': 2.0}
2025-05-07 11:27:22,281 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.1637, 'grad_norm': 5.237955570220947, 'learning_rate': 1.5712401055408974e-05, 'epoch': 2.14}
{'loss': 0.1483, 'grad_norm': 5.399138927459717, 'learning_rate': 1.5382585751978893e-05, 'epoch': 2.31}
{'loss': 0.1395, 'grad_norm': 4.852710723876953, 'learning_rate': 1.5052770448548815e-05, 'epoch': 2.47}
{'loss': 0.1449, 'grad_norm': 10.045330047607422, 'learning_rate': 1.4722955145118736e-05, 'epoch': 2.64}
{'loss': 0.1326, 'grad_norm': 8.779192924499512, 'learning_rate': 1.4393139841688655e-05, 'epoch': 2.8}
{'loss': 0.1266, 'grad_norm': 7.6697001457214355, 'learning_rate': 1.4063324538258576e-05, 'epoch': 2.97}
{'eval_loss': 0.6632740497589111, 'eval_f1-score': {'Openness to Change': 0.19, 'Conservation': 0.4}, 'eval_macro-avg-f1-score': 0.3, 'eval_runtime': 186.5003, 'eval_samples_per_second': 79.914, 'eval_steps_per_second': 19.979, 'epoch': 3.0}
{'loss': 0.0812, 'grad_norm': 1.1549397706985474, 'learning_rate': 1.3733509234828497e-05, 'epoch': 3.13}
{'loss': 0.0749, 'grad_norm': 7.606285095214844, 'learning_rate': 1.3403693931398417e-05, 'epoch': 3.3}
{'loss': 0.0772, 'grad_norm': 7.5615620613098145, 'learning_rate': 1.307387862796834e-05, 'epoch': 3.46}
{'loss': 0.0763, 'grad_norm': 6.152196407318115, 'learning_rate': 1.274406332453826e-05, 'epoch': 3.63}
{'loss': 0.0699, 'grad_norm': 6.200289249420166, 'learning_rate': 1.241424802110818e-05, 'epoch': 3.79}
{'loss': 0.0649, 'grad_norm': 7.754605770111084, 'learning_rate': 1.20844327176781e-05, 'epoch': 3.96}
{'eval_loss': 0.8864732384681702, 'eval_f1-score': {'Openness to Change': 0.15, 'Conservation': 0.4}, 'eval_macro-avg-f1-score': 0.28, 'eval_runtime': 187.0325, 'eval_samples_per_second': 79.687, 'eval_steps_per_second': 19.922, 'epoch': 4.0}
{'loss': 0.0516, 'grad_norm': 1.334297776222229, 'learning_rate': 1.1754617414248021e-05, 'epoch': 4.12}
{'loss': 0.0469, 'grad_norm': 7.926774978637695, 'learning_rate': 1.1424802110817944e-05, 'epoch': 4.29}
{'loss': 0.0446, 'grad_norm': 7.157505512237549, 'learning_rate': 1.1094986807387865e-05, 'epoch': 4.45}
{'loss': 0.0473, 'grad_norm': 4.138248920440674, 'learning_rate': 1.0765171503957785e-05, 'epoch': 4.62}
{'loss': 0.0419, 'grad_norm': 2.0230298042297363, 'learning_rate': 1.0435356200527704e-05, 'epoch': 4.78}
{'loss': 0.0464, 'grad_norm': 2.4241414070129395, 'learning_rate': 1.0105540897097625e-05, 'epoch': 4.95}
{'eval_loss': 1.0347861051559448, 'eval_f1-score': {'Openness to Change': 0.16, 'Conservation': 0.37}, 'eval_macro-avg-f1-score': 0.26, 'eval_runtime': 186.4589, 'eval_samples_per_second': 79.932, 'eval_steps_per_second': 19.983, 'epoch': 5.0}
{'loss': 0.0301, 'grad_norm': 1.0565202236175537, 'learning_rate': 9.775725593667546e-06, 'epoch': 5.11}
{'loss': 0.032, 'grad_norm': 6.374502182006836, 'learning_rate': 9.445910290237469e-06, 'epoch': 5.28}
{'loss': 0.0306, 'grad_norm': 11.437522888183594, 'learning_rate': 9.11609498680739e-06, 'epoch': 5.44}
{'loss': 0.029, 'grad_norm': 0.014292714186012745, 'learning_rate': 8.786279683377308e-06, 'epoch': 5.61}
{'loss': 0.0314, 'grad_norm': 0.9992882013320923, 'learning_rate': 8.456464379947231e-06, 'epoch': 5.77}
{'loss': 0.03, 'grad_norm': 8.449752807617188, 'learning_rate': 8.126649076517152e-06, 'epoch': 5.94}
{'eval_loss': 1.2348408699035645, 'eval_f1-score': {'Openness to Change': 0.18, 'Conservation': 0.41}, 'eval_macro-avg-f1-score': 0.3, 'eval_runtime': 186.6112, 'eval_samples_per_second': 79.867, 'eval_steps_per_second': 19.967, 'epoch': 6.0}
{'loss': 0.0249, 'grad_norm': 8.166118621826172, 'learning_rate': 7.79683377308707e-06, 'epoch': 6.1}
{'loss': 0.0233, 'grad_norm': 1.4091168642044067, 'learning_rate': 7.4670184696569924e-06, 'epoch': 6.27}
{'loss': 0.0202, 'grad_norm': 9.168622016906738, 'learning_rate': 7.137203166226914e-06, 'epoch': 6.43}
{'loss': 0.0193, 'grad_norm': 11.613517761230469, 'learning_rate': 6.807387862796835e-06, 'epoch': 6.6}
{'loss': 0.0169, 'grad_norm': 0.1901329606771469, 'learning_rate': 6.477572559366755e-06, 'epoch': 6.76}
{'loss': 0.0199, 'grad_norm': 12.29749584197998, 'learning_rate': 6.1477572559366764e-06, 'epoch': 6.93}
{'eval_loss': 1.3126580715179443, 'eval_f1-score': {'Openness to Change': 0.16, 'Conservation': 0.37}, 'eval_macro-avg-f1-score': 0.26, 'eval_runtime': 185.7864, 'eval_samples_per_second': 80.221, 'eval_steps_per_second': 20.055, 'epoch': 7.0}
{'train_runtime': 36372.9844, 'train_samples_per_second': 26.676, 'train_steps_per_second': 0.834, 'train_loss': 0.13507163279738643, 'epoch': 7.0}
2025-05-07 18:40:42,196 - INFO - 

VALIDATION
2025-05-07 18:40:42,196 - INFO - ==========
2025-05-07 18:43:47,491 - INFO - Openness to Change: 0.19
2025-05-07 18:43:47,491 - INFO - Conservation: 0.40
2025-05-07 18:43:47,491 - INFO - Macro average: 0.30
2025-05-07 18:43:47,605 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
