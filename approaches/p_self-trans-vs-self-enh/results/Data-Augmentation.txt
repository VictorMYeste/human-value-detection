2025-04-06 09:48:06,722 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-06 09:48:06,722 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-06 09:48:13,122 - INFO - Setting random seed to 42
2025-04-06 09:48:13,123 - INFO - Running training for labels: ['Self-Transcendence', 'Self-Enhancement']
2025-04-06 09:48:13,123 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-04-06 09:48:14,096 - INFO - Loading lexicon embeddings for: No lexicon used
2025-04-06 09:48:14,096 - INFO - Preparing datasets for training and validation
2025-04-06 09:48:35,142 - INFO - Arguments validated successfully.
2025-04-06 09:48:35,147 - INFO - Previous sentences or augmented data detected. Adjusting batch size: 4 -> 2 and gradient accumulation steps: 4 -> 8
2025-04-06 09:48:35,147 - INFO - Clearing old checkpoints in models/checkpoints
2025-04-06 09:48:36,437 - INFO - Using CUDA for training.
2025-04-06 09:48:37,589 - INFO - TRAINING
2025-04-06 09:48:37,589 - INFO - ========
2025-04-06 09:48:37,589 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Filter labels: ['Presence']
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
Adding topic detection features: No
Applying token pruning: No

2025-04-06 09:48:37,592 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.5791, 'grad_norm': 4.513143539428711, 'learning_rate': 1.9361022364217256e-05, 'epoch': 0.32}
{'loss': 0.5563, 'grad_norm': 1.9193013906478882, 'learning_rate': 1.8722044728434506e-05, 'epoch': 0.64}
{'loss': 0.5424, 'grad_norm': 3.145251989364624, 'learning_rate': 1.808306709265176e-05, 'epoch': 0.96}
{'eval_loss': 0.49254468083381653, 'eval_f1-score': {'Self-Transcendence': 0.51, 'Self-Enhancement': 0.47}, 'eval_macro-avg-f1-score': 0.49, 'eval_runtime': 157.83, 'eval_samples_per_second': 48.153, 'eval_steps_per_second': 12.038, 'epoch': 1.0}
2025-04-06 11:03:32,494 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.4793, 'grad_norm': 6.334065914154053, 'learning_rate': 1.744408945686901e-05, 'epoch': 1.28}
{'loss': 0.4562, 'grad_norm': 3.5239555835723877, 'learning_rate': 1.6805111821086264e-05, 'epoch': 1.6}
{'loss': 0.4401, 'grad_norm': 4.781501293182373, 'learning_rate': 1.6166134185303515e-05, 'epoch': 1.92}
{'eval_loss': 0.5335591435432434, 'eval_f1-score': {'Self-Transcendence': 0.5, 'Self-Enhancement': 0.55}, 'eval_macro-avg-f1-score': 0.52, 'eval_runtime': 159.4986, 'eval_samples_per_second': 47.649, 'eval_steps_per_second': 11.912, 'epoch': 2.0}
2025-04-06 12:18:37,689 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.3422, 'grad_norm': 6.414053916931152, 'learning_rate': 1.552715654952077e-05, 'epoch': 2.24}
{'loss': 0.2993, 'grad_norm': 6.975147724151611, 'learning_rate': 1.488817891373802e-05, 'epoch': 2.56}
{'loss': 0.293, 'grad_norm': 7.132174968719482, 'learning_rate': 1.4249201277955273e-05, 'epoch': 2.87}
{'eval_loss': 0.6699076294898987, 'eval_f1-score': {'Self-Transcendence': 0.46, 'Self-Enhancement': 0.55}, 'eval_macro-avg-f1-score': 0.5, 'eval_runtime': 157.5715, 'eval_samples_per_second': 48.232, 'eval_steps_per_second': 12.058, 'epoch': 3.0}
{'loss': 0.2219, 'grad_norm': 8.273521423339844, 'learning_rate': 1.3610223642172523e-05, 'epoch': 3.19}
{'loss': 0.1856, 'grad_norm': 4.71503210067749, 'learning_rate': 1.2971246006389777e-05, 'epoch': 3.51}
{'loss': 0.1844, 'grad_norm': 7.2365336418151855, 'learning_rate': 1.233226837060703e-05, 'epoch': 3.83}
{'eval_loss': 0.8768484592437744, 'eval_f1-score': {'Self-Transcendence': 0.46, 'Self-Enhancement': 0.54}, 'eval_macro-avg-f1-score': 0.5, 'eval_runtime': 165.6292, 'eval_samples_per_second': 45.886, 'eval_steps_per_second': 11.471, 'epoch': 4.0}
{'loss': 0.145, 'grad_norm': 5.136754035949707, 'learning_rate': 1.1693290734824283e-05, 'epoch': 4.15}
{'loss': 0.1104, 'grad_norm': 11.802887916564941, 'learning_rate': 1.1054313099041534e-05, 'epoch': 4.47}
{'loss': 0.1149, 'grad_norm': 8.613828659057617, 'learning_rate': 1.0415335463258786e-05, 'epoch': 4.79}
{'eval_loss': 1.062862753868103, 'eval_f1-score': {'Self-Transcendence': 0.44, 'Self-Enhancement': 0.52}, 'eval_macro-avg-f1-score': 0.48, 'eval_runtime': 156.6054, 'eval_samples_per_second': 48.53, 'eval_steps_per_second': 12.132, 'epoch': 5.0}
{'loss': 0.0903, 'grad_norm': 5.061124801635742, 'learning_rate': 9.77635782747604e-06, 'epoch': 5.11}
{'loss': 0.0745, 'grad_norm': 9.430679321289062, 'learning_rate': 9.137380191693292e-06, 'epoch': 5.43}
{'loss': 0.0719, 'grad_norm': 8.50204086303711, 'learning_rate': 8.498402555910544e-06, 'epoch': 5.75}
{'eval_loss': 1.3078762292861938, 'eval_f1-score': {'Self-Transcendence': 0.44, 'Self-Enhancement': 0.52}, 'eval_macro-avg-f1-score': 0.48, 'eval_runtime': 155.4243, 'eval_samples_per_second': 48.898, 'eval_steps_per_second': 12.225, 'epoch': 6.0}
{'loss': 0.0674, 'grad_norm': 1.039252758026123, 'learning_rate': 7.859424920127796e-06, 'epoch': 6.07}
{'loss': 0.0471, 'grad_norm': 12.710419654846191, 'learning_rate': 7.220447284345049e-06, 'epoch': 6.39}
{'loss': 0.047, 'grad_norm': 2.0391721725463867, 'learning_rate': 6.581469648562301e-06, 'epoch': 6.71}
{'eval_loss': 1.596295714378357, 'eval_f1-score': {'Self-Transcendence': 0.46, 'Self-Enhancement': 0.5}, 'eval_macro-avg-f1-score': 0.48, 'eval_runtime': 159.465, 'eval_samples_per_second': 47.659, 'eval_steps_per_second': 11.915, 'epoch': 7.0}
{'train_runtime': 31588.1314, 'train_samples_per_second': 15.858, 'train_steps_per_second': 0.495, 'train_loss': 0.2459756778606677, 'epoch': 7.0}
2025-04-06 18:35:06,178 - INFO - 

VALIDATION
2025-04-06 18:35:06,178 - INFO - ==========
2025-04-06 18:37:52,408 - INFO - Self-Transcendence: 0.46
2025-04-06 18:37:52,408 - INFO - Self-Enhancement: 0.55
2025-04-06 18:37:52,408 - INFO - Macro average: 0.50
2025-04-06 18:37:52,743 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
