2025-02-28 09:10:04,320 - INFO - PyTorch version 2.6.0+cu118 available.
2025-02-28 09:10:04,320 - INFO - PyTorch version 2.6.0+cu118 available.
2025-02-28 09:10:09,870 - INFO - Setting random seed to 42
2025-02-28 09:10:09,870 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-02-28 09:10:10,251 - INFO - Loading lexicon embeddings for: No lexicon used
2025-02-28 09:10:10,251 - INFO - Preparing datasets for training and validation
2025-02-28 09:10:52,339 - INFO - Arguments validated successfully.
2025-02-28 09:10:52,345 - INFO - Previous sentences or augmented data detected. Adjusting batch size: 4 -> 2 and gradient accumulation steps: 4 -> 8
2025-02-28 09:10:52,345 - INFO - Clearing old checkpoints in models/checkpoints
2025-02-28 09:10:53,736 - INFO - Using CUDA for training.
2025-02-28 09:10:54,952 - INFO - TRAINING
2025-02-28 09:10:54,952 - INFO - ========
2025-02-28 09:10:54,952 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
Adding topic detection features: No
Applying token pruning: No

2025-02-28 09:10:54,955 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6886, 'grad_norm': 1.7605551481246948, 'learning_rate': 1.9670184696569924e-05, 'epoch': 0.16}
{'loss': 0.6649, 'grad_norm': 2.8262341022491455, 'learning_rate': 1.9340369393139843e-05, 'epoch': 0.33}
{'loss': 0.6499, 'grad_norm': 2.062528133392334, 'learning_rate': 1.9010554089709766e-05, 'epoch': 0.49}
{'loss': 0.6436, 'grad_norm': 2.092491626739502, 'learning_rate': 1.8680738786279685e-05, 'epoch': 0.66}
{'loss': 0.6361, 'grad_norm': 2.5911362171173096, 'learning_rate': 1.8350923482849604e-05, 'epoch': 0.82}
{'loss': 0.6329, 'grad_norm': 3.088078022003174, 'learning_rate': 1.8021108179419526e-05, 'epoch': 0.99}
{'eval_loss': 0.6608814597129822, 'eval_f1-score': {'Presence': 0.68}, 'eval_macro-avg-f1-score': 0.68, 'eval_runtime': 180.0085, 'eval_samples_per_second': 82.796, 'eval_steps_per_second': 20.699, 'epoch': 1.0}
2025-02-28 10:35:00,608 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.5799, 'grad_norm': 5.555934906005859, 'learning_rate': 1.769129287598945e-05, 'epoch': 1.15}
{'loss': 0.5719, 'grad_norm': 4.018036365509033, 'learning_rate': 1.7361477572559368e-05, 'epoch': 1.32}
{'loss': 0.5623, 'grad_norm': 4.0783305168151855, 'learning_rate': 1.703166226912929e-05, 'epoch': 1.48}
{'loss': 0.5435, 'grad_norm': 5.274254322052002, 'learning_rate': 1.670184696569921e-05, 'epoch': 1.65}
{'loss': 0.5526, 'grad_norm': 5.5822272300720215, 'learning_rate': 1.637203166226913e-05, 'epoch': 1.81}
{'loss': 0.5355, 'grad_norm': 5.048949241638184, 'learning_rate': 1.604221635883905e-05, 'epoch': 1.98}
{'eval_loss': 0.7091759443283081, 'eval_f1-score': {'Presence': 0.58}, 'eval_macro-avg-f1-score': 0.58, 'eval_runtime': 179.8193, 'eval_samples_per_second': 82.883, 'eval_steps_per_second': 20.721, 'epoch': 2.0}
2025-02-28 11:59:22,346 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.4367, 'grad_norm': 5.379115104675293, 'learning_rate': 1.5712401055408974e-05, 'epoch': 2.14}
{'loss': 0.4108, 'grad_norm': 8.895135879516602, 'learning_rate': 1.5382585751978893e-05, 'epoch': 2.31}
{'loss': 0.4132, 'grad_norm': 8.573197364807129, 'learning_rate': 1.5052770448548815e-05, 'epoch': 2.47}
{'loss': 0.3977, 'grad_norm': 7.448724269866943, 'learning_rate': 1.4722955145118736e-05, 'epoch': 2.64}
{'loss': 0.397, 'grad_norm': 7.092752456665039, 'learning_rate': 1.4393139841688655e-05, 'epoch': 2.8}
{'loss': 0.3842, 'grad_norm': 8.072490692138672, 'learning_rate': 1.4063324538258576e-05, 'epoch': 2.97}
{'eval_loss': 0.9867374300956726, 'eval_f1-score': {'Presence': 0.61}, 'eval_macro-avg-f1-score': 0.61, 'eval_runtime': 180.2516, 'eval_samples_per_second': 82.684, 'eval_steps_per_second': 20.671, 'epoch': 3.0}
{'loss': 0.2881, 'grad_norm': 8.852542877197266, 'learning_rate': 1.3733509234828497e-05, 'epoch': 3.13}
{'loss': 0.2632, 'grad_norm': 16.69712257385254, 'learning_rate': 1.3403693931398417e-05, 'epoch': 3.3}
{'loss': 0.2585, 'grad_norm': 8.707256317138672, 'learning_rate': 1.307387862796834e-05, 'epoch': 3.46}
{'loss': 0.2717, 'grad_norm': 15.871854782104492, 'learning_rate': 1.274406332453826e-05, 'epoch': 3.63}
{'loss': 0.2624, 'grad_norm': 10.630406379699707, 'learning_rate': 1.241424802110818e-05, 'epoch': 3.79}
{'loss': 0.2513, 'grad_norm': 7.634902000427246, 'learning_rate': 1.20844327176781e-05, 'epoch': 3.96}
{'eval_loss': 1.1387348175048828, 'eval_f1-score': {'Presence': 0.6}, 'eval_macro-avg-f1-score': 0.6, 'eval_runtime': 179.9748, 'eval_samples_per_second': 82.812, 'eval_steps_per_second': 20.703, 'epoch': 4.0}
{'loss': 0.1934, 'grad_norm': 12.854368209838867, 'learning_rate': 1.1754617414248021e-05, 'epoch': 4.12}
{'loss': 0.1741, 'grad_norm': 13.39553451538086, 'learning_rate': 1.1424802110817944e-05, 'epoch': 4.29}
{'loss': 0.1646, 'grad_norm': 11.22421932220459, 'learning_rate': 1.1094986807387865e-05, 'epoch': 4.45}
{'loss': 0.1659, 'grad_norm': 20.88151741027832, 'learning_rate': 1.0765171503957785e-05, 'epoch': 4.62}
{'loss': 0.1682, 'grad_norm': 9.840149879455566, 'learning_rate': 1.0435356200527704e-05, 'epoch': 4.78}
{'loss': 0.1665, 'grad_norm': 19.160186767578125, 'learning_rate': 1.0105540897097625e-05, 'epoch': 4.95}
{'eval_loss': 1.6056050062179565, 'eval_f1-score': {'Presence': 0.6}, 'eval_macro-avg-f1-score': 0.6, 'eval_runtime': 179.6502, 'eval_samples_per_second': 82.961, 'eval_steps_per_second': 20.74, 'epoch': 5.0}
{'loss': 0.1221, 'grad_norm': 10.160273551940918, 'learning_rate': 9.775725593667546e-06, 'epoch': 5.11}
{'loss': 0.1166, 'grad_norm': 30.873472213745117, 'learning_rate': 9.445910290237469e-06, 'epoch': 5.28}
{'loss': 0.1099, 'grad_norm': 4.3902764320373535, 'learning_rate': 9.11609498680739e-06, 'epoch': 5.44}
{'loss': 0.1164, 'grad_norm': 6.707527160644531, 'learning_rate': 8.786279683377308e-06, 'epoch': 5.61}
{'loss': 0.1162, 'grad_norm': 22.78993797302246, 'learning_rate': 8.456464379947231e-06, 'epoch': 5.77}
{'loss': 0.1072, 'grad_norm': 0.7835299968719482, 'learning_rate': 8.126649076517152e-06, 'epoch': 5.94}
{'eval_loss': 2.007996082305908, 'eval_f1-score': {'Presence': 0.59}, 'eval_macro-avg-f1-score': 0.59, 'eval_runtime': 180.1038, 'eval_samples_per_second': 82.752, 'eval_steps_per_second': 20.688, 'epoch': 6.0}
{'loss': 0.0949, 'grad_norm': 0.25185662508010864, 'learning_rate': 7.79683377308707e-06, 'epoch': 6.1}
{'loss': 0.0864, 'grad_norm': 20.880216598510742, 'learning_rate': 7.4670184696569924e-06, 'epoch': 6.27}
{'loss': 0.0833, 'grad_norm': 2.267509698867798, 'learning_rate': 7.137203166226914e-06, 'epoch': 6.43}
{'loss': 0.087, 'grad_norm': 9.33818531036377, 'learning_rate': 6.807387862796835e-06, 'epoch': 6.6}
{'loss': 0.087, 'grad_norm': 24.81241226196289, 'learning_rate': 6.477572559366755e-06, 'epoch': 6.76}
{'loss': 0.0841, 'grad_norm': 24.750953674316406, 'learning_rate': 6.1477572559366764e-06, 'epoch': 6.93}
{'eval_loss': 2.4593617916107178, 'eval_f1-score': {'Presence': 0.58}, 'eval_macro-avg-f1-score': 0.58, 'eval_runtime': 180.144, 'eval_samples_per_second': 82.734, 'eval_steps_per_second': 20.683, 'epoch': 7.0}
{'train_runtime': 35367.9527, 'train_samples_per_second': 27.434, 'train_steps_per_second': 0.857, 'train_loss': 0.3198587628431784, 'epoch': 7.0}
2025-02-28 19:00:23,955 - INFO - 

VALIDATION
2025-02-28 19:00:23,955 - INFO - ==========
2025-02-28 19:03:23,903 - INFO - Presence: 0.61
2025-02-28 19:03:23,903 - INFO - Macro average: 0.61
2025-02-28 19:03:24,006 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
