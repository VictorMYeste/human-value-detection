2025-05-03 05:37:42,945 - INFO - PyTorch version 2.6.0+cu118 available.
2025-05-03 05:37:42,946 - INFO - PyTorch version 2.6.0+cu118 available.
2025-05-03 05:37:48,441 - INFO - Setting random seed to 42
2025-05-03 05:37:48,442 - INFO - Running training for labels: ['Self-Transcendence', 'Self-Enhancement']
2025-05-03 05:37:48,442 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-05-03 05:37:48,627 - INFO - Loading lexicon embeddings for: No lexicon used
2025-05-03 05:37:48,627 - INFO - Preparing datasets for training and validation
2025-05-03 05:38:30,316 - INFO - Arguments validated successfully.
2025-05-03 05:38:30,323 - INFO - Previous sentences or augmented data detected. Adjusting batch size: 4 -> 2 and gradient accumulation steps: 4 -> 8
2025-05-03 05:38:30,323 - INFO - Clearing old checkpoints in models/checkpoints
2025-05-03 05:38:31,715 - INFO - Using CUDA for training.
2025-05-03 05:38:32,929 - INFO - TRAINING
2025-05-03 05:38:32,929 - INFO - ========
2025-05-03 05:38:32,929 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Filter labels: []
Batch size: 2
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 8
Early stopping patience: 4
Multilayer: No
ResidualBlock: No
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 0
Using data augmentation with paraphrasing: Yes
Adding topic detection features: No
Applying token pruning: No

2025-05-03 05:38:32,931 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.4196, 'grad_norm': 1.8423815965652466, 'learning_rate': 1.9670184696569924e-05, 'epoch': 0.16}
{'loss': 0.406, 'grad_norm': 2.5398495197296143, 'learning_rate': 1.9340369393139843e-05, 'epoch': 0.33}
{'loss': 0.4022, 'grad_norm': 1.4423398971557617, 'learning_rate': 1.9010554089709766e-05, 'epoch': 0.49}
{'loss': 0.3953, 'grad_norm': 1.2224161624908447, 'learning_rate': 1.8680738786279685e-05, 'epoch': 0.66}
{'loss': 0.3987, 'grad_norm': 1.943162441253662, 'learning_rate': 1.8350923482849604e-05, 'epoch': 0.82}
{'loss': 0.3883, 'grad_norm': 2.0002567768096924, 'learning_rate': 1.8021108179419526e-05, 'epoch': 0.99}
{'eval_loss': 0.34602630138397217, 'eval_f1-score': {'Self-Transcendence': 0.27, 'Self-Enhancement': 0.16}, 'eval_macro-avg-f1-score': 0.22, 'eval_runtime': 182.8839, 'eval_samples_per_second': 81.494, 'eval_steps_per_second': 20.374, 'epoch': 1.0}
2025-05-03 07:03:50,676 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.356, 'grad_norm': 3.136817216873169, 'learning_rate': 1.769129287598945e-05, 'epoch': 1.15}
{'loss': 0.3479, 'grad_norm': 2.8724093437194824, 'learning_rate': 1.7361477572559368e-05, 'epoch': 1.32}
{'loss': 0.3325, 'grad_norm': 2.8167002201080322, 'learning_rate': 1.703166226912929e-05, 'epoch': 1.48}
{'loss': 0.3285, 'grad_norm': 2.937631130218506, 'learning_rate': 1.670184696569921e-05, 'epoch': 1.65}
{'loss': 0.3272, 'grad_norm': 2.5757343769073486, 'learning_rate': 1.637203166226913e-05, 'epoch': 1.81}
{'loss': 0.3201, 'grad_norm': 2.8925955295562744, 'learning_rate': 1.604221635883905e-05, 'epoch': 1.98}
{'eval_loss': 0.38884973526000977, 'eval_f1-score': {'Self-Transcendence': 0.38, 'Self-Enhancement': 0.33}, 'eval_macro-avg-f1-score': 0.36, 'eval_runtime': 182.7258, 'eval_samples_per_second': 81.565, 'eval_steps_per_second': 20.391, 'epoch': 2.0}
2025-05-03 08:29:07,166 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.2496, 'grad_norm': 3.5042524337768555, 'learning_rate': 1.5712401055408974e-05, 'epoch': 2.14}
{'loss': 0.2386, 'grad_norm': 5.085449695587158, 'learning_rate': 1.5382585751978893e-05, 'epoch': 2.31}
{'loss': 0.233, 'grad_norm': 4.604790210723877, 'learning_rate': 1.5052770448548815e-05, 'epoch': 2.47}
{'loss': 0.228, 'grad_norm': 4.961979389190674, 'learning_rate': 1.4722955145118736e-05, 'epoch': 2.64}
{'loss': 0.2292, 'grad_norm': 5.214975357055664, 'learning_rate': 1.4393139841688655e-05, 'epoch': 2.8}
{'loss': 0.2193, 'grad_norm': 5.654881954193115, 'learning_rate': 1.4063324538258576e-05, 'epoch': 2.97}
{'eval_loss': 0.47547677159309387, 'eval_f1-score': {'Self-Transcendence': 0.29, 'Self-Enhancement': 0.25}, 'eval_macro-avg-f1-score': 0.27, 'eval_runtime': 183.2945, 'eval_samples_per_second': 81.312, 'eval_steps_per_second': 20.328, 'epoch': 3.0}
{'loss': 0.157, 'grad_norm': 6.52916145324707, 'learning_rate': 1.3733509234828497e-05, 'epoch': 3.13}
{'loss': 0.1461, 'grad_norm': 6.669550895690918, 'learning_rate': 1.3403693931398417e-05, 'epoch': 3.3}
{'loss': 0.1428, 'grad_norm': 6.180150508880615, 'learning_rate': 1.307387862796834e-05, 'epoch': 3.46}
{'loss': 0.1472, 'grad_norm': 5.363010406494141, 'learning_rate': 1.274406332453826e-05, 'epoch': 3.63}
{'loss': 0.1385, 'grad_norm': 9.263331413269043, 'learning_rate': 1.241424802110818e-05, 'epoch': 3.79}
{'loss': 0.1399, 'grad_norm': 6.13343620300293, 'learning_rate': 1.20844327176781e-05, 'epoch': 3.96}
{'eval_loss': 0.6112793684005737, 'eval_f1-score': {'Self-Transcendence': 0.28, 'Self-Enhancement': 0.31}, 'eval_macro-avg-f1-score': 0.3, 'eval_runtime': 183.2306, 'eval_samples_per_second': 81.34, 'eval_steps_per_second': 20.335, 'epoch': 4.0}
{'loss': 0.0995, 'grad_norm': 6.763592720031738, 'learning_rate': 1.1754617414248021e-05, 'epoch': 4.12}
{'loss': 0.0894, 'grad_norm': 7.795858860015869, 'learning_rate': 1.1424802110817944e-05, 'epoch': 4.29}
{'loss': 0.0837, 'grad_norm': 13.383594512939453, 'learning_rate': 1.1094986807387865e-05, 'epoch': 4.45}
{'loss': 0.0861, 'grad_norm': 14.055925369262695, 'learning_rate': 1.0765171503957785e-05, 'epoch': 4.62}
{'loss': 0.0868, 'grad_norm': 10.568642616271973, 'learning_rate': 1.0435356200527704e-05, 'epoch': 4.78}
{'loss': 0.0858, 'grad_norm': 5.88200569152832, 'learning_rate': 1.0105540897097625e-05, 'epoch': 4.95}
{'eval_loss': 0.7662521004676819, 'eval_f1-score': {'Self-Transcendence': 0.29, 'Self-Enhancement': 0.32}, 'eval_macro-avg-f1-score': 0.3, 'eval_runtime': 182.6986, 'eval_samples_per_second': 81.577, 'eval_steps_per_second': 20.394, 'epoch': 5.0}
{'loss': 0.0662, 'grad_norm': 10.499119758605957, 'learning_rate': 9.775725593667546e-06, 'epoch': 5.11}
{'loss': 0.0538, 'grad_norm': 5.9440741539001465, 'learning_rate': 9.445910290237469e-06, 'epoch': 5.28}
{'loss': 0.0588, 'grad_norm': 6.556035041809082, 'learning_rate': 9.11609498680739e-06, 'epoch': 5.44}
{'loss': 0.059, 'grad_norm': 10.139878273010254, 'learning_rate': 8.786279683377308e-06, 'epoch': 5.61}
{'loss': 0.0554, 'grad_norm': 9.409576416015625, 'learning_rate': 8.456464379947231e-06, 'epoch': 5.77}
{'loss': 0.0541, 'grad_norm': 0.8504878282546997, 'learning_rate': 8.126649076517152e-06, 'epoch': 5.94}
{'eval_loss': 0.9696498513221741, 'eval_f1-score': {'Self-Transcendence': 0.25, 'Self-Enhancement': 0.29}, 'eval_macro-avg-f1-score': 0.27, 'eval_runtime': 183.4236, 'eval_samples_per_second': 81.255, 'eval_steps_per_second': 20.314, 'epoch': 6.0}
{'loss': 0.0459, 'grad_norm': 7.858007431030273, 'learning_rate': 7.79683377308707e-06, 'epoch': 6.1}
{'loss': 0.0391, 'grad_norm': 2.265277624130249, 'learning_rate': 7.4670184696569924e-06, 'epoch': 6.27}
{'loss': 0.036, 'grad_norm': 2.0048835277557373, 'learning_rate': 7.137203166226914e-06, 'epoch': 6.43}
{'loss': 0.0368, 'grad_norm': 10.201898574829102, 'learning_rate': 6.807387862796835e-06, 'epoch': 6.6}
{'loss': 0.041, 'grad_norm': 8.006382942199707, 'learning_rate': 6.477572559366755e-06, 'epoch': 6.76}
{'loss': 0.0364, 'grad_norm': 5.4738311767578125, 'learning_rate': 6.1477572559366764e-06, 'epoch': 6.93}
{'eval_loss': 1.1032859086990356, 'eval_f1-score': {'Self-Transcendence': 0.27, 'Self-Enhancement': 0.3}, 'eval_macro-avg-f1-score': 0.29, 'eval_runtime': 183.1417, 'eval_samples_per_second': 81.38, 'eval_steps_per_second': 20.345, 'epoch': 7.0}
{'loss': 0.032, 'grad_norm': 3.618633508682251, 'learning_rate': 5.817941952506597e-06, 'epoch': 7.09}
{'loss': 0.0261, 'grad_norm': 11.248225212097168, 'learning_rate': 5.488126649076517e-06, 'epoch': 7.26}
{'loss': 0.0268, 'grad_norm': 14.273622512817383, 'learning_rate': 5.158311345646439e-06, 'epoch': 7.42}
{'loss': 0.0288, 'grad_norm': 0.07848071306943893, 'learning_rate': 4.828496042216359e-06, 'epoch': 7.59}
{'loss': 0.0257, 'grad_norm': 4.4263763427734375, 'learning_rate': 4.49868073878628e-06, 'epoch': 7.75}
{'loss': 0.0269, 'grad_norm': 5.609523296356201, 'learning_rate': 4.168865435356201e-06, 'epoch': 7.92}
{'eval_loss': 1.2197357416152954, 'eval_f1-score': {'Self-Transcendence': 0.26, 'Self-Enhancement': 0.31}, 'eval_macro-avg-f1-score': 0.29, 'eval_runtime': 183.6278, 'eval_samples_per_second': 81.164, 'eval_steps_per_second': 20.291, 'epoch': 8.0}
{'train_runtime': 40973.2971, 'train_samples_per_second': 23.681, 'train_steps_per_second': 0.74, 'train_loss': 0.16459606532906643, 'epoch': 8.0}
2025-05-03 17:01:27,278 - INFO - 

VALIDATION
2025-05-03 17:01:27,278 - INFO - ==========
2025-05-03 17:04:30,319 - INFO - Self-Transcendence: 0.28
2025-05-03 17:04:30,319 - INFO - Self-Enhancement: 0.31
2025-05-03 17:04:30,319 - INFO - Macro average: 0.30
2025-05-03 17:04:30,558 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
