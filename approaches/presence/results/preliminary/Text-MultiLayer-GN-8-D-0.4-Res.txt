2025-01-15 16:54:12,545 - HVD - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-01-15 16:54:12,833 - HVD - INFO - Loading lexicon embeddings for: No lexicon used
2025-01-15 16:54:12,833 - HVD - INFO - Preparing datasets for training and validation
2025-01-15 16:54:30,965 - HVD - INFO - Arguments validated successfully.
2025-01-15 16:54:31,417 - HVD - INFO - Using CUDA for training.
2025-01-15 16:54:32,669 - HVD - INFO - TRAINING
2025-01-15 16:54:32,669 - HVD - INFO - ========
2025-01-15 16:54:32,669 - HVD - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: Text-MultiLayer-GN-8-D-0.4-Res
Batch size: 4
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.01
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: Yes
Previous sentences used: No
Using lexicon: No
Adding linguistic features: No
Number of categories (lexicon): 0

2025-01-15 16:54:32,674 - accelerate.utils.other - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6786, 'grad_norm': 172610.90625, 'learning_rate': 1.928469241773963e-05, 'epoch': 0.36}
{'loss': 0.6528, 'grad_norm': 203737.75, 'learning_rate': 1.8569384835479257e-05, 'epoch': 0.71}
{'eval_loss': 0.6500133275985718, 'eval_f1-score': {'Presence': 0.64}, 'eval_marco-avg-f1-score': 0.64, 'eval_runtime': 326.0566, 'eval_samples_per_second': 45.71, 'eval_steps_per_second': 5.714, 'epoch': 1.0}
2025-01-15 17:39:12,399 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.6403, 'grad_norm': 230539.140625, 'learning_rate': 1.7854077253218886e-05, 'epoch': 1.07}
{'loss': 0.6056, 'grad_norm': 178282.59375, 'learning_rate': 1.7138769670958512e-05, 'epoch': 1.43}
{'loss': 0.6027, 'grad_norm': 194170.890625, 'learning_rate': 1.642346208869814e-05, 'epoch': 1.79}
{'eval_loss': 0.6658168435096741, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 326.2138, 'eval_samples_per_second': 45.688, 'eval_steps_per_second': 5.711, 'epoch': 2.0}
2025-01-15 18:23:54,464 - HVD - INFO - Skipping evaluation for warm-up phase (epoch 2).
{'loss': 0.5592, 'grad_norm': 303514.40625, 'learning_rate': 1.570815450643777e-05, 'epoch': 2.14}
{'loss': 0.4921, 'grad_norm': 463057.3125, 'learning_rate': 1.4992846924177399e-05, 'epoch': 2.5}
{'loss': 0.4883, 'grad_norm': 697879.6875, 'learning_rate': 1.4277539341917026e-05, 'epoch': 2.86}
{'eval_loss': 0.7695634365081787, 'eval_f1-score': {'Presence': 0.59}, 'eval_marco-avg-f1-score': 0.59, 'eval_runtime': 325.8918, 'eval_samples_per_second': 45.733, 'eval_steps_per_second': 5.717, 'epoch': 3.0}
{'loss': 0.4051, 'grad_norm': 866077.875, 'learning_rate': 1.3562231759656654e-05, 'epoch': 3.22}
{'loss': 0.3567, 'grad_norm': 710310.1875, 'learning_rate': 1.284692417739628e-05, 'epoch': 3.57}
{'loss': 0.3548, 'grad_norm': 538016.9375, 'learning_rate': 1.213161659513591e-05, 'epoch': 3.93}
{'eval_loss': 0.9425086975097656, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 325.5866, 'eval_samples_per_second': 45.776, 'eval_steps_per_second': 5.722, 'epoch': 4.0}
{'loss': 0.2485, 'grad_norm': 587595.375, 'learning_rate': 1.1416309012875537e-05, 'epoch': 4.29}
{'loss': 0.2401, 'grad_norm': 705799.4375, 'learning_rate': 1.0701001430615166e-05, 'epoch': 4.65}
{'eval_loss': 1.1968731880187988, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 326.1564, 'eval_samples_per_second': 45.696, 'eval_steps_per_second': 5.712, 'epoch': 5.0}
{'loss': 0.2359, 'grad_norm': 383261.53125, 'learning_rate': 9.985693848354794e-06, 'epoch': 5.0}
{'loss': 0.1549, 'grad_norm': 698265.5625, 'learning_rate': 9.270386266094421e-06, 'epoch': 5.36}
{'loss': 0.1583, 'grad_norm': 714900.5625, 'learning_rate': 8.555078683834049e-06, 'epoch': 5.72}
{'eval_loss': 1.5244148969650269, 'eval_f1-score': {'Presence': 0.6}, 'eval_marco-avg-f1-score': 0.6, 'eval_runtime': 326.2781, 'eval_samples_per_second': 45.679, 'eval_steps_per_second': 5.71, 'epoch': 6.0}
{'loss': 0.1515, 'grad_norm': 588130.0, 'learning_rate': 7.839771101573678e-06, 'epoch': 6.08}
{'loss': 0.1124, 'grad_norm': 429052.09375, 'learning_rate': 7.124463519313305e-06, 'epoch': 6.43}
{'loss': 0.1181, 'grad_norm': 169063.890625, 'learning_rate': 6.409155937052933e-06, 'epoch': 6.79}
{'eval_loss': 1.968010425567627, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 325.6851, 'eval_samples_per_second': 45.762, 'eval_steps_per_second': 5.72, 'epoch': 7.0}
{'loss': 0.1013, 'grad_norm': 206952.078125, 'learning_rate': 5.693848354792561e-06, 'epoch': 7.15}
{'loss': 0.0901, 'grad_norm': 588737.125, 'learning_rate': 4.978540772532189e-06, 'epoch': 7.51}
{'loss': 0.0929, 'grad_norm': 965863.0, 'learning_rate': 4.2632331902718175e-06, 'epoch': 7.86}
{'eval_loss': 2.5947749614715576, 'eval_f1-score': {'Presence': 0.62}, 'eval_marco-avg-f1-score': 0.62, 'eval_runtime': 326.2114, 'eval_samples_per_second': 45.688, 'eval_steps_per_second': 5.711, 'epoch': 8.0}
{'loss': 0.0738, 'grad_norm': 686275.6875, 'learning_rate': 3.5479256080114456e-06, 'epoch': 8.22}
{'loss': 0.0731, 'grad_norm': 208509.59375, 'learning_rate': 2.8326180257510733e-06, 'epoch': 8.58}
{'loss': 0.0803, 'grad_norm': 840551.5625, 'learning_rate': 2.1173104434907013e-06, 'epoch': 8.94}
{'eval_loss': 3.4910106658935547, 'eval_f1-score': {'Presence': 0.61}, 'eval_marco-avg-f1-score': 0.61, 'eval_runtime': 325.7067, 'eval_samples_per_second': 45.759, 'eval_steps_per_second': 5.72, 'epoch': 9.0}
{'train_runtime': 24145.8724, 'train_samples_per_second': 18.537, 'train_steps_per_second': 0.579, 'train_loss': 0.3091829613272561, 'epoch': 9.0}
2025-01-15 23:36:59,087 - HVD - INFO - 

VALIDATION
2025-01-15 23:36:59,087 - HVD - INFO - ==========
2025-01-15 23:42:26,367 - HVD - INFO - Presence: 0.62
2025-01-15 23:42:26,367 - HVD - INFO - Macro average: 0.62
2025-01-15 23:42:27,387 - HVD - INFO - UPLOAD to https://huggingface.co/Text-MultiLayer-GN-8-D-0.4-Res (using HF_TOKEN environment variable)
2025-01-15 23:42:27,387 - HVD - INFO - SAVE to models
