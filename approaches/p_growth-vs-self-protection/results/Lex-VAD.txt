2025-04-02 09:14:10,319 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-02 09:14:10,321 - INFO - PyTorch version 2.6.0+cu118 available.
2025-04-02 09:14:16,561 - INFO - Setting random seed to 42
2025-04-02 09:14:16,562 - INFO - Running training for labels: ['Growth Anxiety-Free', 'Self-Protection Anxiety-Avoidance']
2025-04-02 09:14:16,562 - INFO - Initializing tokenizer for model: microsoft/deberta-base
2025-04-02 09:14:16,766 - INFO - Loading lexicon embeddings for: VAD
2025-04-02 09:14:16,801 - INFO - Preparing datasets for training and validation
2025-04-02 09:14:29,640 - INFO - Arguments validated successfully.
2025-04-02 09:14:29,642 - INFO - Clearing old checkpoints in models/checkpoints
2025-04-02 09:14:30,652 - INFO - Using CUDA for training.
2025-04-02 09:14:31,890 - INFO - TRAINING
2025-04-02 09:14:31,890 - INFO - ========
2025-04-02 09:14:31,890 - INFO - Training configuration:
Pre-trained model: microsoft/deberta-base
Model name: None
Filter labels: ['Presence']
Batch size: 4
Number of epochs: 10
Learning rate: 2e-05
Weight decay: 0.15
Gradient accumulation steps: 4
Early stopping patience: 4
Multilayer: No
Previous sentences used: No
Using lexicon: VAD
Adding linguistic features: No
Adding NER features: No
Number of categories (lexicon): 3
Using data augmentation with paraphrasing: No
Adding topic detection features: No
Applying token pruning: No

2025-04-02 09:14:31,892 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.5662, 'grad_norm': 4.341806888580322, 'learning_rate': 1.8611111111111114e-05, 'epoch': 0.69}
{'eval_loss': 0.5357317328453064, 'eval_f1-score': {'Growth Anxiety-Free': 0.71, 'Self-Protection Anxiety-Avoidance': 0.83}, 'eval_macro-avg-f1-score': 0.77, 'eval_runtime': 151.7152, 'eval_samples_per_second': 50.094, 'eval_steps_per_second': 6.262, 'epoch': 1.0}
2025-04-02 09:31:20,625 - INFO - Skipping evaluation for warm-up phase (epoch 0).
{'loss': 0.4776, 'grad_norm': 5.917706489562988, 'learning_rate': 1.7222222222222224e-05, 'epoch': 1.39}
{'eval_loss': 0.5376173853874207, 'eval_f1-score': {'Growth Anxiety-Free': 0.72, 'Self-Protection Anxiety-Avoidance': 0.83}, 'eval_macro-avg-f1-score': 0.77, 'eval_runtime': 151.615, 'eval_samples_per_second': 50.127, 'eval_steps_per_second': 6.266, 'epoch': 2.0}
2025-04-02 09:48:09,382 - INFO - Skipping evaluation for warm-up phase (epoch 1).
{'loss': 0.4226, 'grad_norm': 6.521677017211914, 'learning_rate': 1.5833333333333333e-05, 'epoch': 2.08}
{'loss': 0.3117, 'grad_norm': 7.038310527801514, 'learning_rate': 1.4444444444444446e-05, 'epoch': 2.77}
{'eval_loss': 0.6198527812957764, 'eval_f1-score': {'Growth Anxiety-Free': 0.71, 'Self-Protection Anxiety-Avoidance': 0.82}, 'eval_macro-avg-f1-score': 0.76, 'eval_runtime': 151.0557, 'eval_samples_per_second': 50.313, 'eval_steps_per_second': 6.289, 'epoch': 3.0}
{'loss': 0.2461, 'grad_norm': 4.21157169342041, 'learning_rate': 1.3055555555555557e-05, 'epoch': 3.47}
{'eval_loss': 0.7486441135406494, 'eval_f1-score': {'Growth Anxiety-Free': 0.72, 'Self-Protection Anxiety-Avoidance': 0.83}, 'eval_macro-avg-f1-score': 0.77, 'eval_runtime': 150.6703, 'eval_samples_per_second': 50.441, 'eval_steps_per_second': 6.305, 'epoch': 4.0}
{'loss': 0.1921, 'grad_norm': 6.70849609375, 'learning_rate': 1.1666666666666668e-05, 'epoch': 4.16}
{'loss': 0.14, 'grad_norm': 9.902876853942871, 'learning_rate': 1.0277777777777777e-05, 'epoch': 4.86}
{'eval_loss': 0.8993672728538513, 'eval_f1-score': {'Growth Anxiety-Free': 0.72, 'Self-Protection Anxiety-Avoidance': 0.82}, 'eval_macro-avg-f1-score': 0.77, 'eval_runtime': 151.3287, 'eval_samples_per_second': 50.222, 'eval_steps_per_second': 6.278, 'epoch': 5.0}
{'loss': 0.1115, 'grad_norm': 8.412675857543945, 'learning_rate': 8.888888888888888e-06, 'epoch': 5.55}
{'eval_loss': 1.065306544303894, 'eval_f1-score': {'Growth Anxiety-Free': 0.71, 'Self-Protection Anxiety-Avoidance': 0.83}, 'eval_macro-avg-f1-score': 0.77, 'eval_runtime': 150.2203, 'eval_samples_per_second': 50.592, 'eval_steps_per_second': 6.324, 'epoch': 6.0}
{'loss': 0.0908, 'grad_norm': 12.446885108947754, 'learning_rate': 7.500000000000001e-06, 'epoch': 6.24}
{'loss': 0.076, 'grad_norm': 7.3607258796691895, 'learning_rate': 6.111111111111112e-06, 'epoch': 6.94}
